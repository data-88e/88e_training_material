

--- SUMMARY for LECTURES ---

- week: 1
  count: 1
  notebooks:
  - file: lec01/lec01.md
    title: lec01
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec01/lec01.ipynb
- week: 2
  count: 5
  notebooks:
  - file: lec02/Avocados_demand.md
    title: Avocados_demand
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/Avocados_demand.ipynb
  - file: lec02/demand-curve-Fa24.md
    title: demand-curve-Fa24
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/demand-curve-Fa24.ipynb
  - file: lec02/Demand_Steps_24.md
    title: Demand_Steps_24
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/Demand_Steps_24.ipynb
  - file: lec02/PriceElasticity.md
    title: PriceElasticity
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/PriceElasticity.ipynb
  - file: lec02/ScannerData_Beer.md
    title: ScannerData_Beer
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/ScannerData_Beer.ipynb
- week: 3
  count: 5
  notebooks:
  - file: lec03/3.0-CubicCostCurve.md
    title: 3.0-CubicCostCurve
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.0-CubicCostCurve.ipynb
  - file: lec03/3.1-Supply.md
    title: 3.1-Supply
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.1-Supply.ipynb
  - file: lec03/3.2-sympy.md
    title: 3.2-sympy
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.2-sympy.ipynb
  - file: lec03/3.3a-california-energy.md
    title: 3.3a-california-energy
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.3a-california-energy.ipynb
  - file: lec03/3.3b-a-really-hot-tuesday.md
    title: 3.3b-a-really-hot-tuesday
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.3b-a-really-hot-tuesday.ipynb
- week: 4
  count: 6
  notebooks:
  - file: lec04/lec04-CSfromSurvey.md
    title: lec04-CSfromSurvey
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-CSfromSurvey.ipynb
  - file: lec04/lec04-CSfromSurvey-closed.md
    title: lec04-CSfromSurvey-closed
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-CSfromSurvey-closed.ipynb
  - file: lec04/lec04-four-plot.md
    title: lec04-four-plot
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-four-plot.ipynb
  - file: lec04/lec04-four-plot-24.md
    title: lec04-four-plot-24
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-four-plot-24.ipynb
  - file: lec04/lec04-Supply-Demand.md
    title: lec04-Supply-Demand
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-Supply-Demand.ipynb
  - file: lec04/lec04-Supply-Demand-closed.md
    title: lec04-Supply-Demand-closed
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-Supply-Demand-closed.ipynb
- week: 5
  count: 2
  notebooks:
  - file: lec05/Lec5-Cobb-Douglas.md
    title: Lec5-Cobb-Douglas
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec05/Lec5-Cobb-Douglas.ipynb
  - file: lec05/Lec5-CobbD-AER1928.md
    title: Lec5-CobbD-AER1928
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec05/Lec5-CobbD-AER1928.ipynb
- week: 6
  count: 5
  notebooks:
  - file: lec06/6.1-Sympy-Differentiation.md
    title: 6.1-Sympy-Differentiation
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.1-Sympy-Differentiation.ipynb
  - file: lec06/6.2-3D-utility.md
    title: 6.2-3D-utility
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.2-3D-utility.ipynb
  - file: lec06/6.3-QuantEcon-Optimization.md
    title: 6.3-QuantEcon-Optimization
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.3-QuantEcon-Optimization.ipynb
  - file: lec06/6.4-latex.md
    title: 6.4-latex
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.4-latex.ipynb
  - file: lec06/6.5-Edgeworth.md
    title: 6.5-Edgeworth
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.5-Edgeworth.ipynb
- week: 7
  count: 2
  notebooks:
  - file: lec07/7.1-inequality.md
    title: 7.1-inequality
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec07/7.1-inequality.ipynb
  - file: lec07/7.2-historical-inequality.md
    title: 7.2-historical-inequality
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec07/7.2-historical-inequality.ipynb
- week: 8
  count: 1
  notebooks:
  - file: lec08/macro-fred-api.md
    title: macro-fred-api
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec08/macro-fred-api.ipynb
- week: 9
  count: 1
  notebooks:
  - file: lec09/lecNB-prisoners-dilemma.md
    title: lecNB-prisoners-dilemma
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec09/lecNB-prisoners-dilemma.ipynb
- week: 10
  count: 2
  notebooks:
  - file: lec10/lec10.1-mapping.md
    title: lec10.1-mapping
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec10/lec10.1-mapping.ipynb
  - file: lec10/Lec10.2-waterguard.md
    title: Lec10.2-waterguard
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec10/Lec10.2-waterguard.ipynb
- week: 11
  count: 2
  notebooks:
  - file: lec11/11.1-slr.md
    title: 11.1-slr
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec11/11.1-slr.ipynb
  - file: lec11/11.2-mlr.md
    title: 11.2-mlr
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec11/11.2-mlr.ipynb
- week: 12
  count: 3
  notebooks:
  - file: lec12/lec12-1_Interest_Payments.md
    title: lec12-1_Interest_Payments
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec12/lec12-1_Interest_Payments.ipynb
  - file: lec12/lec12-2-stocks-options.md
    title: lec12-2-stocks-options
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec12/lec12-2-stocks-options.ipynb
  - file: lec12/Lec12-4-PersonalFinance.md
    title: Lec12-4-PersonalFinance
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec12/Lec12-4-PersonalFinance.ipynb
- week: 13
  count: 5
  notebooks:
  - file: lec13/Co2_ClimateChange.md
    title: Co2_ClimateChange
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/Co2_ClimateChange.ipynb
  - file: lec13/ConstructingMAC.md
    title: ConstructingMAC
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/ConstructingMAC.ipynb
  - file: lec13/EmissionsTracker.md
    title: EmissionsTracker
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/EmissionsTracker.ipynb
  - file: lec13/KuznetsHypothesis.md
    title: KuznetsHypothesis
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/KuznetsHypothesis.ipynb
  - file: lec13/RoslingPlots.md
    title: RoslingPlots
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/RoslingPlots.ipynb
- week: 15
  count: 1
  notebooks:
  - file: lec15/vibecession.md
    title: vibecession
    type: lecture-notebook
    source_path: /Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec15/vibecession.ipynb




--- START lec01/lec01.md ---

---
title: "lec01"
type: lecture-notebook
week: 1
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec01/lec01.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24 <br>
            Dr. Eric Van Dusen <br>
            Rohan Jha, Matt Yep, Alan Liang<br>
        <br>
</table>

# The Financial Benefits of Your Major

```python
#%pip install ipywidgets datascience
import ipywidgets as widgets
from datascience import *
import numpy as np
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
import pandas as pd
%matplotlib inline 
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
```

Welcome to Data 88E: *Economics Models*! This class will explore the intersection of Data Science and Economics. 
Specifically, we will utilize methods and techniques in data science to examine both basic and upper-division Economics concepts.
Throughout the course, we will consider a variety of economic problems both on the macro and micro level. 

In the first demo of the course, we hope to give you a sense of the types problems you can expect to explore this semester by considering a problem that may be of personal relevance to you: the post-graduate incomes of different majors at Cal.

We will be using various visualization techniques to analyze the median incomes of different majors at UC Berkeley, in addition to the median incomes of those same majors at other colleges.
If you forgot, the median income is the "middle" value: if you sorted all the individual incomes of a major in ascending order, the median would be the value that's exactly in the middle. The median is also called the 50th percentile -- at the median, exactly 50% of the individuals have an income lower than the median.

Do not be concerned if you don't understand the code below: this entire exercise is purely a demo to motivate many profound concepts in Economics. 
If you're interested, you may choose to come back to this demo at the end of the course and consider all the different techniques utilized in it - it'd be a great way of reflecting upon how much you've learnt!

## Data Collection

Before we can use data science to tackle any issue, we must–well–obtain data (kind of mind-boggling, I know).
Recall that we want to examine the median incomes of different majors at UC Berkeley as well as the median incomes of those same majors at other colleges.
The term 'other colleges' is a fairly general one, and in this case we shall consider the average median incomes of those majors at alll other colleges in the United States.

In order to obtain a dataset, you can either collect it yourself (via surveys, questionnaires, etc.) or you can use datasets that others have gathered for you.
In this demo, we are combining 3 different datasets:
- The median income for each major at Cal was obtained from [Cal's 2019 First Destination survey](https://career.berkeley.edu/survey/survey).
- The median income for each major overall was obtained from surveys conducted by the American Community Survey (ACS) from 2010 to 2012, a very popular data source for Economics Research! In the survey, ACS essentially calls college graduates and asked them their income as well as what they majored in at college. (As a side note, FiveThirtyEight later published this [article](https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/) using the results of the survey.) In this project, we will be using a modified version of the ACS survey - we will only be looking at the respondents who are 28 or younger. Can you think of why we would do this?
- The longitudinal data on long-run outcomes of UC Berkeley alumni was obtained from the [University of California webpage](https://www.universityofcalifornia.edu/infocenter/berkeley-outcomes). We will use this dataset later for a slightly different analysis.

Take a moment to consider the ways in which the 3 different datasets were created. 
Is it fair to draw direct comparisons between the datasets? What would be some potential issues and how could the differences in our datasets affect our analysis?


### Mean vs Median
Before proceeding further, it is important to consider why we are choosing to look at the median, and not the average, income.
In order to answer this question, let us think about what the *distribution* of incomes for a population would look like. 
Most likely, you would see a high amount of incomes around or slightly below the mean, with a few massive outlier incomes above the mean.
For example, consider a theatre major who becomes a star on Broadway - while they'd be doing absolutely fantastic in their career, they are not representative of the average theatre graduate from Berkeley and would likely pull the average income way up.
For this reason, using the median is more *robust*: it gives us a better idea of what the typical graduate for any major can generally expect to earn.

Now we'll load in all the data.
Take a look at the tables for each dataset.
Note that `P25th` referes to the 25th percentile of incomes (the income level at which exactly 25% of incomes are lower) and `P75th` refers to the 75th percentile of incomes (the income level at which exactly 75% of incomes are lower).
You may not know what all the different columns in the tables mean. That's okay! 
As data scientists, we often encounter a lot of irrelevant data that we will discard later.

```python
# Load in table of all majors' median incomes at Cal
cal_income = Table.read_table("cal_income.csv")
cal_income.show(10)
```

```python
# Load in table of all other universities' average major median incomes
other_income = Table.read_table("recent-grads.csv") 
other_income.show(10)
```

To make direct comparisons across majors, we combined all the tables above into a single one for us to use below.

```python
majors = Table.read_table("cal_vs_all.csv")
majors.show(10)
```

Our combined table above dropped the columns in above tables that we didn't need to conduct our exploration. 
It has a column `Median Income Difference`: this column is the Berkeley median income minus the overall median income for each major. 
It gives us a sense of the value of Cal over the average university: the difference is the additional income we recieve from obtaining a Cal degree.

Before moving forward, take a second to consider how well the above tables would match with each other.
For example, Electrical Engineering and Computer Science (EECS) is a popular major at Berkeley. However, the `majors` dataset didn't have a direct equivalent for it.
Instead, the `majors` dataset had Electrical Engineering, Electrical Engineering Technologies and Computer Engineering as separate majors. 
Since in theory students in EECS focus more on computer engineering, we chose to use the computer engineering data for drawing comparions in our final, combined table.
However, there's room for ambiguity here and that is another potential flaw in our exploration!

The interactive widgets below allows you to select majors to compare them. Play around with this for a bit; what do you observe?

```python
def display_major_table(major):
    selected = majors.where("Major", are.contained_in(major))
    selected.show()
    
    ind = np.arange(selected.num_rows) 
    width = 0.35

    fig, ax = plt.subplots()
    rects1 = ax.bar(ind - width/2, selected.column("Cal Median"), width,
                    label='Cal Median', color = "navy")
    rects2 = ax.bar(ind + width/2, selected.column("Overall Median"), width,
                    label='Overall Median', color = "goldenrod")
    
    ax.set_ylabel('Income')
    ax.set_xticks(ind)
    ax.set_xticklabels(selected.column("Major"))
    ax.set_title("Median Incomes for Selected Majors at Cal (Blue) and Other Universities (Gold)", fontsize= 13)
    plt.xticks(rotation=90)


dropdown_majors = widgets.SelectMultiple(
    options=majors.column("Major"),
    description="Major",
    disabled=False
)
display(widgets.interactive(display_major_table, major=dropdown_majors))
```

## The most lucrative major

Let's imagine that Belfort is a freshman at Berkeley. 
Belfort is a very unique individual - he is someone who would be equally good at all majors and enjoys all majors equally.
He also believes that money is the most important thing in the world.
These assumptions are vastly oversimplifcations and not the case for anyone in real life; but in Economics you'll find that we end up making significant simplifications to abstract away all the potential complications!

Since Belfort is equally good at and happy with all majors at Cal, the major he chooses is purely dependent on how much money he can earn after college with that major.
Therefore, he will choose the major with the highest median income.

Let us sort our table by the `Cal Median` column in descending order to see which major that would be.

```python
majors.sort('Cal Median',descending=True).show(10)
```

However, what if instead of just wanting the maximum amount of cash after college, Belfort was super proud of getting into Berkeley and instead wanted to maximise the amount of additional benefit he recieves from attending Berkeley over other universities?
Let's sort our table by the `Median Income Difference` column in descending order to see which major would give Belfort the highest additional income from going to Berkeley.  

Do any values in either of our sorted table surprise you? Why might that be?

```python
majors.sort("Median Income Difference",descending=True).show(10)
```

Let's now do the opposite and sort our table by the `Median Income Difference` column in ascending order.

```python
majors.sort("Median Income Difference").show(10)
```

As you can see in the table, the median income difference for both nuclear engineering and astrophysics is negative. 
This is rather peculiar, especially since Cal has great programs for both those majors.
Why would Cal graduates earn less income than the graduates of the average college for those 2 majors? 

*Hint*: consider what each of our datasets describes. Is it an apples-to-apples comparison?

The answer is because the dataset with the median incomes for all colleges actually described recent graduates (people who are 28 and younger, rather than just fresh college graduates.
It is likely that most people who graduate with Nuclear Engineering and Astrophysics degrees in other colleges tend to stay in the industry for their fields and keep accumulating experience. This would explain why a dataset that includes data from 28 year olds would have a higher median income than fresh Cal graduates in the same field.

### Generalization and Causal Inference

While Belfort would be equally good at and enjoy all majors equally, the implications for him may not generalize for anyone else. 

If you decide to major in EECS, it does not mean that you will have more income post-grad than that from majoring in anything else. For example, someone who is a fantastic artist but not very good at computer science would probably make far more money majoring in art than in EECS. In addition, just because EECS has the highest median income overall doesn't mean that any individual is guaranteed a high median income if they decide to major in EECS. Overall, our results are not completely *generalizable* to others.

The causal effect of majoring in EECS on incomes may also be overstated. Consider the fact that those who major in EECS tend to come from families whose parents are already in working in tech. As a result, these students will likely also find better jobs post-grad. This is what we call a confounding variable -- it is positively correlated with our treatment variable of majoring in EECS. If we do not observe this confounding factor, our analysis would likely overstate the effect of an EECS degree on post-grad incomes.

### A brief discourse on utility 

From seeing our results, you may be wondering: *why doesn't everyone just major in EECS then*? 

For Belfort, money is the only thing in the world that derives happiness. 
However, this is once again an oversimplication for the vast majority of people in the real world.
People derive utility from sources other than just their bank account; often, we choose relatively lower-paying jobs in order to derive more happiness in our lives.

**Utility** is a measure of 'satisfaction' or 'happiness' when we consume a good, while **disutility** is a measure of 'unsatisfaction' or 'harm' when we consume a *bad*. In economics, one assumption we will always make is that people will always seek to maximize their utility and minimize their disutility. 

However, we encounter *diminishing marginal utility* as we consume more and more of a good. For example, the utility of the first cookie you eat is likely much higher than the 20th cookie you eat. Similarly, we may encounter *increasing marginal disutility* as we consume more and more of a bad. For example, the disutility of the first hour doing something you dread is probably not as bad as the 10th hour of doing something you dread.

For many of us, money provides a source of utility, while working or the studying required to get there may provide a source of disutility. This presents a tradeoff: for some, the disutility from studying EECS or working as a programmer greatly outweights the utility from the money. Perhaps another job in a different field requires significantly lower disutility to achieve, without much impact on the utility of the income associated with it. This phenomenon would explain why not everyone decides to major in EECS!

## Incomes Over Time: Computer Science vs Economics 

So far our exploration of the data has considered a rather general timeframe of "post graduation", but perhaps it would be more insightful if we dove in and took a look at how incomes differed across various ages. We can do so using the table below that consists of data collected by the [UC ClioMetric History Project](http://uccliometric.org/). 
By compiling a database of digital student transcripts of all UC undergraduates and then linking California wages to individuals in the transcript database, 
they were able to produce comprehensive dashboards that visualize demographics, major choices, and long-run incomes of each UC campus’s alumni over the past 70 years. 


You can learn more about how they collected the data [here](https://www.universityofcalifornia.edu/infocenter/long-run-methodology), but for now we will just focus on a snapshot of the data. 
In particular, we are interested in exploring how percentile incomes compare between Cal alumni who majored in computer science versus those who majored in economics. Run the following cell to check it out.

```python
cs_vs_econ=Table.read_table("cs_econ_by_age.csv")
cs_vs_econ.show(10)
```

Here we have a table where each row corresponds to a certain age. 
It begins with age 23 (typically how old people are when getting their first job out of college), and goes up until age 62 (when people retire from the workforce). 

Let's graph our data using some line plots: line plots are especially useful when trying to visualize trends that change over time. 

Run the following cell to visualize the data. The yellow lines correspond to CS incomes, while the blue lines correspond to Econ incomes.

```python
df = cs_vs_econ.to_df()
df.plot("Age",color=["goldenrod","gold","yellow","khaki","navy","mediumblue","royalblue","cornflowerblue"])
plt.ylabel("Annual Income in Dollars")
plt.title("Comparing Incomes of Computer Science Majors vs Economics Majors");
fig = plt.gcf()
fig.set_size_inches(15, 11)
```

Let's take a look at how the incomes compare at each percentile. The most notable trend is probably the fact that for the most the part, CS majors typically make more money than Econ majors.
We especially observe this trend in the 25th and 50th percentiles, and this follows accordingly with our exploration earlier in the notebook that students who majored in L&S Computer Science on average had a higher median income than pretty much every other major.

The graph above helps explain why CS is one of the most popular majors here at Berkeley. 
Software engineering and related disciplines have a huge demand for fresh talent, so that individuals entering these fields are duly compensated. This in and of itself is a big appeal of students wanting to major in computer science. 

Right off the bat out of college, fresh CS graduates are looking at a six figure income: even the 25th percentile of CS majors are making over $103,000 by the time they are 25. At any given percentile level, CS graduates outearn their Economics counterparts immediately post-graduation.

### Intertemporal Effects

However, we don't just choose our majors based off of the immediate post-graduation income. Intuitively, we would want to maximize our total lifetime income we make. This idea is called *intertermporal utility maximization*: Economists often take into account one's utility across all possible time periods.

Considering lifetime income, which major would be better? Well, it depends: if you are at the bottom 25th or 50th percentile, CS is better. This suggests that the typical Economics major will likely never outearn the the typical CS major.

However, we observe that Economics majors at higher percentiles ultimately overtake CS majors in earnings. Intuitively, this should make sense. Perhaps top Economics majors are more likely to become successful executives later on in their careers while make more money than top CS majors, who continue to stay as software engineers or become engineering managers.

### Risk
We observe that Economics majors have a much larger spread in their lifetime earnings than CS majors: top Economics graduates make significantly more than bottom Economics graduates, compared to top CS and bottom CS graduates.

This brings to the light the notion of risk. Majoring in CS carries relatively lower variance: whether you are 
a top notch developer in the 75th percentile or a subpar developer in the 25th percentile, you will probably make a solid living. Since most CS majors typically end up pursuing similar careers in the realms of software development, there is less **variance** in the occupations. There is also less **risk**: most CS majors can expect to land a software engineering job and continue working as one twenty years into their career.

The same story is less true for Economics majors: a top-notch Economics major will significantly out-earn a subpar Economics major. This is partly due to the vastly differing occupation types Economics majors become: analysts, bankers, consultants, just to name a few (Economics is a very versatile degree, afterall). Twenty years down the line, the top Economics majors will have higher ceilings perhaps with a lot of opportunity for leadership or executive positions. There is a lot more **variance** in the types of jobs and thus salaries that Economics majors will attain. In addition, there is a lot more **risk**: not every Economics major will become a top executive, and most end up with more 'average' jobs that earn less than that of similar percentile CS majors.

This means that if you know you are at the top regardless of your major, you will tend to ultimately earn more income as an Economics major than a CS major. But a caveat: it's impossible to guarantee that you will be in the top, no matter how hard you work. Often, life has uncertainties and involves a great deal of luck to become successful.

The table and graph above only considered the percentile incomes of CS and Economics majors, but if you're interested in drawing more comparisons, check out [this page](https://www.universityofcalifornia.edu/infocenter/berkeley-outcomes). You can compare between all types of popular majors at UC Berkeley, and even toggle around with more features like gender, ethnicity, or even specific courses.

## An Afterword

In this demo, we've covered a series of fundamental Economics ideas such as income, utility, and risk. We've also gone over a series of more statistical concepts that are also at the heart of empirical Economics such as causal inference, generalization, and unobserved variables. We did this all using data science techniques and visualizations, while being cognizant of potential sources of error. We will be revisiting a lot of these themes in later parts of the course.

From this exercise you may have noticed that pursuing either Economics, Data Science, or both are great options in terms of post-graduation incomes. This means you've probably chosen wisely to have ended up in this class. 

Welcome to Data 88E!

```python

```



--- END lec01/lec01.md ---



--- START lec02/Avocados_demand.md ---

---
title: "Avocados_demand"
type: lecture-notebook
week: 2
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/Avocados_demand.ipynb"
---

```python
# HIDDEN
%pip install datascience ipywidgets
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter("ignore")
from matplotlib import patches
from datascience import *
%matplotlib inline
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets
```

# Empirical Demand Curves

*This deck largely mirrors parts 2-3 of the [demand chapter](https://data-88e.github.io/textbook/content/01-demand/index.html):*
- *An Empirical Demand Curve*
- *Log-log and Semi-log Demand Curves*

## Avocados

Let's examine some historical data on non-organic avocado prices and sales volumes in San Francisco from 2015 to 2018. The original dataset is taken from Kaggle and can be found [here](https://www.kaggle.com/neuromusic/avocado-prices).

```python
avocados = Table.read_table("avocados.csv") # is it avocados or avocadoes?
avocados
```

## Visualizing the Relationship between Price and Quantity

We would expect to see a downward-sloping line between price and quantity; if a product's price increases, consumers will purchase less, and if a product's price decreases, then consumers will purchase more.

```python
avocados.scatter("Total Volume", "Average Price", width = 7, height = 7)
plt.title("Demand Curve for Avocados", fontsize = 16);
```

## Linear Demand Curves

### Demand with Price as a Function of Quantity

First, we will fit a demand curve expressed in terms of price as a function of quantity. This aligns with the axes of supply and demand curves, in which the quantity is on the x-axis and price is on the y-axis:

$$P(Q) = m\cdot Q + b$$

We will now quantify our demand curve using NumPy's [`np.polyfit` function](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). 

It takes 3 parameters:
- array of x-coordinates
- array of y-coordinates
- degree of polynomial 

With degree 1, `np.polyfit` returns an array of size 2, where the first element is the slope and the second is the $y$-intercept.

```python
slope, intercept = np.polyfit(avocados.column("Total Volume"), avocados.column("Average Price"), 1)
print("The slope is:", slope)
print("The intercept is:", intercept)
```

Our demand curve is $P(Q) = -0.00000109Q+ 2.2495$:
- The slope is -0.00000109 
- The $y$-intercept is 2.2495

This means that as the quantity demanded increases by 1 unit (in this case, 1 avocado), we would expect to see price to decrease by 0.00000109 units (in this case, \$0.000214).

#### Plotting our demand curve

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
quantities = np.arange(400000, 1600000, 1000)
predicted_prices = slope * quantities + intercept
plt.plot(quantities, predicted_prices, color = 'red', label = "Demand curve")
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```

### Demand with Quantity as a Function of Price

Our interpretation of the demand curve and its slope above was probably not quite intuitive: changes in quantity demanded likely do not trigger changes in price, but instead it is the other way around. In addition, the slope was tiny: the marginal increase of one additional avocado sold had very little effect from the change in price.

Thus, it is more intuitive to think the effect a one dollar change in price has on the quantity demanded, and to flip our axes:

$$D(P) = Q(P) = m\cdot P + b$$

One key thing to remember: our axes are flipped for this demand curve!

#### Fitting our data using this function, we get:

```python
slope, intercept = np.polyfit(avocados.column("Average Price"), avocados.column("Total Volume"), 1)
print("The slope is:", slope)
print("The intercept is:", intercept)
```

Our demand curve is roughly $Q(P) = -476413P+ 1446952$:
- The slope is -476413 
- The $y$-intercept is 1446952

This means that as the price increases by 1 unit (in this case, \$1), we would expect to see quantity demanded to decrease by 476413 units (in this case, 476413 avocados). 

*Note that this demand curve is not the same as the previous demand curve! It is not simply the inverse of the previous demand curve.*

Plotting this line on a graph, we see a slightly different demand curve.

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
prices = np.arange(0.2, 2.3, 0.01)
predicted_quantities = slope * prices + intercept
plt.plot(predicted_quantities, prices, color = 'red', label = "Demand curve")

plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```

### A Large Caveat
So far, we have examined demand curves assuming that they were linear: specifically, we've assumed that the relationship between quantity demanded and price was linear: for a \$1 change in price, we can expect a fixed change in units demanded at any price level.

Is this intuitively true?

As humans, we think about changes as proportions. What this implies is that these curves should be exponential in nature: at higher prices, a larger change in price will yield the same change quantity as compared to that in lower prices. 

Perhaps a better model for demand, then, is that a 1\% change in price will lead to a fixed absolute change in units demanded, or a fixed percentage change in units demanded. 

To model this, we turn to log-log and semi-log demand curves, respectively.

## An Important Aside: Using Logarithms for Proportional Changes

Let's consider how the variable GDP behaves.

GDP tends to grow by a certain percent each year; no one is particularly interested in how *much* GDP changes from one year to the next, or from one country to another, but rather by *what percent* it changes.

If you were to plot GDP over time for a country, it might look something like this:

```python
GDPs = make_array(100)
for _ in np.arange(99):
    GDPs = np.append(GDPs, GDPs.item(-1) * 1.05) 
plt.figure(figsize=(8,6))
plt.plot(np.arange(100), GDPs)
plt.xlabel('Years')
plt.ylabel('GDP')
plt.title('GDP Over Time');
```

This relationship is fundamentally non-linear.

However, for variables that change by proportions we can apply a log transformation to make the relationship linear.

```python
ln_GDPs = np.log(GDPs)

plt.figure(figsize=(8,6))
plt.plot(np.arange(100), ln_GDPs)
plt.xlabel('Years')
plt.ylabel('log GDP')
plt.title('GDP Over Time');
```

We've now uncovered a linear relationship between years and GDP! You can interpret the slope of this line as the approximate *percent change* in GDP for an increase in one year (you will later see why the slope is not exactly 0.05). To verify:

```python
print('Slope between years 0 and 1: ', ln_GDPs[1] - ln_GDPs[0])
```

To generalize our results, taking the natural log of a variable allows us to interpret its change as a percentage change instead of an absolute change:

$$\text{slope} = \frac{\text{Change in GDP %}}{\text{Change in year}} \approx \frac{\text{Change in log-GDP} \times 100}{\text{Change in year}} $$

## Semi-log Demand Curves

Suppose that a change in price by \$1 leads to a m\% change in quantity demanded. 

This means that our slope would be:

$$\text{slope} = \frac{m \text{% change in quantity}}{\text{\$1 change in price}}\approx \frac{m \times 0.01 \text{ change in log-quantity}}{\text{\$1 change in price}} $$

We have to log transform our quantity-demanded variable to capture the above relationship. This is known as the semi-log demand curve, in which the price and log-quantity are linearly related:

$$\ln{D(P)} = m\cdot P + b$$

Let's gain some more intuition of this relationship. By exponentiating both sides, this is equivalent to:

$$\begin{align*}
\ln{D(P)} &= m\cdot P + b \\
D(P) &= e^{m\cdot P + b}\\
&= e^be^{m\cdot P } \\
\end{align*}$$

What do the slope and intercept represent? 

$b$ (specifically $e^b$) corresponds to the 'baseline' quantity demanded when price is 0, since $e^{m \cdot P} = e^0 = 1$. 

$m$ corresponds roughly to how much a one dollar change in price will lead to a percentage change in quantity demanded.

To see this, imagine that P goes up by one dollar such that we have:

$$
\begin{align*}
D(P+1) &= e^be^{m \cdot (P+1) }  \\
&= e^be^{m + m \cdot P }\\
&= e^be^me^{m \cdot P }\\
&= e^mD(P) \\
&\approx (1+m)D(P)
\end{align*}$$

The last line relies on the fact $e^{x} \approx (1+x)$ when $x$ is small. 

Our results leads to the caveat that our transformation is only approximate and only valid when our $m$ is small.

### Visualizing the Semi-log Relationship
Plotting $D(P) = e^be^{m\cdot P}$, we get:

```python
m = -0.05
b = 5
price = np.arange(0,100) 
quantity = (np.e ** (price * m)) * (np.e ** b) 
plt.figure(figsize=(7,5))
plt.plot(quantity, price)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Semi-log Demand Curve');
```

### Fitting Semi-log to Avocados

```python
log_quantity = np.log(avocados.column("Total Volume"))
slope, intercept = np.polyfit(avocados.column("Average Price"), log_quantity, 1)
print("The slope is: ", slope)
print("The intercept is: ", intercept)
```

For every one dollar change in price of avocados, we would expect the change in quantity demanded to decrease by 57%. 

*Take this result with a grain of salt – recall that our approximation typically is valid for small values of $m$, and here our $m=-0.57$*.

#### Plotting on log quantity

```python
plt.scatter(log_quantity, avocados.column("Average Price"))
prices = np.arange(0.5, 2.3, 0.01)
predicted_quantities = slope * prices + intercept
plt.plot(predicted_quantities, prices, color = 'red', label = "Semi-log demand curve")
plt.xlabel("Log Quantity")
plt.ylabel("Price")
plt.legend();
```

#### Plotting without axes transformations

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
prices = np.arange(0.5, 2.3, 0.01)
predicted_quantities = np.e ** (slope * prices + intercept)
plt.plot(predicted_quantities, prices, color = 'red', label = "Semi-log demand curve")
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```

## Log-log Demand Curves

Now suppose that a 1% change in price leads to a m% change in quantity demanded. 

This means that our slope would be:

$$\text{slope} = \frac{m \text{% change in quantity}}{1 \text{% change in price}} \approx \frac{m \times 0.01 \text{ change in log-quantity}}{ 0.01 \text{ change in log-price}} = \frac{\text{change by } m  \text{ in log-quantity}}{\text{ change by 1 in log-price}}$$

In this case, we have to log transform both our quantity-demanded variable as well as price variable to capture the above relationship. This is known as the log-log demand curve, in which the log-price and log-quantity are linearly related:

$$\ln{D(P)} = m\cdot\ln{P} + b$$

Let's gain some more intuition of this relationship. By exponentiating both sides, this is equivalent to:

$$\begin{align*}
D(P) &= e^{m\cdot\ln{P} + b}\\
&= e^be^{m\cdot\ln{P}} \\
&= e^b(e^{\ln{P}})^m \\
&= e^bP^m \\
\end{align*}$$

In this setup, $b$ does not have as clear a meaning. For $m$, we can once again suppose that P goes up by one dollar:

$$\begin{align*}
D(P+1) &= e^b(P+1)^m \\
&\approx e^b (1+m)P^m\\
&\approx (1+m) D(P) 
\end{align*}$$

Where we utilize the approximation that $(P+1)^m \approx P^m \times (1+m)$. Our caveat from the previous section about $m$ being small continues to be in place here: typically, we do not want our $m$ to be larger than $0.2$, or else the approxmation will fall apart.

### Visualizing the Log-log Relationship
Plotting $D(P) = e^bP^m$, we get:

```python
m = -0.05
b = 5
price = np.arange(0,100)
quantity = (price ** m) * (np.e ** b)
plt.figure(figsize=(7,5))
plt.plot(quantity, price)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Log-log Demand Curve');
```

#### A caveat about our log-log model 
Since our model is ultimately linear between log-price and log-quantity, the slope of log-price to log-quantity is always the same. 

This means that at any price level, we assume a 1% change in price will yield the same percentage change in quantity. 

This is also known as fixed elasticities.

### Elasticities

Elasticity is defined: 
$$\varepsilon = \frac{\Delta \% \text{Quantity}}{\Delta \% \text{Price}}$$

Similarly, in point-slope form: 
$$\varepsilon = \frac{\frac{\Delta Q}{Q}}{\frac{\Delta P}{P}} = \frac{\Delta Q}{\Delta P} \frac{P}{Q}$$

Elasticity behaves like the slope in calculus; thus when approximating using the point-slope formula it is typically only valid in small % changes of quantity or price.

Elastic demand: $\varepsilon > 1$
- % change in price leads to a greater % change in quantity
- Profit increases from a decrease in price
- Examples: McDonalds, Toyota Prius, electronic devices

Inelastic demand: $\varepsilon < 1$
- % change in price leads to a smaller % change in quantity
- Profit decreases from a decrease in price
- Examples: insulin, gasoline, cigarettes

Consider the log-log demand curve, which assumes constant elasticity:

```python
m = -0.05
b = 5
price = np.arange(0,100)
quantity = (price ** m) * (np.e ** b)
plt.figure(figsize=(7,5))
plt.plot(quantity, price)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Log-log Demand Curve');
```

Now consider the linear demand curve; which segments are elastic and which are inelastic?

```python
price = np.arange(0,100)
quantity = -1 * price + 100
plt.figure(figsize=(7,5))
plt.plot(quantity, price)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Linear Demand Curve');
```

### Fitting Log-log to Avocados

```python
log_quantity = np.log(avocados.column("Total Volume"))
log_price = np.log(avocados.column("Average Price"))
slope, intercept = np.polyfit(log_price, log_quantity, 1)
print("The slope is: ", slope)
print("The intercept is: ", intercept)
```

For every 1% change in price of avocados, we would expect the change in quantity demanded to decrease by $-0.816\%$. 

Is demand elastic or inelastic?

#### Plotting log-log demand curve with both axes log-transformed

```python
plt.scatter(log_quantity, log_price)
prices = np.arange(-0.2, 0.8, 0.01)
predicted_quantities = slope * prices + intercept
plt.plot(predicted_quantities, prices, color = 'red', label = "Log-log demand curve")

plt.xlabel("Log Quantity")
plt.ylabel("Log Price")
plt.legend();
```

#### Plotting without axes transformations

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
prices = np.arange(0.6, 2.5, 0.01)
predicted_quantities = (np.e ** intercept) * (prices ** slope)
plt.plot(predicted_quantities, prices, color = 'red', label = "Log-log demand curve")
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```

## Which Model is Better: Linear, Semi-log, or Log-log?

There is no correct answer here, in fact justifying one approach over another is surprisingly profound. 

- One way to approach this is to look at the graphs produced above and which red line goes through our data points "best" (but what does "best" mean? We'll save this for another day...)
- Another approach is to utilize our real-world knowledge to conclude which relationship is more accurate: do consumers react similarly to price changes that are in a proportion manner or in a absolute manner? This may also depend on the price, the promotion around it, the product itself, and many other factors.

### An Afterword
This example highlights how ambiguity is a big part of doing data science. We can approach ambiguity with statistical methods and with domain knowledge. Either way, as long as you can ultimately justify your approach, that is what is key in conducting robust data science.

```python

```



--- END lec02/Avocados_demand.md ---



--- START lec02/Demand_Steps_24.md ---

---
title: "Demand_Steps_24"
type: lecture-notebook
week: 2
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/Demand_Steps_24.ipynb"
---

## Demand Curve step by step


We will create a few demand curves based on the class. Start by filling in the form at 
 - https://forms.gle/uxgsxtnedqeANshCA     or 
 -  https://tinyurl.com/data88fa24demand

```python
import pandas as pd
from datascience import *
import numpy as np
%matplotlib inline
```

### Find the Sheet ID in the URL of the Google Sheet!

Take a look at the data 
https://docs.google.com/spreadsheets/d/1jp-XrFPk0eUNDUVWGa7Rmw9b0P8_jobTG0oLpvcHB9s/edit?resourcekey=&gid=418675525#gid=418675525

```python
sheet_id = "1jp-XrFPk0eUNDUVWGa7Rmw9b0P8_jobTG0oLpvcHB9s"
sheet_name = "Form1"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"
```

Read it into a datascience table

```python
demand_table = Table.read_table(url)
demand_table
```

```python
demand_table.ihist("Coffee",bins=7)
```

```python
demand_table.ihist("Burrito",bins=7)
```

## Gonna roll with Burrito for this example

Step 1 - Lets pull out just Burritos 

This is a table with just Burrito prices that people are willing to pay ( bids)

```python
Burrito = demand_table.select("Burrito")
Burrito
```

Step 2 - Let's count the number at each price

And sort the table so that it is descending from high to low price

```python
# count the number at each price
Burrito_counts = demand_table.group("Burrito")
Burrito_counts = Burrito_counts.sort('Burrito', descending=True)

Burrito_counts
```

Step 3 - Let's pull out those counts

```python
counts = Burrito_counts.column("count")
print(counts)
```

Step 4 - use a numpy command called cumulative sum to get the number of people who will buy at each price

```python
cumulative_counts = counts.cumsum()
cumulative_counts
```

Step 5 - make an array of the prices of the burritos in descending order

```python
prices = make_array(20,17.5,15,12.5,10,7.5,5,2.5)
prices
```

Step 6 -  make a table with the prices and the cumulative counts

```python
demand_curve = Table().with_columns("Price", prices, "Cumulative Count", cumulative_counts)
demand_curve
```

```python
demand_curve.iscatter("Cumulative Count","Price")
```

```python
demand_curve.iscatter("Cumulative Count","Price", fit_line=True)
```

```python
# fit a line to the data using numpy        
m, b = np.polyfit(cumulative_counts,prices,  1)
print(m, b)
```

```python
# add a new column to the table with the log of price
demand_curve = demand_curve.with_column("Log Price", np.log(prices))
demand_curve
```

```python

```

```python

```



--- END lec02/Demand_Steps_24.md ---



--- START lec02/PriceElasticity.md ---

---
title: "PriceElasticity"
type: lecture-notebook
week: 2
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/PriceElasticity.ipynb"
---

## Price Elasticity of Demand

```python
def calculate_ped(initial_quantity, new_quantity, initial_price, new_price):
    # Calculate the percentage change in quantity demanded
    percent_change_in_quantity = ((new_quantity - initial_quantity) / initial_quantity) * 100
    
    # Calculate the percentage change in price
    percent_change_in_price = ((new_price - initial_price) / initial_price) * 100
    
    # Calculate the price elasticity of demand (PED)
    ped = percent_change_in_quantity / percent_change_in_price
    
    return ped
```

## Let's start with the Textbook Example
https://data88e.org/textbook/content/01-demand/04-elasticity.html

![](PED-txt.png)

### Example 1

```python

initial_quantity = 5
new_quantity = 4
initial_price = 5
new_price = 6

ped = calculate_ped(initial_quantity, new_quantity, initial_price, new_price)
print(f"The price elasticity of demand is {ped:.2f}")
```

### Example 2

```python
initial_quantity = 4
new_quantity = 5
initial_price = 6
new_price = 5

ped = calculate_ped(initial_quantity, new_quantity, initial_price, new_price)
print(f"The price elasticity of demand is {ped:.2f}")
```

## How about the point slope formula for eslasticity of demand?

Slope = -1 along the line

```python
slope=-1
```

### Example 1  - point slope

```python
initial_quantity = 5
initial_price = 5
ped = slope * (initial_price/initial_quantity)
print(f"The price elasticity of demand is {ped:.2f}")
```

### Example 2  - point slope

```python
initial_quantity = 4
initial_price = 6
ped = slope * (initial_price/initial_quantity)
print(f"The price elasticity of demand is {ped:.2f}")
```

```python

```



--- END lec02/PriceElasticity.md ---



--- START lec02/ScannerData_Beer.md ---

---
title: "ScannerData_Beer"
type: lecture-notebook
week: 2
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/ScannerData_Beer.ipynb"
---

```python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import plotly.express as px
%matplotlib inline
```

```python
try:
  import gdown
except:
  !pip install gdown
  import gdown
```

```python
#This file is too big and appears to crash the Datahub - could potentially work on a local machine with more RAM
#!  gdown 1z0XeoE6PYPOUqzzKLagvmiFohdOF3CLJ
# n_wber.csv = 140M
```

This is a smaller version of the dataset with half of the data ( approx 2m rows) 

The following command pulls the dataset over from Google Drive

```python
#https://drive.google.com/file/d/1qZpdbEpvvWQJlZKpzu8rwd53JrrufzdW/view?usp=sharing
!  gdown 1qZpdbEpvvWQJlZKpzu8rwd53JrrufzdW
# s_wber.csv = 70M
```

# A Quick Look on the Inverse Demand Curves for Beer

Start off by pulling in open source data on beer sales from [The University of Chicago Booth School of Business](https://www.chicagobooth.edu/research/kilts/datasets/dominicks).

```python
df = pd.read_csv('s_wber.csv')
df
```

Can you tell what each instance (row) and the features (columns) represent?

```python
df.describe()
```

One way that I like to approach a dataset where I don't know the specific details of it is to first summarize the whole table, then delve deeper into each feature. Here's a quick example of my approach:

```python
len(df['UPC'].unique()) # Number of Unique beers?
```

```python
len(df['STORE'].unique()) # Number of unique stores
```

```python
len(df['WEEK'].unique()) # Number of unique weeks
```

```python
df['PRICE'].mean() # Average beer price
```

```python
df['PRICE'].max() # most expensive beer
```

```python
df['PRICE'].min() # FREE BEER?!
```

```python
df['QTY'].mean() # avg beers bought ?
```

```python
df['MOVE'].mean() # avg beers bought ?
```

Can you tell what this plot is showing below? I'm not sure if I can!

```python
df[['MOVE','PRICE']].plot();
```

```python
df['QUANTITY'] = df['MOVE'] # Interestingly enough, quantity is not denoted as QTY, by 'MOVE'.
```

By now, we should be aware that we're looking at a dataset of beer sales, where the respective price and quantities for each transaction is represented. Let's filter out all the free beer - although it would be very nice to keep that!

```python
g = df[['QUANTITY','PRICE']]
g = g[g['PRICE']>0]
g
```

To create the demand curve itself, we need to remember that we're looking for quantity demanded at each given price. Hence, we group by price and 'ask' for the sum at the given price. Then, flip that around (just trick), and do the cumulative sum (since a demand curve is cumulative), and then flip it around one last time (since we're looking at the inverse demand curve).

```python
demand = np.flip(np.cumsum(np.flip(g.groupby('PRICE')['QUANTITY'].sum())))
demand = demand.to_frame().reset_index()
demand
```

Now, let's use Plotly Express' [Scatterplot function](https://plotly.com/python-api-reference/generated/plotly.express.scatter.html) to visualize the inverse demand curve for all beers!

```python
px.scatter(demand,x='QUANTITY', y='PRICE', trendline='ols', title='Inverse Demand Curve for all Beers')
```

This is pretty cool! Hover over the line to see the Ordinary Least Squares approximation of the inverse demand curve. What does it tell you?

Now, do you expect the price elasticity of demand to differ for different prices? Yes! Usually, a more expensive good (luxury beers?) tend to have a higher PED. Could we visualize this?

```python
demand.describe()
```

Let's plot multiple demand curves for different price segments of beer. We could start with all beers above the mean. Let's call them expensive.

```python
demand['EXPENSIVE'] = demand['PRICE']>demand['PRICE'].mean()
demand.head(5)
```

Before you plot, think about how this curve might differ from the previous one. Then, check if your intuition was right!

```python
px.scatter(demand,x='QUANTITY', y='PRICE', trendline='ols', color='EXPENSIVE',title='Inverse Demand Curve for Expensive and Cheaper Beer')
```

Did your economic intuition help you? Now, what's happening with the really expensive beer?

```python
demand['REALLY EXPENSIVE'] = demand['PRICE']>10.5 #75th Percentile of Price
demand.head(5)
```

```python
px.scatter(demand,x='QUANTITY', y='PRICE', trendline='ols', color='REALLY EXPENSIVE',title='Inverse Demand Curve for Really Expensive Beer')
```

And the really, really expensive beer?

```python
demand['REALLY, REALLY EXPENSIVE'] = demand['PRICE']>15
demand.head(5)
```

```python
px.scatter(demand,x='QUANTITY', y='PRICE', trendline='ols', color='REALLY, REALLY EXPENSIVE',title='Inverse Demand Curve for Really, Really Expensive Beer is almost Vertical!')
```

This notebook should have given you the data science skills to plot up simple, but powerful inverse demand curves. It should have also gotten you thinking about how demand curves differ for different price segments for the same goods.

Made by Peter F. Grinde-Hollevik.

```python

```



--- END lec02/ScannerData_Beer.md ---



--- START lec02/demand-curve-Fa24.md ---

---
title: "demand-curve-Fa24"
type: lecture-notebook
week: 2
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec02/demand-curve-Fa24.ipynb"
---

# Creating a Demand Curve

```python
import pandas as pd
import os
import json
import numpy as np
from datascience import *
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
%matplotlib inline
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
```

We will create a few demand curves based on the class. Start by filling in the form at https://forms.gle/KhCjC4nwwhtRDjSJ9 or https://tinyurl.com/data-88e-demand!

```python
sheet_id = "1jzgX74fgWo91Dyv7SbD4AmSFvUN5APc79BaOENpsyv8"
sheet_name = "Form1"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"
```

```python
pd.read_csv(url)
```

```python
df_demand=pd.read_csv(url)
```

```python
demand_table = Table.from_df(df_demand)
demand_table = demand_table.drop('Timestamp')
demand_table
```

Let's try graphing all our different responses!

```python
for i in demand_table.labels:
    demand_table.hist(i);
```

Let's start by looking at the demand for masks. How many people would buy masks at a given price? Let's assume that a person would be willing to buy the good at a price less than their bid price.

```python
# This is a column of bid values for masks that you've all inputted. 
masks = demand_table.select('Masks')
masks
```

```python
# This cell does some python magic. You do not need to worry about what's going on. 
prices = pd.DataFrame({'price':[0.25, 0.5, 0.75, 1.00, 1.25, 1.5,1.75,2]})
MasksByPrice = masks.group("Masks")
mbp = MasksByPrice.to_df()
mask = (
    prices
    .merge(mbp, left_on='price', how='left', right_on='Masks')
    .fillna(0).drop('Masks', axis=1)
)
masks_table = Table.from_df(mask)
Q_demand = np.flip(np.cumsum(np.flip(masks_table.group("price", sum).column(1))))
masks_demand = Table().with_columns(
    'price', prices.price, 
    'quantity', Q_demand
)
masks_demand
```

```python
# Let's graph our results
masks_demand.scatter("quantity", "price")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for a pack of surgical masks');
```

Now let's find the slope and intercept of the line of best fit. The cell below defines some functions that you'll learn about in the later portions of Data 8.

```python
std_units = lambda a: (a - np.mean(a)) / np.std(a)
corr = lambda x, y: np.mean(std_units(x) * std_units(y))
slope = lambda x, y: corr(x, y) * np.std(y) / np.std(x)
intercept = lambda x, y: np.mean(y) - slope(x, y) * np.mean(x)
```

```python
slope(masks_demand["quantity"], masks_demand["price"])
```

```python
intercept(masks_demand["quantity"], masks_demand["price"])
```

We can use the same code as above to create demand curves for our other products as well!

```python
#Gourmet Burrito
prices_burrito = pd.DataFrame({'price':[2.50, 5, 7.50, 10, 12.5, 15,17.5,20]})

burritos = demand_table.select('Burrito')
burritosByPrice = burritos.group("Burrito")
bbp = burritosByPrice.to_df()
gb = (
    prices_burrito
    .merge(bbp, left_on='price', how='left', right_on='Burrito')
    .fillna(0).drop('Burrito', axis=1)
)

burritos_table = Table.from_df(gb)
Q_demand_burrito = np.flip(np.cumsum(np.flip(burritos_table.group("price", sum).column(1))))

gb_demand = Table().with_columns(
    'price', prices_burrito.price, 
    'quantity', Q_demand_burrito
)

burrito_slope = slope(gb_demand["quantity"], gb_demand["price"])
burrito_intercept = intercept(gb_demand["quantity"], gb_demand["price"])
print("Slope: " + str(burrito_slope))
print("Intercept: " +  str(burrito_intercept))
```

```python
#Greek Theatre Tickets
prices_tickets = pd.DataFrame({'price':[25, 50, 75, 100, 125, 150,175,200]})

tickets = demand_table.select('GreekTix')
ticketsByPrice = tickets.group("GreekTix")
tbp = ticketsByPrice.to_df()
gt = (
    prices_tickets
    .merge(tbp, left_on='price', how='left', right_on='GreekTix')
    .fillna(0).drop('GreekTix', axis=1)
)

tickets_table = Table.from_df(gt)
Q_demand_tickets = np.flip(np.cumsum(np.flip(tickets_table.group("price", sum).column(1))))

gt_demand = Table().with_columns(
    'price', prices_tickets.price, 
    'quantity', Q_demand_tickets
)

tickets_slope = slope(gt_demand["quantity"], gt_demand["price"])
tickets_intercept = intercept(gt_demand["quantity"], gt_demand["price"])
print("Slope: " + str(tickets_slope))
print("Intercept: " +  str(tickets_intercept))
```

```python
#Iphone 14
prices_iphone = pd.DataFrame({'price':[250, 500, 750, 1000, 1250, 1500,1750,2000, 2250, 2500, 2750, 3000]})

iphones = demand_table.select('iPhone')
iphonesByPrice = iphones.group("iPhone")
ibp = iphonesByPrice.to_df()
iphone14 = (
    prices_iphone
    .merge(ibp, left_on='price', how='left', right_on="iPhone")
    .fillna(0).drop("iPhone", axis=1)
)

iphones_table = Table.from_df(iphone14)
Q_demand_iphones = np.flip(np.cumsum(np.flip(iphones_table.group("price", sum).column(1))))

iphone14_demand = Table().with_columns(
    'price', prices_iphone.price, 
    'quantity', Q_demand_iphones
)

iphones_slope = slope(iphone14_demand["quantity"], iphone14_demand["price"])
iphones_intercept = intercept(iphone14_demand["quantity"], iphone14_demand["price"])
print("Slope: " + str(iphones_slope))
print("Intercept: " +  str(iphones_intercept))
```

Comparing the demand curves for our four products, what similarities or differences do you notice? In particular, think about what the slopes of the curves might reveal to us about consumer preferences.

```python
masks_demand.scatter("quantity", "price")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for a pack of surgical masks');

gb_demand.scatter("quantity", "price")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Gourmet Burritos');

gt_demand.scatter("quantity", "price")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Greek Theatre Tickets');

iphone14_demand.scatter("quantity", "price")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for iPhone14');
```





--- END lec02/demand-curve-Fa24.md ---



--- START lec03/3.0-CubicCostCurve.md ---

---
title: "3.0-CubicCostCurve"
type: lecture-notebook
week: 3
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.0-CubicCostCurve.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
        Shashank Dalmia, Ergun Acikoz</p></td></tr>
</table>

```python
try: 
    from csaps import csaps
except:
    !pip install csaps
    from csaps import csaps
```

```python
import sympy as sp
import matplotlib.pyplot as plt
import numpy as np
from ipywidgets import interact, widgets
from datascience import *
%matplotlib inline
from csaps import csaps
```

# Cubic Cost Functions

The cubic cost function is one that is commonly used to model a cost function within a firm.  

This notebook uses parameters from a recent article  with some parameters for the cubic cost function, in a journal on Teaching Applied Economics

>"Tractable Cubic Cost Functions for Teaching Microeconomics"
>June 21,2023; Applied Economics Teaching Resources (AETR);
>Scott M. Swinton and Hanzhe Zhang

https://www.aaea.org/UserFiles/file/AETR_2021_003RProofFinal.pdf

- d=3600
- c=177
- b=-15
- a=0.5

Let's use these as starting values but then use widgets to allow us to play with the parameters!

```python
# Define the variables and parameters
q = sp.symbols('q')  # Quantity of goods produced

# Define the cubic cost function
a = widgets.FloatSlider(value=0.5, min=0, max=1, step=0.01, description='a')
b = widgets.FloatSlider(value=-15, min=-30, max=30, step=1, description='b')
c = widgets.FloatSlider(value=177.0, min=0, max=250, step=5, description='c')
d = widgets.FloatSlider(value=3600, min=0, max=10000, step=100, description='d')

def plot_cubic_cost_function(a, b, c, d):
    cost_function = a * q**3 + b * q**2 + c * q + d

    cost_function_np = sp.lambdify(q, cost_function, 'numpy')

    quantity_values = np.linspace(0, 25, 100)
    cost_values = cost_function_np(quantity_values)

    plt.figure(figsize=(8, 6))
    plt.plot(quantity_values, cost_values, label='Cost Function', color='blue')
    plt.xlabel('Quantity Produced')
    plt.ylabel('Cost')
    plt.title('Cubic Cost Function')
    plt.legend()
    plt.grid(True)
    plt.show()

interact(plot_cubic_cost_function, a=a, b=b, c=c, d=d)
```

##  Great - we can make a cost function that displays the model characteristics that we want 
**Now let's go ahead and make an table from the Function**

We can use Sympy and get a Numpy version of the equation as well

```python
q = sp.symbols('q') 
cost_function = a * q**3 + b * q**2 + c * q + d


# You dont know how to do this but we can also convert the SymPy expression to a numpy function
cost_function_np = sp.lambdify(q, cost_function, 'numpy')
```

Le'ts make the model  by specifying those cubic parameters that we did in the last section

```python
d=2000
c=177
b=-15
a=0.5
```

```python
quantity_values = np.linspace(0, 30, 16)  # Levels of Q from 0 to 20
quantity_values
```

```python
total_costs = cost_function_np(quantity_values)
```

```python

cost_table = Table().with_columns(
    'Quantity-Q', quantity_values,
    'Fixed Costs-FC', d,
    'Total Costs-TC', total_costs
)
cost_table
```

```python
total_variable_cost = cost_table.column("Total Costs-TC") - cost_table.column('Fixed Costs-FC')
total_variable_cost
```

```python
cost_table = cost_table.with_column("Total Variable Cost-TVC", total_variable_cost)
cost_table
```

```python
average_total_cost = cost_table.column("Total Costs-TC") / cost_table.column("Quantity-Q")
average_total_cost[0] = 0
average_total_cost
```

```python
cost_table = cost_table.with_column("Average Total Cost-ATC", average_total_cost)
cost_table
```

```python
average_variable_cost = cost_table.column("Total Variable Cost-TVC") / cost_table.column("Quantity-Q")
average_variable_cost[0] = 0

cost_table = cost_table.with_column("Average Variable Cost-AVC", average_variable_cost)
cost_table
```

```python
average_fixed_cost = cost_table.column("Fixed Costs-FC") / cost_table.column("Quantity-Q")
average_fixed_cost[0] = 0

cost_table = cost_table.with_column("Average Fixed Cost-AFC", average_fixed_cost)
cost_table
```

## And Finally Marginal Costs

```python
marginal_cost = np.diff(total_costs)
marginal_cost = np.append(make_array(0), marginal_cost)
marginal_cost
```

```python
cost_table = cost_table.with_column("Marginal Cost", marginal_cost)
cost_table
```

```python

```

```python
plt.plot(cost_table.column("Quantity-Q"), cost_table.column("Fixed Costs-FC"), marker='o')
plt.plot(cost_table.column("Quantity-Q"), cost_table.column("Total Variable Cost-TVC"), marker='o')
plt.plot(cost_table.column("Quantity-Q"), cost_table.column("Total Costs-TC"), marker='o')
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('TFC, TVC and TC')
plt.legend(make_array("Total Fixed Cost","Total Variable Cost","Total Cost"))

plt.show()
```

```python
plt.plot(cost_table.column("Quantity-Q")[1:], cost_table.column("Average Fixed Cost-AFC")[1:], marker='o')
plt.plot(cost_table.column("Quantity-Q")[1:], cost_table.column("Average Variable Cost-AVC")[1:], marker='o')
plt.plot(cost_table.column("Quantity-Q")[1:], cost_table.column("Average Total Cost-ATC")[1:], marker='o')
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('AFC, AVC and ATC')
plt.legend(make_array("Average Fixed Cost","Average Variable Cost","Average Total Cost"))

plt.show()
```

```python
# You do not need to understand what the code below is doing. 
output = cost_table.column("Quantity-Q")[1:]
mc = cost_table.column("Marginal Cost")[1:]
avc = cost_table.column("Average Variable Cost-AVC")[1:]
atc = cost_table.column("Average Total Cost-ATC")[1:]

sp_mc = csaps(output, mc, smooth=0.85)
sp_avc = csaps(output, avc, smooth=0.85)
sp_atc = csaps(output, atc, smooth=0.85)

output_s = np.linspace(output.min(), output.max(), 150)
mc_s = sp_mc(output_s)
avc_s = sp_avc(output_s)
atc_s = sp_atc(output_s)

plt.plot(output, mc, marker = 'o', color = 'tab:blue')
plt.plot(output_s, mc_s, alpha=0.7, lw = 2, label='_nolegend_', color = 'tab:blue')
plt.plot(output, avc, marker = 'o', color = 'tab:green')
plt.plot(output_s, avc_s, alpha=0.7, lw = 2, label='_nolegend_', color = 'tab:green')
plt.plot(output, atc, marker = 'o', color = 'tab:orange')
plt.plot(output_s, atc_s, alpha=0.7, lw = 2, label='_nolegend_', color = 'tab:orange')
plt.hlines(y=min(avc), xmin = 11.5, xmax = 13, lw=3, color='r', zorder = 10)
plt.hlines(y=min(atc), xmin = 16, xmax = 18, lw=3, color='r', zorder = 10)
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('MC, AVC and ATC')
plt.legend(make_array("Marginal Cost","Average Variable Cost","Average Total Cost"))

plt.show()
```

```python

```



--- END lec03/3.0-CubicCostCurve.md ---



--- START lec03/3.1-Supply.md ---

---
title: "3.1-Supply"
type: lecture-notebook
week: 3
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.1-Supply.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24<br>
            Dr. Eric Van Dusen <br>
        Shashank Dalmia <br> 
            Ergun Acikoz <br>
            Akhil Venkatesh 
        </p></td></tr>
</table>

# Lecture Notebook 3.1: Supply Curves

```python
try: 
    from csaps import csaps
except:
    !pip install csaps
    from csaps import csaps
```

```python
from datascience import *
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import pandas as pd
from utils import *
import sympy
```

```python
from __future__ import print_function
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from matplotlib import patches
import warnings
warnings.filterwarnings("ignore")
```

## The Supply Curve

The supply of a commodity refers to the quantity for which producers or sellers are willing produce and offer for sale, at a particular price in some given period of time.

To answer questions like *"at a given price, what will be the supply of a good in the market?"*, we need to know the market supply curve. A supply curve is simply a curve (or graph) which shows the quantites of a good that can be produced and the prices they will be sold at.

It is good to discern between individual and market supply. **Individual supply** refers to the supply offered by a single firm or producer, while **market supply** refers to the supply offered by all the firms or producers in a market. It is the horizontal summation of the individual supply curves in the market.

The following table and graph will give an example of a market with two firm: A and B.

```python
market_supply = Table().with_columns("Price", make_array(2, 3, 4),
                                     "Quantity supplied by A", make_array(20, 30, 40),
                                     "Quantity supplied by B", make_array(30, 40, 50),
                                     "Market Supply", make_array(50, 70, 90))
market_supply
```

```python
plt.plot(market_supply.column(1), market_supply.column(0), marker='o')
plt.plot(market_supply.column(2), market_supply.column(0), marker='o')
plt.plot(market_supply.column(3), market_supply.column(0), marker='o')
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Market Supply')
plt.legend(make_array("Quantity supplied by A","Quantity supplied by B","Market Supply"), bbox_to_anchor=(1.04,1), loc="center left")

plt.show()
```

Market behaviour relating to supply is based on the behaviour of the individual firms that comprise it. Now, how does an individual firm make its decision about production?

It does so based on the costs associated with production. If the price of a good is enough to recover the costs, the firm produces. Generally, costs increase with the quantity of production. So, to induce producers to increase the quantity supplied, the prices need to increase to compensate for the increased costs.

## Costs and Firm Behavior

We will split costs into two categories: **Fixed costs** and **Variable costs**.

Fixed Costs are costs associated with fixed factors (or inputs) of production. For example, land for a factory, capital equipment like machinery, etc. The quantity of these inputs cannot be changed quickly in the short term. A factory owner cannot purchase land quickly enough to ramp up production in a week. A key point to note is that fixed costs are irrespective of the quantity, i.e., they do not change with the quantity produced.

Variable Costs are costs associated with variable factors (or inputs) of production. For example, labor, raw materials, etc. The quantity of these inputs can be changed quickly in the short term to adjust supply. A factory owner can hire more laborers or purchase more raw material to increase output. Variable costs change as the supply changes.

We will create a table with the following firm costs:

* **Output:** Units produced and supplied
* **Total Fixed Cost (TFC):** Cost incurred by firm on usage of all fixed factors.
* **Total Variable Cost (TVC):** Cost incurred by firm on usage of all variable factors.
* **Total Cost (TC):** Sum of the total fixed and variable costs
* **Marginal Cost (MC):** Addition to total cost as one more unit of output is produced
* **Average Fixed Cost (AFC):** Cost per unit of fixed factors
* **Average Variable Cost (AVC):** Cost per unit of variable factors
* **Average Total Cost (ATC):** Total cost per unit

```python
individual_firm_costs = Table.read_table('individual_firm_costs.csv')
individual_firm_costs.show()
```

First, let's calculate `total_cost`, which is the sum of total fixed cost and total variable cost.

```python
total_cost = individual_firm_costs.column("Total Fixed Cost") + individual_firm_costs.column("Total Variable Cost")
total_cost
```

We will now add the total costs array to the table.

```python
individual_firm_costs = individual_firm_costs.with_column("Total Cost", total_cost)
individual_firm_costs
```

Average Fixed Cost can be calculated as the Total Fixed Costs divided by the output.

 At zero level of output, we would by dividing by zero, which is invalid.
 So we have to manually fix that

```python

average_fixed_cost = individual_firm_costs.column("Total Fixed Cost") / individual_firm_costs.column("Output")
average_fixed_cost[0] = 0
average_fixed_cost
```

Now we will add the AFC column back into the table.

```python
individual_firm_costs = individual_firm_costs.with_column("Average Fixed Cost", average_fixed_cost)
individual_firm_costs
```

Similarly, Average Variable Cost can be calculated as the Total Variable Cost divided by the output.

```python
average_variable_cost = individual_firm_costs.column("Total Variable Cost")/individual_firm_costs.column("Output")
average_variable_cost[0] = 0
average_variable_cost
```

Now we will add the AVC column to the table.

```python
individual_firm_costs = individual_firm_costs.with_column("Average Variable Cost", average_variable_cost)
individual_firm_costs
```

Similarly, Average Total Cost can be calculated as the Total Cost divided by the output.

```python
average_total_cost = individual_firm_costs.column("Total Cost")/individual_firm_costs.column("Output")
average_total_cost[0] = 0
average_total_cost
```

```python
individual_firm_costs = individual_firm_costs.with_column("Average Total Cost", average_total_cost)
individual_firm_costs
```

Marginal Cost can be calculated as the difference between Total Cost at the current output level and Total Cost at the previous output level (or TVC, as TFC is fixed).

For this we are going to use the function `np.diff`. You can read about it on http://data8.org/fa22/reference/#array-functions-and-methods

```python
marginal_cost = np.diff(total_cost)
marginal_cost = np.append(make_array(0), marginal_cost)
marginal_cost
```

Once again, we add the MC column back to the table.

```python
individual_firm_costs = individual_firm_costs.with_column("Marginal Cost", marginal_cost)
individual_firm_costs
```

Let's look at some plots!

```python
plt.plot(individual_firm_costs.column("Output"), individual_firm_costs.column("Total Fixed Cost"), marker='o')
plt.plot(individual_firm_costs.column("Output"), individual_firm_costs.column("Total Variable Cost"), marker='o')
plt.plot(individual_firm_costs.column("Output"), individual_firm_costs.column("Total Cost"), marker='o')
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('TFC, TVC and TC')
plt.legend(make_array("Total Fixed Cost","Total Variable Cost","Total Cost"))

plt.show()
```

What have we observed?

1. TFC is flat
2. Vertical difference between TVC and TC is TFC, which is constant

```python
plt.plot(individual_firm_costs.column("Output")[1:], individual_firm_costs.column("Average Fixed Cost")[1:], marker='o')
plt.plot(individual_firm_costs.column("Output")[1:], individual_firm_costs.column("Average Variable Cost")[1:], marker='o')
plt.plot(individual_firm_costs.column("Output")[1:], individual_firm_costs.column("Average Total Cost")[1:], marker='o')
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('AFC, AVC and ATC')
plt.legend(make_array("Average Fixed Cost","Average Variable Cost","Average Total Cost"))

plt.show()
```

```python
# You do not need to understand what the code below is doing. 
output = individual_firm_costs.column("Output")[1:]
mc = individual_firm_costs.column("Marginal Cost")[1:]
avc = individual_firm_costs.column("Average Variable Cost")[1:]
atc = individual_firm_costs.column("Average Total Cost")[1:]

sp_mc = csaps(output, mc, smooth=0.85)
sp_avc = csaps(output, avc, smooth=0.85)
sp_atc = csaps(output, atc, smooth=0.85)

output_s = np.linspace(output.min(), output.max(), 150)
mc_s = sp_mc(output_s)
avc_s = sp_avc(output_s)
atc_s = sp_atc(output_s)

plt.plot(output, mc, marker = 'o', color = 'tab:blue')
plt.plot(output_s, mc_s, alpha=0.7, lw = 2, label='_nolegend_', color = 'tab:blue')
plt.plot(output, avc, marker = 'o', color = 'tab:green')
plt.plot(output_s, avc_s, alpha=0.7, lw = 2, label='_nolegend_', color = 'tab:green')
plt.plot(output, atc, marker = 'o', color = 'tab:orange')
plt.plot(output_s, atc_s, alpha=0.7, lw = 2, label='_nolegend_', color = 'tab:orange')
plt.hlines(y=min(avc), xmin = 6, xmax = 8, lw=3, color='r', zorder = 10)
plt.hlines(y=min(atc), xmin = 7.5, xmax = 9.5, lw=3, color='r', zorder = 10)
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('MC, AVC and ATC')
plt.legend(make_array("Marginal Cost","Average Variable Cost","Average Total Cost"))

plt.show()
```

Notice something important: _A company decides to produce if it the price is greater than or equal to its Average Variable Cost._

There are 3 different scenarios:
1. A firm chooses to not produce at all
2. A firm chooses to produce at a loss (minimizing quantity)
3. A firm chooses to produce at a profit. 

Play around with the slider to see them.

```python
interact(lambda price: firm_behaviour(price, individual_firm_costs), price=widgets.IntSlider(min=20,max=60,step=1,value=25));
```

In the above example if the price is 24 for any amount of production, the firm will lose money. In this case, they shut down and the loss is limited to its fixed costs.

If the price is above the minimum of the AVC (25 in the example), for some amount of production, the firm will maximise its profits (i.e. minimise its losses). Profits are Total Revenue minus Total Costs, where Total Revenue is Price times Quantity.

Therefore, based on the price, each firm looks at its costs and makes a decision on whether to produce. At low prices, only the firms with the lowest production costs produce. As the price increases, firms with higher production costs find it feasible to produce and begin to supply. Thus, the market supply rises with higher prices. Firms with lower costs make extra profits.

```python

```

```python

```



--- END lec03/3.1-Supply.md ---



--- START lec03/3.2-sympy.md ---

---
title: "3.2-sympy"
type: lecture-notebook
week: 3
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.2-sympy.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24<br>
            Dr. Eric Van Dusen <br>
        <br>
</table>

# Lecture Notebook 3.2: SymPy & Market Equilibrium

# SymPy - Symbolic Python

```python
from datascience import *
import numpy as np
from sympy import *
from sympy import solve
import sympy
init_printing()
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as patches
%matplotlib inline
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
mpl.rcParams['figure.dpi'] = 150
import warnings
warnings.filterwarnings('ignore')
```

Python has many tools, such as the [SymPy library](https://docs.sympy.org/latest/tutorial/index.html) that we can use for expressing and evaluating formulas and functions in economics. 

Since SymPy helps with symbolic math, we start out by create a symbol using `Symbol`, which we assign to a variable name. Then, we can use the symbols to construct symbolic expressions.

```python
x = Symbol('x');
x
```

**Example 1**: Now let's try using SymPy to create a symbolic expression for 2 equations. Let's create the equation for the first curve. It will be called $y$ and will be in terms of `x`

```python
y = 2 * x - 3
y
```

Similarly, we will also use `x` to express a relationship with $y_1$

```python
y_1 = 2 - x
y_1
```

We want to see the value of `x` at which $y = y_1$. To solve this by hand, we would set up the following equation to solve for $Q$:

$$
P_D = P_S\\
2-Q = 2Q-3
$$

Before we try solving this with SymPy, let's solve this by hand. 

$$ y = y_1 $$
$$ 2x - 3 = 2 - x $$
$$ 3x = 5 $$
$$ x = \frac{5}{3} $$

We can confirm our `Q` value is true by plugging it back into the original equation. 

$$ y = 2x - 3 $$
$$ y = 2 \times \frac{5}{3} - 3 $$
$$ y = \frac{1}{3} $$

$$ y_1 = 2 - x $$
$$ y_1 = 2 - (-\frac{5}{3}) $$
$$ y_1 = \frac{1}{3} $$

Using SymPy, we call `solve`, which takes in 2 arguments that represent the 2 sides of an equation and solves for the underlying variable such that the equation holds. Here, we pass in $P_D$ and $P_S$, both represented in terms of $Q$, to solve for the value of $Q$ such that $P_D=P_S$. It's good to know that `solve` is a custom function built for this class, and will be provided in the notebooks for you.

```python
x_star = solve(y, y_1)
x_star
```

The value of $x$ that equates $y$ and $y_1$ is known as the optimal x value, and we denote it as $x^*$. Here, $x^* = \frac{5}{3}$. 

With $x^*$ determined, we can substitute this value as $x$ to thus calculate $y$ or $y_1$. We substitute values using the `subs` function, which follows the syntax `expression.subs(symbol_we_want_to_substitute, value_to_substitute_with)`. This let's us see if our answer was correct.

```python
y.subs(x, x_star)
```

We can also substitute $x^*$ into $y_1$, and should get the same results.

```python
y_1.subs(x, x_star)
```

Thus, the equilibrium `x` and `y` values are $\frac{5}{3}$ and 0.33, respectively. 

**Example 2** We will now put this in the context of suppply and demand. We can represent both as non-log functions. 

To define an upward sloping supply curve with price expressed as a function of quantity, let's define `Q` first.

```python
Q = Symbol('Q')
Q
```

Suppose our demand function is $\text{P}_{D}=-2 \cdot \text{Q} + 10$. $P_D$ is the price of the good purchased (how much the consumer pays). Using SymPy, this would be

```python
P_D = -2 * Q + 10
P_D
```

The supply function is $\text{P}_{S}=3 \cdot \text{Q} + 1$. which denotes the price of the supplied good (how much the producer earns), in terms of $Q$.Using SymPy, this would be

```python
P_S = 3 * Q + 1
P_S
```

We will now try to find the market equilibrium. The market price equilibrium $P^*$ is the price at which the quantity supplied and quantity demanded of a good or service are equal to each other. Similarly, the market quantity equilibrium $Q^*$ is the quantity at which the price paid by consumers is equal to the price received by producers. 

To solve for the equilibrium given the supply and demand curve, we know that the price paid by consumers must equal to the price earned by suppliers. Thus, $P_D = P_S$, allowing us to set the two equations equal to each other and solve for the equilibrium quantity and thus equilibrium price.

Combined, the price equilibrium and quantity equilibrium form a point on the graph with quantity and price as its axes, called the equilibrium point. This point is the point at which the demand and supply curves intersect.

Let's take a step back and solve this by hand first.

$$ P_D = P_S $$
$$ -2Q + 10 = 3Q + 1 $$
$$ 5Q = 9 $$
$$ Q = \frac{9}{5} $$

Now that we have our optimal Q value, let's plug it in each equation and see if our condition holds true. 

$$ P_D = -2Q + 10 $$
$$ P_D = -2 \times \frac{9}{5} + 10 $$
$$ P_D = \frac{32}{5} = 6.4 $$

$$ P_S = 3Q + 1 $$
$$ P_S = 3 \times \frac{9}{5} + 1 $$
$$ P_S = \frac{32}{5} = 6.4 $$

Now, let's use SymPy

First, we solve for the quantity equilibrium.

```python
Q_star = solve(P_D, P_S)
Q_star
```

Next, we plug the quantity equilibrium into our demand or supply expression to get the price equilibrium

```python
P_S.subs(Q, Q_star)
```

Graphically, we can plot the supply and demand curves with quantity on the $x$ axis and price on the $y$ axis. The point at which they intersect is the equilibrium point.

```python
def plot_equation(equation, price_start, price_end, label=None):
    plot_prices = [price_start, price_end]
    plot_quantities = [equation.subs(list(equation.free_symbols)[0], c) for c in plot_prices]
    plt.plot(plot_prices, plot_quantities, label=label)
    
def plot_intercept(eq1, eq2):
    ex = sympy.solve(eq1-eq2)[0]
    why = eq1.subs(list(eq1.free_symbols)[0], ex)
    plt.scatter([ex], [why])
    return (ex, why)
    
plot_equation(P_D, 0, 5)
plot_equation(P_S, 0, 5)
plt.ylim(0,20)
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.title("Supply & Demand with SymPy")
plot_intercept(P_D, P_S);
```

# Market Equilibrium with Real Data

We will now explore the relationship between price and quantity of oranges produced between 1924 and 1938. Since the [data](https://data-88e.github.io/textbook/content/references.html#demand-fruits) is from the 1920s and 1930s, it is important to remember that the prices are much lower than what they would be today because of inflation, competition, innovations, and other factors. For example, in 1924, a ton of oranges would have costed $\$6.63$; that same amount in 2019 is \$100.78, holding all factors except for inflation constant. 

Remember, the **market equilibrium** is the price and quantity at which the demand and supply curves intersect. The price and resulting transaction quantity at the equilibrium is what we would predict to observe in the market. This is the point at which the demand and supply curves meet and represents the "optimal" level of production and price in that market.

```python
fruitprice = Table.read_table('fruitprice.csv')
fruitprice
```

Let's walk through how to the market equilibrium using the market for oranges as an example.

### Data Preprocessing

Because we are only examining the relationship between prices and quantity for oranges, we can create a new table with the relevant columns: `Year`, `Orange Price`, and `Orange Unloads`. Here, `Orange Price` is measured in dollars, while `Orange Unloads` is measured in tons.

```python
oranges_raw = fruitprice.select("Year", "Orange Price", "Orange Unloads")
oranges_raw
```

Next, we will rename our columns. In this case, let's rename `Orange Unloads` to `Quantity` and `Orange Price` to `Price` for brevity and understandability.

```python
oranges = oranges_raw.relabel("Orange Unloads", "Quantity").relabel("Orange Price", "Price")
oranges
```

### Visualize the  Relationship

Let's first take a look to see what the relationship between price and quantity is. We would expect to see a downward-sloping relationship between price and quantity; if a product's price increases, consumers will purchase less, and if a product's price decreases, then consumers will purchase more. 

We will create a scatterplot between the points.

```python
oranges.scatter("Quantity", "Price", width=3, height=3)
plt.title("Demand Curve for Oranges", fontsize = 16);
```

### Fit a Polynomial

We will now quantify our demand curve using NumPy's [`np.polyfit` function](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). Recall that `np.polyfit` returns an array of size 2, where the first element is the slope and the second is the $y$-intercept.

For this exercise, we will be expressing demand and supply as quantities in terms of price.

```python
orange_parameters = np.polyfit(oranges.column("Price"), oranges.column("Quantity"), 1)
print ("The coefficient is", orange_parameters[0])
print ("The y-intercept is", orange_parameters[1])
```

This shows that the demand curve is $D(P) = -3433 P+ 53626$. The slope is -3433 and $y$-intercept is 53626. That means that as price increases by 1 unit (in this case, \$1), quantity decreases by 3433 units (in this case, 3433 tons).

### Create the Demand Curve

We will now use SymPy to write out this demand curve. To do so, we start by creating a symbol `P` that we can use to create the equation.

```python
P = sympy.Symbol("P")
demand = orange_parameters[0] * P + orange_parameters[1]
demand
```

### Create the Supply Curve

As the price of the oranges increases, the quantity of oranges that orange manufacturers are willing to supply increases. They capture the producer's side of market decisions and are upward-sloping.

Let's now assume that the supply curve is given by $S(P) = 4348P$. (Note that this supply curve is not based on data.)

```python
supply_coefficient = 4348
supply = supply_coefficient * P
supply
```

This means that as the price of oranges increases by 1, the quantity supplied increases by 4348. At a price of 0, no oranges are supplied.

### Find the Price Equilibrium
The equilbrium consists of 2 components: the quantity equilbrium and price equilbrium. 
The price equilibrium is the price at which the supply curve and demand curve intersect: the price of the good that consumers desire to purchase at is equivalent to the price of the good that producers want to sell at. There is no shortage of surplus of the product at this price.


Let's find the price equilibrium. To do this, we will use the provided `solve` function. This is a custom function that leverages some SymPy magic and will be provided to you in assignments.

```python
P_star = solve(demand, supply)
P_star
```

This means that the price of oranges that consumers want to purchase at and producers want to provide is about \$6.89.

### Find the Quantity Equilibrium

Similarly, the quantity equilibrium is the quantity of the good that consumers desire to purchase is equivalent to the quantity of the good that producers supply; there is no shortage or surplus of the good at this quantity.

```python
demand.subs(P, P_star)
```

```python
supply.subs(P, P_star)
```

This means that the number of tons of oranges that consumers want to purchase and producers want to provide in this market is about 29,967 tons of oranges.

### Visualize the Market Equilibrium 

Now that we have our demand and supply curves and price and quantity equilibria, we can visualize them on a graph to see what they look like.

```python
def plot_equation(equation, price_start, price_end, label=None):
    plot_prices = [price_start, price_end]
    plot_quantities = [equation.subs(list(equation.free_symbols)[0], c) for c in plot_prices]
    plt.plot(plot_quantities, plot_prices, label=label)
    
def plot_intercept(eq1, eq2):
    ex = sympy.solve(eq1-eq2)[0]
    why = eq1.subs(list(eq1.free_symbols)[0], ex)
    plt.scatter([why], [ex], zorder=10, color="tab:orange")
    return (ex, why)
```

We can leverage these functions and the equations we made earlier to create a graph that shows the market equilibrium.

```python
plot_equation(demand, 2, 12, label = "Demand")
plot_equation(supply, 2, 12, label = "Supply")
plt.ylim(0,13)
plt.title("Orange Supply and Demand in 1920's and 1930's", fontsize = 15)
plt.xlabel("Quantity (Tons)", fontsize = 14)
plt.ylabel("Price ($)", fontsize = 14)
plot_intercept(supply, demand)
plt.legend(loc = "upper right", fontsize = 12)
plt.show()
```

You can also practice on your own and download additional data sets [here](http://users.stat.ufl.edu/~winner/datasets.html), courtesy of the University of Flordia's Statistics Department.





--- END lec03/3.2-sympy.md ---



--- START lec03/3.3a-california-energy.md ---

---
title: "3.3a-california-energy"
type: lecture-notebook
week: 3
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.3a-california-energy.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24<br>
            Dr. Eric Van Dusen <br>
        Shashank Dalmia <br> 
            Ergun Acikoz <br>
            Akhil Venkatesh 
        </p></td></tr>
</table>

### Let's look at a real life example! This comes from **EEP 147 Regulation of Energy and the Environment**. 

The class plays a game called the Energy Strategy Game.  The dataset comes from that game.

```python
from datascience import *
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
%matplotlib inline
import matplotlib.pyplot as plt
from utils import *
```

```python
ESG_table = Table.read_table('ESGPorfolios_forcsv.csv').select(
    "Group", "Group_num", "UNIT NAME", "Capacity_MW", "Total_Var_Cost_USDperMWH").sort(
    "Total_Var_Cost_USDperMWH", descending = False).relabel(4, "Average Variable Cost")
```

```python
ESG_table
```

This table shows some electricity generation plants in California and their costs. The Capacity is the output the firm is capable of producing. The Average Variable Cost shows the minimum variable cost per MW produced. At a price below AVC, the firm supplies nothing. At a price above the AVC, the firm can supply up to its capacity. Being a profit-maximising firm, it will try to supply its full capacity.

First, let's look at just the Big Coal producers, a portfolio of electricity producing plants, and understand the data.

```python
selection = 'Big Coal'
Group = ESG_table.where("Group", selection)
```

```python
Group
```

```python
# Make the plot
plt.figure(figsize=(9,6))
plt.bar(new_x_group, height_group, width=width_group, edgecolor = "black")
# Add title and axis names
plt.title(selection)
plt.xlabel('Capacity_MW')
plt.ylabel('Variable Cost/Price')

plt.show()
```

This figure is the Big Coal Supply curve. It shows the price of electricity, and the quantity supplied at those prices (which depends on Variable Cost). For example, at any Variable Cost at or above 36.5, the producer FOUR CORNERS	(the one with the lowest production costs) will supply, and so on.

Let's interact with it by changing the market price.

```python
interact(group_plot, price=widgets.IntSlider(min=20,max=80,step=1,value=37));
```

We are going to repeat the same process, this time for all the energy sources. They have been colored according to production group.

```python
interact(ESG_plot, price=widgets.IntSlider(min=0,max=90,step=1,value=37));
```

### Energy manipulation for profit (In class demo)

```python
def modified_profit(price, tbl):
    tbl = tbl.where("Average Variable Cost", are.below_or_equal_to(price))
    profit_per_unit = price - tbl.column("Average Variable Cost") 
    profit_per_plant = profit_per_unit * tbl.column("Capacity_MW")
    tbl = tbl.with_columns("profit_per_unit", profit_per_unit, 
                          "profit_per_plant", profit_per_plant)
    print("Price", price)
    print("Total Profit", sum(profit_per_plant))
    return tbl
```

```python
Group
```

```python
modified_profit(45, Group)
```

```python
Group_wo_hun = Group.take(make_array(0,2,3, 4,5))
Group_wo_hun
```

```python
interact(ESG_plot_wo_hun, price=widgets.IntSlider(min=0,max=90,step=1,value=37));
```

```python
modified_profit(50, Group_wo_hun)
```







--- END lec03/3.3a-california-energy.md ---



--- START lec03/3.3b-a-really-hot-tuesday.md ---

---
title: "3.3b-a-really-hot-tuesday"
type: lecture-notebook
week: 3
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec03/3.3b-a-really-hot-tuesday.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24<br>
            Dr. Eric Van Dusen <br>
            Peter F. Grinde-Hollevik <br>
            <br>
</table>

# Lecture Notebook 3.3b: A Really Hot Tuesday & Renewable Energy Portfolio

First we will start off looking at the Renewable Energy in the California Grid

```python
import pandas as pd
import numpy as np
import plotly.express as px
```

```python
cds = pd.read_csv('CAISO_2017to2018_stack.csv', index_col=0)
cds.head()
```

```python
cds = cds.rename({'Unnamed: 0':'dt'}, axis=1)
cds
```

```python
cds_date = pd.to_datetime(cds.index)
cds_date
```

```python
cds['hour'] = cds_date.hour
cds
```

```python
cds_piv = cds.pivot_table(
    values = 'MWh',
    index = 'hour',
    columns = 'Source')
cds_piv
```

```python
cds_piv.plot()
```

```python
px.line(cds_piv)
```

```python
cds_piv['total'] = cds_piv.sum(axis=1
                              )
cds_piv
```

```python
px.line(cds_piv['total'])
```

```python
cds['month'] = cds_date.month
cds
```

```python
cds_piv = cds.pivot_table(
    values='MWh',
    index = 'month',
    columns = 'Source',
    aggfunc='max')
cds_piv
```

```python
px.line(cds_piv)
```

## Taking a Closer look at  Tuesday, September 6th, 2022 Demand for Electricity

![Screenshot%202022-09-09%20at%2014.16.41.png](attachment:Screenshot%202022-09-09%20at%2014.16.41.png)

```python
hot_tues = pd.read_csv('CAISO-demand-20220906.csv')
hot_tues
```

```python
ht = hot_tues.T.iloc[1:,:]
to_plot = ht.iloc[:, 0:3]
to_plot = to_plot.rename({0:'Day Ahead Forecast', 1: 'Hour Ahead Forecast', 2:'Demand'}, axis=1)
to_plot
```

```python
px.line(to_plot, title='Electricity Demand Hot Tuesday').update_layout(
    xaxis_title="Time", yaxis_title="MWh"
)
```

```python

```

```python

```



--- END lec03/3.3b-a-really-hot-tuesday.md ---



--- START lec04/lec04-CSfromSurvey-closed.md ---

---
title: "lec04-CSfromSurvey-closed"
type: lecture-notebook
week: 4
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-CSfromSurvey-closed.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
        Kidong Kim</p></td></tr>
</table>

# Lecture 4: Demand Survey and Surplus #

The idea for this demo is to use the student's demand curve to motivate the concept of surplus.

```python
from datascience import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
%matplotlib inline

import sympy
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"

from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
import warnings
warnings.filterwarnings("ignore")
```

## Section 1: Market Demand and Consumer Surplus

Let's start off with the demand from a student demand survey:
 - We had 4 "goods" and a range of prices available for each good
 - Students made bids on their willingness to pay for each of the 4 goods
 - The dataset for Fall 2022 has ~100 observations
 - This dataset has been exported and we read it in below:

```python
# sheet_id = "1jzgX74fgWo91Dyv7SbD4AmSFvUN5APc79BaOENpsyv8"
# sheet_name = "Form1"
# url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"

sheet_id = "1jzgX74fgWo91Dyv7SbD4AmSFvUN5APc79BaOENpsyv8"
sheet_name = "Form1"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"


df_demand=pd.read_csv(url)

DemandTable = Table.from_df(df_demand)
DemandTable = DemandTable.drop('Timestamp')
DemandTable
```

```python
for i in DemandTable.labels:
    DemandTable.ihist(i, bins=7);
```

**Let's focus on the burritos first.** How many people are willing to buying a gourment burrito at any given price?   
We can assume that a person would be willing to buy the good at a price less than their bid price.

```python
BurritosTable = DemandTable.select('Burrito')
BurritosTable
```

```python
# Count how many people are in each answer pool
BurritosTable.group("Burrito")
```

```python
# Create a bar plot
table = BurritosTable.group("Burrito")

def plot_histogram(data, bins, title="Title", x_label = "Price", y_label = "Count"):
    plt.bar(bins, data, edgecolor="brown", align="center", width = 2)
    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.show()
    return 

burrito_bins = table.column(0) # Select column using method call
burrito_data = table['count'] # Select column using indexing
burrito_title = "Demand of Burritos according to different prices"

plot_histogram(burrito_data, burrito_bins, burrito_title)
```

In the visualization above, the height of each bar isn't quite right - someone who is willing to pay \\$10 for a burrito will also pay \\$2.5 for the same burrito.

```python
Qdemand = np.flip(np.cumsum(np.flip(BurritosTable.group("Burrito").column("count"))))
Qdemand
```

```python
DemandBurr= Table().with_columns([
    'priceBurr', [2.5, 5, 7.5, 10.00, 12.5, 15,17.5, 20], # those are the prices
    'Qdemand', Qdemand
])
DemandBurr
```

```python
burrito_Qdemand_bins = DemandBurr.column('priceBurr') # Select column using method call
burrito_Qdemand_data = DemandBurr['Qdemand'] # Select column using indexing
burrito_Qdemand_title = "Quantity demanded of Burritos with different prices"
Qdemand_x_label = "Price of Burrito"
Qdemand_y_label = "Quantity demanded"

plot_histogram(burrito_Qdemand_data, burrito_Qdemand_bins, burrito_Qdemand_title, Qdemand_x_label, Qdemand_y_label)
```

### Let's take a look at this table and think about the Consumer Surplus

First, let's sort the table from the most expensive burritos to the least expensive ones. 

Then, if the price is \$10, how many people are willing to pay more than the price? These people would be getting a **surplus** by only having to pay a cheaper price than the one they would be willing to pay.

```python
DemandBurr.sort("priceBurr", descending = True)
```

It looks like 
- 52 people would have been willing to pay up to \\$12.5
- 25 people would have been willing to pay up to \\$15.0
- 8 person would have been willing to pay up to \\$17.5
- 4 people would have been willing to pay up to \\$20.0

Let's add up these values

```python
CS_counting = 52*(12.5-10)+25*(15-10)+8*(17.5-10)+4*(20-10)
print('The consumer surplus from counting consumers is', CS_counting)
```

```python
# calculate the total consumer surplus given a demand table and the price of the good
def consumer_surplus(demand_table, price):
    
    # only people with a willingness to pay higher than the market price will buy the good
    demand_table_in_market = demand_table.where(0, are.above_or_equal_to(price))
    
    cs = (demand_table_in_market.column(0) - price) * demand_table_in_market.column(1)
    total_cs = sum(cs)
    
    return total_cs
```

```python
cs_burrito = consumer_surplus(DemandBurr, 11)
print('The consumer surplus from counting consumers is', cs_burrito)
```

How can we visualize the consumer surplus on the demand and supply diagram? We'll start by creating a demand curve first like before.

```python
DemandBurr.scatter("Qdemand", "priceBurr")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Gourmet Burrito');
```

```python
DemandBurr.plot("Qdemand", "priceBurr", linewidth= 3)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Gourmet Burrito');
```

Now that we have a demand curve - Let's create for a model that makes a linear approximation like we did in lecture 2.

```python
DemandGM = np.polyfit(DemandBurr.column("Qdemand"), DemandBurr.column("priceBurr"),1)
DemandGM
```

```python
burr_slope = DemandGM.item(0)
burr_slope
```

```python
burr_intercept = DemandGM.item(1)
burr_intercept
```

```python
# plot the actual demand curve
DemandBurr.plot("Qdemand", "priceBurr", linewidth=3)

# plot the linear approximation
burr_quantities = np.arange(0,120,0.01)
burr_prices = burr_slope * burr_quantities + burr_intercept
plt.plot(burr_quantities, burr_prices, linewidth=3)

plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Burrito');
```

How many people does the model think would buy burritos when its price is \\$10? Let's answer this question using sympy.

```python
# Set up the demand curve expression 
Q = sympy.Symbol("Q")
demand = burr_slope * Q + burr_intercept

# Solve for Q_star when price is 10
Q_Star = solve(demand, 10)
Q_Star
```

Now we will visualize the consumer surplus of the burrito market.

```python
DemandBurr.plot("Qdemand", "priceBurr", linewidth= 3) #Black : Demand for buritto

plt.plot(np.arange(0,82,0.01), burr_slope * np.arange(0,82,0.01) + burr_intercept, linewidth= 3) #Blue : 

price = 10
plt.plot([0,Q_Star],[price, price], color = 'r', linewidth= 3) #Red : Price

triangle1 = patches.Polygon([[0,10],[Q_Star,10],[0,burr_intercept]],closed=True,color="green") #Consumer surplus
currentAxis = plt.gca()
currentAxis.add_patch(triangle1)

plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Burrito')

burr_price = 10

# Code for Slope and Intercept - What are the slope and intercept of the fit line
std_units = lambda a: (a - np.mean(a)) / np.std(a)
corr = lambda x, y: np.mean(std_units(x) * std_units(y))
slope = lambda x, y: corr(x, y) * np.std(y) / np.std(x)
intercept = lambda x, y: np.mean(y) - slope(x, y) * np.mean(x)



# Sums up the surplus at the give price
def surplus(bins, data, price):
    #Finding the quatity at the point where red line indicating surplus and blue line indicating the relationship btw quantiy and price.
    slope_sur = slope(data, bins)
    intercept_sur = intercept(data, bins)
    Q = sympy.Symbol("Q")
    demand = slope_sur * Q + intercept_sur
    Q_Star = solve(demand, price)
    
    #Sums up the surplus and print it out.
    total_surplus = .5 * Q_Star * (intercept_sur - price)
    return total_surplus

agg_surplus = surplus(DemandBurr["priceBurr"], DemandBurr["Qdemand"], burr_price)
print("Consumer surplus is equal to green triangle: " + str(0.5 * (burr_intercept - burr_price) * Q_Star))
```

###  Let's try again for Greek Theater Tickets

```python
GreekTixTable = DemandTable.select('GreekTix')
GreekTixTable
```

```python
# apply the same trick to obtain demand at each price
Qdemand = np.flip(np.cumsum(np.flip(GreekTixTable.group("GreekTix").column("count"))))
```

```python
DemandGreekTix = Table().with_columns([
    'priceTix', [25, 50, 75, 100, 125, 150, 175, 200],
    'Qdemand', Qdemand
])
DemandGreekTix
```

```python
tix_slope = slope(DemandGreekTix["Qdemand"], DemandGreekTix["priceTix"])
tix_intercept = intercept(DemandGreekTix["Qdemand"], DemandGreekTix["priceTix"])
tix_slope, tix_intercept
```

How many people does the Model think would buy at \\$100? Let's again use sympy.

```python
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
Q = sympy.Symbol("Q")
demand = tix_slope * Q + tix_intercept

Q_Star = solve(demand, 100)
Q_Star
```

Visualize the consumer surplus for Greek Theater tickets!

```python
DemandGreekTix.plot("Qdemand", "priceTix", linewidth=3, zorder=20) #Black : Demand for Greek Theater

triangle1 = patches.Polygon([[0,100],[Q_Star,100],[0,tix_intercept]], closed=True, color="green", zorder=1)
currentAxis = plt.gca()
currentAxis.add_patch(triangle1)

plt.plot(np.arange(0,82,0.01), tix_slope * np.arange(0,82,0.01) + tix_intercept, linewidth= 3, zorder=5) #Blue : Demand

price = 100

# This line to interactive version
plt.plot([0,Q_Star],[price]*2, color = 'r', linewidth= 3, zorder = 10) #Red : Price

plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Greek Theater Tickets');

agg_surplus = surplus(DemandGreekTix["priceTix"], DemandGreekTix["Qdemand"], price)
print("Consumer surplus is equal to green triangle: " + str(agg_surplus))
```

```python

```



--- END lec04/lec04-CSfromSurvey-closed.md ---



--- START lec04/lec04-CSfromSurvey.md ---

---
title: "lec04-CSfromSurvey"
type: lecture-notebook
week: 4
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-CSfromSurvey.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
        Kidong Kim</p></td></tr>
</table>

# Lecture 4: Demand Survey and Surplus #

The idea for this demo is to use the student's demand curve to motivate the concept of surplus.

```python
from datascience import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches
%matplotlib inline

import sympy
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"

from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
import warnings
warnings.filterwarnings("ignore")
```

## Section 1: Market Demand and Consumer Surplus

Let's start off with the demand from a student demand survey:
 - We had 4 "goods" and a range of prices available for each good
 - Students made bids on their willingness to pay for each of the 4 goods
 - The dataset for Fall 2022 has ~100 observations
 - This dataset has been exported and we read it in below:

```python
# sheet_id = "1jzgX74fgWo91Dyv7SbD4AmSFvUN5APc79BaOENpsyv8"
# sheet_name = "Form1"
# url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"

sheet_id = "1jzgX74fgWo91Dyv7SbD4AmSFvUN5APc79BaOENpsyv8"
sheet_name = "Form1"
url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}"


df_demand=pd.read_csv(url)

DemandTable = Table.from_df(df_demand)
DemandTable = DemandTable.drop('Timestamp')
DemandTable
```

```python
for i in DemandTable.labels:
    DemandTable.ihist(i, bins=7);
```

**Let's focus on the burritos first.** How many people are willing to buying a gourment burrito at any given price?   
We can assume that a person would be willing to buy the good at a price less than their bid price.

```python
BurritosTable = DemandTable.select('Burrito')
BurritosTable
```

```python
# Count how many people are in each answer pool
BurritosTable.group("Burrito")
```

```python
# Create a bar plot
table = BurritosTable.group("Burrito")

def plot_histogram(data, bins, title="Title", x_label = "Price", y_label = "Count"):
    plt.bar(bins, data, edgecolor="brown", align="center", width = 2)
    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.show()
    return 

burrito_bins = table.column(0) # Select column using method call
burrito_data = table['count'] # Select column using indexing
burrito_title = "Demand of Burritos according to different prices"

plot_histogram(burrito_data, burrito_bins, burrito_title)
```

In the visualization above, the height of each bar isn't quite right - someone who is willing to pay \\$10 for a burrito will also pay \\$2.5 for the same burrito.

```python
Qdemand = np.flip(np.cumsum(np.flip(BurritosTable.group("Burrito").column("count"))))
Qdemand
```

```python
DemandBurr= Table().with_columns([
    'priceBurr', [2.5, 5, 7.5, 10.00, 12.5, 15,17.5, 20], # those are the prices
    'Qdemand', Qdemand
])
DemandBurr
```

```python
burrito_Qdemand_bins = DemandBurr.column('priceBurr') # Select column using method call
burrito_Qdemand_data = DemandBurr['Qdemand'] # Select column using indexing
burrito_Qdemand_title = "Quantity demanded of Burritos with different prices"
Qdemand_x_label = "Price of Burrito"
Qdemand_y_label = "Quantity demanded"

plot_histogram(burrito_Qdemand_data, burrito_Qdemand_bins, burrito_Qdemand_title, Qdemand_x_label, Qdemand_y_label)
```

### Let's take a look at this table and think about the Consumer Surplus

First, let's sort the table from the most expensive burritos to the least expensive ones. 

Then, if the price is \$10, how many people are willing to pay more than the price? These people would be getting a **surplus** by only having to pay a cheaper price than the one they would be willing to pay.

```python
DemandBurr.sort("priceBurr", descending = True)
```

It looks like 
- 52 people would have been willing to pay up to \\$12.5
- 25 people would have been willing to pay up to \\$15.0
- 8 person would have been willing to pay up to \\$17.5
- 4 people would have been willing to pay up to \\$20.0

Let's add up these values

```python
CS_counting = 52*(12.5-10)+25*(15-10)+8*(17.5-10)+4*(20-10)
print('The consumer surplus from counting consumers is', CS_counting)
```

```python
# calculate the total consumer surplus given a demand table and the price of the good
def consumer_surplus(demand_table, price):
    
    # only people with a willingness to pay higher than the market price will buy the good
    demand_table_in_market = demand_table.where(0, are.above_or_equal_to(price))
    
    cs = (demand_table_in_market.column(0) - price) * demand_table_in_market.column(1)
    total_cs = sum(cs)
    
    return total_cs
```

```python
cs_burrito = consumer_surplus(DemandBurr, 10)
print('The consumer surplus from counting consumers is', cs_burrito)
```

How can we visualize the consumer surplus on the demand and supply diagram? We'll start by creating a demand curve first like before.

```python
DemandBurr.scatter("Qdemand", "priceBurr")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Gourmet Burrito');
```

```python
DemandBurr.plot("Qdemand", "priceBurr", linewidth= 3)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Gourmet Burrito');
```

Now that we have a demand curve - Let's create for a model that makes a linear approximation like we did in lecture 2.

```python
DemandGM = np.polyfit(DemandBurr.column("Qdemand"), DemandBurr.column("priceBurr"),1)
DemandGM
```

```python
burr_slope = DemandGM.item(0)
burr_slope
```

```python
burr_intercept = DemandGM.item(1)
burr_intercept
```

```python
# plot the actual demand curve
DemandBurr.plot("Qdemand", "priceBurr", linewidth=3)

# plot the linear approximation
burr_quantities = np.arange(0,120,0.01)
burr_prices = burr_slope * burr_quantities + burr_intercept
plt.plot(burr_quantities, burr_prices, linewidth=3)

plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Burrito');
```

How many people does the model think would buy burritos when its price is \\$10? Let's answer this question using sympy.

```python
# Set up the demand curve expression 
Q = sympy.Symbol("Q")
demand = burr_slope * Q + burr_intercept

# Solve for Q_star when price is 10
Q_Star = solve(demand, 10)
Q_Star
```

Now we will visualize the consumer surplus of the burrito market.

```python
DemandBurr.plot("Qdemand", "priceBurr", linewidth= 3) #Black : Demand for buritto

plt.plot(np.arange(0,82,0.01), burr_slope * np.arange(0,82,0.01) + burr_intercept, linewidth= 3) #Blue : 

price = 10
plt.plot([0,Q_Star],[price, price], color = 'r', linewidth= 3) #Red : Price

triangle1 = patches.Polygon([[0,10],[Q_Star,10],[0,burr_intercept]],True,color="green") #Consumer surplus
currentAxis = plt.gca()
currentAxis.add_patch(triangle1)

plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Burrito')

burr_price = 10

# Code for Slope and Intercept - What are the slope and intercept of the fit line
std_units = lambda a: (a - np.mean(a)) / np.std(a)
corr = lambda x, y: np.mean(std_units(x) * std_units(y))
slope = lambda x, y: corr(x, y) * np.std(y) / np.std(x)
intercept = lambda x, y: np.mean(y) - slope(x, y) * np.mean(x)



# Sums up the surplus at the give price
def surplus(bins, data, price):
    #Finding the quatity at the point where red line indicating surplus and blue line indicating the relationship btw quantiy and price.
    slope_sur = slope(data, bins)
    intercept_sur = intercept(data, bins)
    Q = sympy.Symbol("Q")
    demand = slope_sur * Q + intercept_sur
    Q_Star = solve(demand, price)
    
    #Sums up the surplus and print it out.
    total_surplus = .5 * Q_Star * (intercept_sur - price)
    return total_surplus

agg_surplus = surplus(DemandBurr["priceBurr"], DemandBurr["Qdemand"], burr_price)
print("Consumer surplus is equal to green triangle: " + str(0.5 * (burr_intercept - burr_price) * Q_Star))
```

###  Let's try again for Greek Theater Tickets

```python
GreekTixTable = DemandTable.select('GreekTix')
GreekTixTable
```

```python
# apply the same trick to obtain demand at each price
Qdemand = np.flip(np.cumsum(np.flip(GreekTixTable.group("GreekTix").column("count"))))
```

```python
DemandGreekTix = Table().with_columns([
    'priceTix', [25, 50, 75, 100, 125, 150, 175, 200],
    'Qdemand', Qdemand
])
DemandGreekTix
```

```python
tix_slope = slope(DemandGreekTix["Qdemand"], DemandGreekTix["priceTix"])
tix_intercept = intercept(DemandGreekTix["Qdemand"], DemandGreekTix["priceTix"])
tix_slope, tix_intercept
```

How many people does the Model think would buy at \\$100? Let's again use sympy.

```python
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
Q = sympy.Symbol("Q")
demand = tix_slope * Q + tix_intercept

Q_Star = solve(demand, 100)
Q_Star
```

Visualize the consumer surplus for Greek Theater tickets!

```python
DemandGreekTix.plot("Qdemand", "priceTix", linewidth=3, zorder=20) #Black : Demand for Greek Theater

triangle1 = patches.Polygon([[0,100],[Q_Star,100],[0,tix_intercept]], True, color="green", zorder=1)
currentAxis = plt.gca()
currentAxis.add_patch(triangle1)

plt.plot(np.arange(0,82,0.01), tix_slope * np.arange(0,82,0.01) + tix_intercept, linewidth= 3, zorder=5) #Blue : Demand

price = 100

# This line to interactive version
plt.plot([0,Q_Star],[price]*2, color = 'r', linewidth= 3, zorder = 10) #Red : Price

plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Demand for Greek Theater Tickets');

agg_surplus = surplus(DemandGreekTix["priceTix"], DemandGreekTix["Qdemand"], price)
print("Consumer surplus is equal to green triangle: " + str(agg_surplus))
```



--- END lec04/lec04-CSfromSurvey.md ---



--- START lec04/lec04-Supply-Demand-closed.md ---

---
title: "lec04-Supply-Demand-closed"
type: lecture-notebook
week: 4
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-Supply-Demand-closed.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
        </p></td></tr>
</table>

```python
import sympy
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
import matplotlib.pyplot as plt
plt.style.use('ggplot')
%matplotlib inline

from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
#import nbinteract as nbi
import numpy as np

import warnings
warnings.filterwarnings('ignore')

import matplotlib
print(matplotlib.__version__)
```

# Supply, Demand, and Equilibrium

## Set-up

With this notebook, we are going to plot and solve equations, hopefully giving some more hands on exposure to the materials that you've already seen in class. 

We have already covered 
- sympy
- getting slope and intercept for deman curve
- solving for equilibrium

## Solving Equations with SymPy

In order to treat variables like the symbols you would using pen and paper, we have to declare them as such first. We are going to use the inverse forms, so our equations will both be a function of quantity. We create a variable for quantity below.

```python
Q = sympy.Symbol("Q")
```

Now when we call this variable, we can see that it is a symbolic variable, and not some other Python type.

We cn decplare an upward sloping supply curve with a name, `supply`.

```python
supply = 3*Q + 4
supply
```

We can then substitue in values for our quantity variable. We do so in the following cell. Using the method `subs`, we take the equation we already defined, `supply`, and plug in the value 3 in place of `Q`.

```python
supply.subs(Q, 3)
```

Next we'll define an inverse-demand equation.

```python
demand = -4*Q+15
demand
```

We are now able to take our supply and demand equations and find where they intersect. When we use the `solve` function, it will tell us the x-value of the point where the two lines intercept. This is the equilibrium quantity, which we will call that value `Q_star`.

```python
Q_star = solve(demand, supply) # our version of solve is simplified for single solution systems
Q_star
```

We can then substitute `Q_star` back into our original inverse-supply and inverse-demand equations to solve for our equilibrium price.

```python
demand.subs(Q, Q_star)
```

```python
supply.subs(Q, Q_star)
```

## Visualizing those supply and demand equations

```python
def plot_equation(equation, price_start, price_end, label=None):
    plot_prices = [price_start, price_end]
    plot_quantities = [equation.subs(list(equation.free_symbols)[0], c) for c in plot_prices]
    plt.plot(plot_prices, plot_quantities, label=label)
    
def plot_intercept(eq1, eq2):
    ex = sympy.solve(eq1-eq2)[0]
    why = eq1.subs(list(eq1.free_symbols)[0], ex)
    plt.scatter([ex], [why])
    return (ex, why)
    
plot_equation(demand, 0, 5)
plot_equation(supply, 0, 5)
plt.ylim(0,20)
plot_intercept(supply, demand)
```

## Modeling a Shift in Demand
We would like to be able to add in the ability to shift the demand curve, using a slider

```python

def shift_demand():
    equation = demand
    def shift_helper(shift):
        plot_equation(equation, -10, 10)
        plot_equation(supply, -10, 10)
        old = plot_intercept(equation, supply)
        print('Original Intercept:', old)
        
        if shift != 0:
            plot_equation(equation + shift, -10, 10, 'shifted')
            new = plot_intercept(equation + shift, supply)
            print('New intercept:', new)
            print('Change in Quantity:', round(float(new[0]-old[0]), 2))
            print('Change in Price:', round(float(new[1]-old[1]), 2))
        else:
            print('Nothing shifted yet, use the slider to move the line!')
        plt.xlim(0,8)
        plt.ylim(0,20)
        plt.legend()
        plt.ylabel("Price")
        plt.xlabel("Quantity")
    interact(shift_helper, shift=(-6, 12, 3))

shift_demand()
```

## Externalities
Ok lets 'shift' the analysis to 

**Marginal Private Benefit (MPB)**
The marginal private benefit (MPB) is the benefit that an individual or firm receives from consuming or producing an additional unit of a good or service. This benefit is internal to the consumer or producer and does not take into account any external costs or benefits that might affect others.

In the case of consumption, the marginal private benefit is the value a consumer places on the next unit of a good, which typically decreases as more units are consumed (due to diminishing marginal utility). This is represented by the private demand curve, which shows the relationship between the quantity demanded and the price.

For example:

If you buy a textbook, the benefit you receive from reading it is the private benefit. You might be willing to pay a certain price for the book based on how much you value the knowledge it provides.

**Marginal Social Benefit (MSB)**
The marginal social benefit (MSB) is the total benefit to society from consuming or producing an additional unit of a good or service. It includes both the marginal private benefit and any external benefits (or subtracts external costs, in the case of negative externalities).

When externalities exist, the marginal social benefit differs from the marginal private benefit. In the case of positive externalities, the MSB is greater than the MPB because the good or service provides additional benefits to third parties that are not captured by the individual consumer.

For example:

Education creates positive externalities. While the student receives private benefits from increased knowledge and skills (MPB), society as a whole also benefits from having a more educated population, leading to increased productivity, lower crime rates, and higher civic engagement. These additional benefits form the social benefit.
In a competitive market without intervention, only the private benefits are considered, leading to under-consumption of goods with positive externalities. The market does not provide enough of the good relative to the socially optimal level because individuals do not account for the benefits that spill over to others.

**Market Outcome vs. Socially Optimal Outcome**
In the presence of a positive externality:

The market outcome is determined by the intersection of the supply curve (marginal cost) and the private demand curve (MPB).
However, the socially optimal outcome occurs at the intersection of the supply curve and the social demand curve (MSB), which lies above the private demand curve due to the external benefits.

Let's do a simpler version where we just solve the following :
\begin{align*}
    \text{Private Demand (MPB)}: & \quad P = 100 - 0.3Q \\
    \text{Social Demand (MSB)}: & \quad P = (100 + \text{Externality Shift}) - 0.3Q \\
    \text{Supply (MC)}: & \quad P = 10 + 0.2Q\\
    \text {Where}: & \quad \text{Externality Shift} = +10\\
\end{align*}

```python
def private_demand(q):
    return 100 - 0.3 * q  

def social_demand(q):
    return 110 - 0.3 * q  

def supply(q):
    return 10 + 0.2 * q  

market_quantity = (100 - 10) / (0.2 + 0.3)  
optimal_quantity = (110 - 10) / (0.2 + 0.3)
```

```python

quantities = np.linspace(0, 300, 500)

#Plotting
plt.figure(figsize=(6, 6))
plt.plot(quantities, private_demand(quantities), label='Private Demand (MPB)', color='green')
plt.plot(quantities, social_demand(quantities), label='Social Demand (MSB)', color='purple', linestyle='--')
plt.plot(quantities, supply(quantities), label='Supply (MC)', color='blue')

plt.axvline(market_quantity, color='green', linestyle='--', label='Market Quantity')
plt.axvline(optimal_quantity, color='purple', linestyle='--', label='Optimal Quantity')

plt.fill_between(quantities, private_demand(quantities), social_demand(quantities), where=(quantities <= optimal_quantity), color='lightblue', alpha=0.3)

plt.title('Positive Externality: Demand Curve')
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()
```

Let's do an interactive version where we solve the following with a variable externality :
\begin{align*}
    \text{Private Demand (MPB)}: & \quad P = 100 - 0.3Q \\
    \text{Social Demand (MSB)}: & \quad P = (100 + \text{Externality Shift}) - 0.3Q \\
    \text{Supply (MC)}: & \quad P = 10 + 0.2Q
\end{align*}

```python
#Everything is wrapped in a function that the slider can interact with

def plot_externality(externality_shift):
    def social_demand(q):
        return (100 + externality_shift) - 0.3 * q  # Marginal Social Benefit (MSB)

#Hard coded values for Equilibrium from solving Demand and Supply
    market_quantity = (100 - 10) / (0.2 + 0.3) 
    optimal_quantity = ((100 + externality_shift) - 10) / (0.2 + 0.3) 
    
    market_price = private_demand(market_quantity)
    optimal_price = social_demand(optimal_quantity)

#Plotting    
    plt.figure(figsize=(8, 6))
    plt.plot(quantities, private_demand(quantities), color='green')
    plt.plot(quantities, social_demand(quantities), color='purple', linestyle='--')
    plt.plot(quantities, supply(quantities), color='blue')

    plt.axvline(market_quantity, color='green', linestyle='--', label=f'Market Quantity: {market_quantity:.2f}')
    plt.axvline(optimal_quantity, color='purple', linestyle='--', label=f'Social Quantity: {optimal_quantity:.2f}')

    plt.axhline(market_price, color='green', linestyle='--', label=f'Market Price: {market_price:.2f}')
    plt.axhline(optimal_price, color='purple', linestyle='--', label=f'Social Price: {optimal_price:.2f}')

  
    plt.fill_between(quantities, private_demand(quantities), social_demand(quantities), where=(quantities <= optimal_quantity), color='lightblue', alpha=0.3)

#Labels on demand
    plt.text(300, private_demand(300) + 2, 'Marginal Private Benefit (MPB)', color='green', fontsize=12)
    plt.text(300, social_demand(300) + 2, 'Marginal Social Benefit (MSB)', color='purple', fontsize=12)
#Labels on supply 
    plt.text(300, supply(300) + 2, 'Supply ', color='blue', fontsize=12)

  
    plt.title(f' Externality: Externality Shift = {externality_shift}')
    plt.xlabel('Quantity')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.show()

#Slider for externality shift
interact(plot_externality, externality_shift=widgets.IntSlider(min=-20, max=20, step=1, value=10, description="Externality Shift"));
```

## Consumer and Producer Surplus

```python
import numpy as np
import matplotlib.pyplot as plt
import sympy
from sympy import symbols
import matplotlib.patches as patches

# Define the quantity symbol for sympy
Q = symbols('Q')

# Simplified Equilibrium function with a range of quantities
def Equilibrium(demandParam, supplyParam):
    # Define demand and supply equations as functions of quantity
    demandEquation = demandParam - Q  # P = demandParam - Q
    supplyEquation = (supplyParam / 10) * Q  # P = (supplyParam / 10) * Q
    
    # Convert the sympy equations to numerical functions using lambdify
    demandFunc = sympy.lambdify(Q, demandEquation, "numpy")
    supplyFunc = sympy.lambdify(Q, supplyEquation, "numpy")
    
    # Define a fixed range of quantities (from 0 to 100)
    quantities = np.linspace(0, 50, 50)
    
    # Calculate corresponding prices for each quantity for demand and supply
    demandPrices = demandFunc(quantities)  # Evaluate demand prices as numerical values
    supplyPrices = supplyFunc(quantities)  # Evaluate supply prices as numerical values

    # Find the equilibrium quantity by solving where demand equals supply
    equilibriumQ = sympy.solve(demandEquation - supplyEquation, Q)[0]
    equilibriumP = demandEquation.subs(Q, equilibriumQ)
    
    # Convert equilibrium values to float for plotting
    equilibriumQ = float(equilibriumQ)
    equilibriumP = float(equilibriumP)
    
    # Plot demand and supply curves
    plt.plot(quantities, demandPrices, label="Demand (MPB)", color="green")
    plt.plot(quantities, supplyPrices, label="Supply (MC)", color="blue")
    
    # Mark equilibrium point
    plt.plot(equilibriumQ, equilibriumP, 'ro', label=f"Equilibrium Q={round(equilibriumQ, 2)}, P={round(equilibriumP, 2)}")
    
    # Shade consumer and producer surplus
    triangle1 = patches.Polygon([[0, equilibriumP], [equilibriumQ, equilibriumP], [0, demandFunc(0)]], closed=True, color="green", alpha=0.3)
    triangle2 = patches.Polygon([[0, equilibriumP], [equilibriumQ, equilibriumP], [0, 0]], closed=True, color="red", alpha=0.3)
    currentAxis = plt.gca()
    currentAxis.add_patch(triangle1)
    currentAxis.add_patch(triangle2)
    
    # Add labels and legend
    plt.xlabel("Quantity")
    plt.ylabel("Price")
    plt.xlim(0, 50)  # Fixed quantity range from 0 to 100
    plt.ylim(0, max(demandPrices))  # Set y-limits based on demand prices
    plt.legend()
    plt.grid(True)
    
    # Display plot
    plt.show()
    
    # Print equilibrium information
    print(f"The equilibrium price is {round(float(equilibriumP), 2)} and equilibrium quantity is {round(float(equilibriumQ), 2)}.")
    print(f"The consumer surplus at this equilibrium is {(demandFunc(0) - equilibriumP) * equilibriumQ * 0.5}")
    print(f"The producer surplus at this equilibrium is {(equilibriumP * equilibriumQ) * 0.5}")

# Example usage
Equilibrium(demandParam=50, supplyParam=20)
```

```python
import sympy
import matplotlib.pyplot as plt
import matplotlib.patches as patches
p = sympy.Symbol("p")
def Equilibrium(demandParam, supplyParam, priceStart):
    demandEquation = demandParam - p
    # change the slope
    supplyEquation = p * (supplyParam/10)
    priceEnd = sympy.solve(demandEquation)[0]
    prices = []
    demandQ = []
    supplyQ = []
    for price in range(priceStart,priceEnd+1):
        prices += [price]
        demandQ += [demandEquation.subs(p,price)]
        supplyQ += [supplyEquation.subs(p,price)]
    
    equilibriumP = sympy.solve(demandEquation-supplyEquation)[0]
    equilibriumQ = demandEquation.subs(p,equilibriumP)
    
    
    
    triangle1 = patches.Polygon([[equilibriumQ,equilibriumP],[0,equilibriumP],[0,priceEnd]],closed=True,color="green")
    triangle2 = patches.Polygon([[equilibriumQ,equilibriumP],[0,equilibriumP],[0,0]],closed=True,color="red")
    currentAxis = plt.gca()
    currentAxis.add_patch(triangle1)
    currentAxis.add_patch(triangle2)
    
    plt.plot(demandQ,prices)
    plt.plot(supplyQ,prices)
    plt.legend(["Demand","Supply"])
    plt.plot(equilibriumQ,equilibriumP, 'ro')
    plt.xlabel("Supply and Demand Quantity")
    plt.ylabel("Price")
    plt.ylim(0, 15)
    plt.xlim(0, 10)
    plt.show()
    print("The equilibrium price is "+str(round(equilibriumP,2))+" and equilibrium quantity is "+str(round(equilibriumQ,2))+".")
    print("The consumer surplus at this equilibrium "+str((priceEnd-equilibriumP)*(equilibriumQ)*.5))
    print("The producer surplus at this equilibrium "+str((equilibriumP)*(equilibriumQ)*.5))


# you can change the range here
slider1 = widgets.IntSlider(min=5, max=15,step=1,value=10, description="Demand Intercept")
slider2 = widgets.IntSlider(min=1, max=20,step=1,value=10, description="Supply Slope")
slider3 = widgets.IntSlider(min=-5, max=5,step=1,value=0)

# Add additional text using widgets.Label or widgets.HTML
text_before = widgets.HTML(
    value="<h3>Equilibrium Simulation</h3><p>This interactive simulation allows you to explore the equilibrium price and quantity based on different demand and supply parameters.</p>"
)

text_after = widgets.HTML(
    value="<p>Adjust the <b>Demand Param</b> and <b>Supply Param</b> sliders to change the shape of the curves, and observe how the equilibrium changes.</p>"
)

# Display all elements together
ui = widgets.VBox([text_before, slider1, slider2, slider3, text_after])
out = widgets.interactive_output(Equilibrium, {'demandParam': slider1, 'supplyParam': slider2, 'priceStart': slider3})

display(ui, out)
```

## Effects of Taxes

```python
def eqSolve(eq1,eq2,tax):
    demandP = sympy.solve(eq1-q,p)[0]
    supplyP = sympy.solve(eq2-q,p)[0]
    demandP = demandP-cTax
    supplyP = supplyP+pTax

    demandQ = sympy.solve(demandP-p,q)[0]
    supplyQ = sympy.solve(supplyP-p,q)[0]
    
    return sympy.solve((demandP-supplyP, demandQ-supplyQ,tax-cTax-pTax), q,p,cTax,pTax)[q]
```

```python

p = sympy.Symbol("p")
q = sympy.Symbol("q")
cTax = sympy.Symbol("cTax")
pTax = sympy.Symbol("pTax")

def EquilibriumTax(demandParam,supplyParam,priceStart,priceEnd,tax):
    demandEquation = demandParam - p
    supplyEquation = p * (supplyParam/10)
    prices = []
    demand = []
    supply = []
    for price in range(priceStart,priceEnd+1):
        prices += [price]
        demand += [demandEquation.subs(p,price)]
        supply += [supplyEquation.subs(p,price)]
        
    
    
    nonTaxPrice = sympy.solve(demandEquation-supplyEquation)[0]
    nonTaxQ = demandEquation.subs(p,nonTaxPrice)

    
    equilibriumQ = eqSolve(demandEquation,supplyEquation,tax)
    equilibriumP1 = sympy.solve(demandEquation-equilibriumQ)[0]
    equilibriumP2 = sympy.solve(supplyEquation-equilibriumQ)[0]
    
    triangle1 = patches.Polygon([[nonTaxQ,nonTaxPrice],[equilibriumQ,nonTaxPrice],[equilibriumQ,equilibriumP1]],closed=True,color="green")
    triangle2 = patches.Polygon([[nonTaxQ,nonTaxPrice],[equilibriumQ,nonTaxPrice],[equilibriumQ,equilibriumP2]],closed=True)
    #triangle1 = patches.Polygon([[0, equilibriumP1], [equilibriumQ, equilibriumP], [0, demandFunc(0)]], closed=True, color="green", alpha=0.3)
    #triangle2 = patches.Polygon([[0, equilibriumP2], [equilibriumQ, equilibriumP], [0, 0]], closed=True, color="red", alpha=0.3)
    currentAxis = plt.gca()
    currentAxis.add_patch(triangle1)
    currentAxis.add_patch(triangle2)
    
    
    rect1 = patches.Rectangle((0,nonTaxPrice),equilibriumQ,equilibriumP1-nonTaxPrice,linewidth=1,facecolor="red")
    rect2 = patches.Rectangle((0,nonTaxPrice),equilibriumQ,equilibriumP2-nonTaxPrice,linewidth=1,facecolor="yellow")
    currentAxis.add_patch(rect1)
    currentAxis.add_patch(rect2)
    
    plt.plot(demand,prices)
    plt.plot(supply,prices)
    
    
    plt.legend([rect1,rect2,triangle1,triangle2], ["Consumer Tax","Producer Tax","Consumer Deadweight Loss","Producer Deadweight Loss"])
    plt.plot(equilibriumQ,equilibriumP1, 'ro')
    plt.plot(equilibriumQ,equilibriumP2, 'ro')
    plt.xlabel("Supply and Demand Quantity")
    plt.ylabel("Price")
    plt.ylim(0, 15)
    plt.xlim(0, 10)
    plt.show()
    print("Without Tax - the equilibrium price is "+str(round(nonTaxPrice,2))+" and equilibrium quantity is "+str(round(nonTaxQ,2)))
    print("With Tax - Price paid by consumers is "+str(equilibriumP1)+" Price received by suppliers is "+str(round(equilibriumP2,2))+" and equilibrium quantity is "+str(equilibriumQ)+".")
    print("Taxes raised from consumers equals "+str(round(equilibriumQ*(equilibriumP1-nonTaxPrice),2)))
    print("Taxes raised from producers equals "+str(round(equilibriumQ*(nonTaxPrice-equilibriumP2),2)))
    print("Total taxes raised equals "+str(equilibriumQ*tax))

slider1 = widgets.IntSlider(min=5, max=15,step=1,value=10)
slider2 = widgets.IntSlider(min=1, max=20,step=1,value=10)
slider3 = widgets.IntSlider(min=-5, max=5,step=1,value=0)
slider4 = widgets.IntSlider(min=5, max=20,step=1,value=10)
slider5 = widgets.IntSlider(min=0, max=8,step=1,value=4)
display(widgets.interactive(EquilibriumTax, demandParam=slider1, supplyParam=slider2, priceStart=slider3, priceEnd=slider4, tax=slider5))
```

## Shifts in Demand and Supply and Incidence of Tax
In this graph the second line with the suppply plus tax is not graphed but the new equilibrium point is.

## Question 3.1 -  Demand Shift - Intercept Slider
Shift the Demand Parameter - This slider is only shifting the intercept which leads to a parrallel shift in Demand. What happens to the amount of tax paid  as demand shifts out.  What happens to the ratio of tax paid from Consumer Surplus to Producer Surplus?

....  Type your answer in here

## Question 3.2 - Supply Shift - Slope Slider
Shift the Supply Parameter - This slider is shifting the slope which leads to a change in the steepness of the Supply curve. What happens to the amount of Consumer Surplus as supply becomes steeper?  What happens to the tax incidence - the ratio of tax coming from Consumer Surplus vs. Producer Surplus?

....  Type your answer in here

## Question 3.3 - Tax Shift - 
Shift the Tax Parameter - This slider is increasing or decreasing the amount of tax. 

Is it affecting the slope or the intercept? 

An increase in tax leads to and increase in the price paid by consumers. In this case how much does a given tax lead to an increase in price?


When shifting the tax alone - how does the incidence of tax between producers and consumers change?

....  Type your answer in here







--- END lec04/lec04-Supply-Demand-closed.md ---



--- START lec04/lec04-Supply-Demand.md ---

---
title: "lec04-Supply-Demand"
type: lecture-notebook
week: 4
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-Supply-Demand.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
        </p></td></tr>
</table>

```python
import sympy
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
import matplotlib.pyplot as plt
plt.style.use('ggplot')
%matplotlib inline

from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
#import nbinteract as nbi
import numpy as np

import warnings
warnings.filterwarnings('ignore')
```

# Supply, Demand, and Equilibrium

## Set-up

With this notebook, we are going to plot and solve equations, hopefully giving some more hands on exposure to the materials that you've already seen in class. 

We have already covered 
- sympy
- getting slope and intercept for deman curve
- solving for equilibrium

## Solving Equations with SymPy

In order to treat variables like the symbols you would using pen and paper, we have to declare them as such first. We are going to use the inverse forms, so our equations will both be a function of quantity. We create a variable for quantity below.

```python
Q = sympy.Symbol("Q")
```

Now when we call this variable, we can see that it is a symbolic variable, and not some other Python type.

We cn decplare an upward sloping supply curve with a name, `supply`.

```python
supply = 3*Q + 4
supply
```

We can then substitue in values for our quantity variable. We do so in the following cell. Using the method `subs`, we take the equation we already defined, `supply`, and plug in the value 3 in place of `Q`.

```python
supply.subs(Q, 3)
```

Next we'll define an inverse-demand equation.

```python
demand = -4*Q+15
demand
```

We are now able to take our supply and demand equations and find where they intersect. When we use the `solve` function, it will tell us the x-value of the point where the two lines intercept. This is the equilibrium quantity, which we will call that value `Q_star`.

```python
Q_star = solve(demand, supply) # our version of solve is simplified for single solution systems
Q_star
```

We can then substitute `Q_star` back into our original inverse-supply and inverse-demand equations to solve for our equilibrium price.

```python
demand.subs(Q, Q_star)
```

```python
supply.subs(Q, Q_star)
```

## Visualizing those supply and demand equations

```python
def plot_equation(equation, price_start, price_end, label=None):
    plot_prices = [price_start, price_end]
    plot_quantities = [equation.subs(list(equation.free_symbols)[0], c) for c in plot_prices]
    plt.plot(plot_prices, plot_quantities, label=label)
    
def plot_intercept(eq1, eq2):
    ex = sympy.solve(eq1-eq2)[0]
    why = eq1.subs(list(eq1.free_symbols)[0], ex)
    plt.scatter([ex], [why])
    return (ex, why)
    
plot_equation(demand, 0, 5)
plot_equation(supply, 0, 5)
plt.ylim(0,20)
plot_intercept(supply, demand)
```

## Modeling a Shift in Demand
We would like to be able to add in the ability to shift the demand curve, using a slider

```python

def shift_demand():
    equation = demand
    def shift_helper(shift):
        plot_equation(equation, -10, 10)
        plot_equation(supply, -10, 10)
        old = plot_intercept(equation, supply)
        print('Original Intercept:', old)
        
        if shift != 0:
            plot_equation(equation + shift, -10, 10, 'shifted')
            new = plot_intercept(equation + shift, supply)
            print('New intercept:', new)
            print('Change in Quantity:', round(float(new[0]-old[0]), 2))
            print('Change in Price:', round(float(new[1]-old[1]), 2))
        else:
            print('Nothing shifted yet, use the slider to move the line!')
        plt.xlim(0,8)
        plt.ylim(0,20)
        plt.legend()
        plt.ylabel("Price")
        plt.xlabel("Quantity")
    interact(shift_helper, shift=(-6, 12, 3))

shift_demand()
```

## Externalities
Ok lets 'shift' the analysis to 

**Marginal Private Benefit (MPB)**
The marginal private benefit (MPB) is the benefit that an individual or firm receives from consuming or producing an additional unit of a good or service. This benefit is internal to the consumer or producer and does not take into account any external costs or benefits that might affect others.

In the case of consumption, the marginal private benefit is the value a consumer places on the next unit of a good, which typically decreases as more units are consumed (due to diminishing marginal utility). This is represented by the private demand curve, which shows the relationship between the quantity demanded and the price.

For example:

If you buy a textbook, the benefit you receive from reading it is the private benefit. You might be willing to pay a certain price for the book based on how much you value the knowledge it provides.

**Marginal Social Benefit (MSB)**
The marginal social benefit (MSB) is the total benefit to society from consuming or producing an additional unit of a good or service. It includes both the marginal private benefit and any external benefits (or subtracts external costs, in the case of negative externalities).

When externalities exist, the marginal social benefit differs from the marginal private benefit. In the case of positive externalities, the MSB is greater than the MPB because the good or service provides additional benefits to third parties that are not captured by the individual consumer.

For example:

Education creates positive externalities. While the student receives private benefits from increased knowledge and skills (MPB), society as a whole also benefits from having a more educated population, leading to increased productivity, lower crime rates, and higher civic engagement. These additional benefits form the social benefit.
In a competitive market without intervention, only the private benefits are considered, leading to under-consumption of goods with positive externalities. The market does not provide enough of the good relative to the socially optimal level because individuals do not account for the benefits that spill over to others.

**Market Outcome vs. Socially Optimal Outcome**
In the presence of a positive externality:

The market outcome is determined by the intersection of the supply curve (marginal cost) and the private demand curve (MPB).
However, the socially optimal outcome occurs at the intersection of the supply curve and the social demand curve (MSB), which lies above the private demand curve due to the external benefits.

Let's do a simpler version where we just solve the following :
\begin{align*}
    \text{Private Demand (MPB)}: & \quad P = 100 - 0.3Q \\
    \text{Social Demand (MSB)}: & \quad P = (100 + \text{Externality Shift}) - 0.3Q \\
    \text{Supply (MC)}: & \quad P = 10 + 0.2Q\\
    \text {Where}: & \quad \text{Externality Shift} = +10\\
\end{align*}

```python
def private_demand(q):
    return 100 - 0.3 * q  

def social_demand(q):
    return 110 - 0.3 * q  

def supply(q):
    return 10 + 0.2 * q  

market_quantity = (100 - 10) / (0.2 + 0.3)  
optimal_quantity = (110 - 10) / (0.2 + 0.3)
```

```python

quantities = np.linspace(0, 300, 500)

#Plotting
plt.figure(figsize=(6, 6))
plt.plot(quantities, private_demand(quantities), label='Private Demand (MPB)', color='green')
plt.plot(quantities, social_demand(quantities), label='Social Demand (MSB)', color='purple', linestyle='--')
plt.plot(quantities, supply(quantities), label='Supply (MC)', color='blue')

plt.axvline(market_quantity, color='green', linestyle='--', label='Market Quantity')
plt.axvline(optimal_quantity, color='purple', linestyle='--', label='Optimal Quantity')

plt.fill_between(quantities, private_demand(quantities), social_demand(quantities), where=(quantities <= optimal_quantity), color='lightblue', alpha=0.3)

plt.title('Positive Externality: Demand Curve')
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()
```

Let's do an interactive version where we solve the following with a variable externality :
\begin{align*}
    \text{Private Demand (MPB)}: & \quad P = 100 - 0.3Q \\
    \text{Social Demand (MSB)}: & \quad P = (100 + \text{Externality Shift}) - 0.3Q \\
    \text{Supply (MC)}: & \quad P = 10 + 0.2Q
\end{align*}

```python
#Everything is wrapped in a function that the slider can interact with

def plot_externality(externality_shift):
    def social_demand(q):
        return (100 + externality_shift) - 0.3 * q  # Marginal Social Benefit (MSB)

#Hard coded values for Equilibrium from solving Demand and Supply
    market_quantity = (100 - 10) / (0.2 + 0.3) 
    optimal_quantity = ((100 + externality_shift) - 10) / (0.2 + 0.3) 
    
    market_price = private_demand(market_quantity)
    optimal_price = social_demand(optimal_quantity)

#Plotting    
    plt.figure(figsize=(8, 6))
    plt.plot(quantities, private_demand(quantities), color='green')
    plt.plot(quantities, social_demand(quantities), color='purple', linestyle='--')
    plt.plot(quantities, supply(quantities), color='blue')

    plt.axvline(market_quantity, color='green', linestyle='--', label=f'Market Quantity: {market_quantity:.2f}')
    plt.axvline(optimal_quantity, color='purple', linestyle='--', label=f'Social Quantity: {optimal_quantity:.2f}')

    plt.axhline(market_price, color='green', linestyle='--', label=f'Market Price: {market_price:.2f}')
    plt.axhline(optimal_price, color='purple', linestyle='--', label=f'Social Price: {optimal_price:.2f}')

  
    plt.fill_between(quantities, private_demand(quantities), social_demand(quantities), where=(quantities <= optimal_quantity), color='lightblue', alpha=0.3)

#Labels on demand
    plt.text(300, private_demand(300) + 2, 'Marginal Private Benefit (MPB)', color='green', fontsize=12)
    plt.text(300, social_demand(300) + 2, 'Marginal Social Benefit (MSB)', color='purple', fontsize=12)
#Labels on supply 
    plt.text(300, supply(300) + 2, 'Supply ', color='blue', fontsize=12)

  
    plt.title(f' Externality: Externality Shift = {externality_shift}')
    plt.xlabel('Quantity')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.show()

#Slider for externality shift
interact(plot_externality, externality_shift=widgets.IntSlider(min=-20, max=20, step=1, value=10, description="Externality Shift"));
```

## Consumer and Producer Surplus

```python
import numpy as np
import matplotlib.pyplot as plt
import sympy
from sympy import symbols
import matplotlib.patches as patches

# Define the quantity symbol for sympy
Q = symbols('Q')

# Simplified Equilibrium function with a range of quantities
def Equilibrium(demandParam, supplyParam):
    # Define demand and supply equations as functions of quantity
    demandEquation = demandParam - Q  # P = demandParam - Q
    supplyEquation = (supplyParam / 10) * Q  # P = (supplyParam / 10) * Q
    
    # Convert the sympy equations to numerical functions using lambdify
    demandFunc = sympy.lambdify(Q, demandEquation, "numpy")
    supplyFunc = sympy.lambdify(Q, supplyEquation, "numpy")
    
    # Define a fixed range of quantities (from 0 to 100)
    quantities = np.linspace(0, 50, 50)
    
    # Calculate corresponding prices for each quantity for demand and supply
    demandPrices = demandFunc(quantities)  # Evaluate demand prices as numerical values
    supplyPrices = supplyFunc(quantities)  # Evaluate supply prices as numerical values

    # Find the equilibrium quantity by solving where demand equals supply
    equilibriumQ = sympy.solve(demandEquation - supplyEquation, Q)[0]
    equilibriumP = demandEquation.subs(Q, equilibriumQ)
    
    # Convert equilibrium values to float for plotting
    equilibriumQ = float(equilibriumQ)
    equilibriumP = float(equilibriumP)
    
    # Plot demand and supply curves
    plt.plot(quantities, demandPrices, label="Demand (MPB)", color="green")
    plt.plot(quantities, supplyPrices, label="Supply (MC)", color="blue")
    
    # Mark equilibrium point
    plt.plot(equilibriumQ, equilibriumP, 'ro', label=f"Equilibrium Q={round(equilibriumQ, 2)}, P={round(equilibriumP, 2)}")
    
    # Shade consumer and producer surplus
    triangle1 = patches.Polygon([[0, equilibriumP], [equilibriumQ, equilibriumP], [0, demandFunc(0)]], True, color="green", alpha=0.3)
    triangle2 = patches.Polygon([[0, equilibriumP], [equilibriumQ, equilibriumP], [0, 0]], True, color="red", alpha=0.3)
    currentAxis = plt.gca()
    currentAxis.add_patch(triangle1)
    currentAxis.add_patch(triangle2)
    
    # Add labels and legend
    plt.xlabel("Quantity")
    plt.ylabel("Price")
    plt.xlim(0, 50)  # Fixed quantity range from 0 to 100
    plt.ylim(0, max(demandPrices))  # Set y-limits based on demand prices
    plt.legend()
    plt.grid(True)
    
    # Display plot
    plt.show()
    
    # Print equilibrium information
    print(f"The equilibrium price is {round(float(equilibriumP), 2)} and equilibrium quantity is {round(float(equilibriumQ), 2)}.")
    print(f"The consumer surplus at this equilibrium is {(demandFunc(0) - equilibriumP) * equilibriumQ * 0.5}")
    print(f"The producer surplus at this equilibrium is {(equilibriumP * equilibriumQ) * 0.5}")

# Example usage
Equilibrium(demandParam=50, supplyParam=20)
```

```python
import sympy
import matplotlib.pyplot as plt
import matplotlib.patches as patches
p = sympy.Symbol("p")
def Equilibrium(demandParam, supplyParam, priceStart):
    demandEquation = demandParam - p
    # change the slope
    supplyEquation = p * (supplyParam/10)
    priceEnd = sympy.solve(demandEquation)[0]
    prices = []
    demandQ = []
    supplyQ = []
    for price in range(priceStart,priceEnd+1):
        prices += [price]
        demandQ += [demandEquation.subs(p,price)]
        supplyQ += [supplyEquation.subs(p,price)]
    
    equilibriumP = sympy.solve(demandEquation-supplyEquation)[0]
    equilibriumQ = demandEquation.subs(p,equilibriumP)
    
    
    
    triangle1 = patches.Polygon([[equilibriumQ,equilibriumP],[0,equilibriumP],[0,priceEnd]],True,color="green")
    triangle2 = patches.Polygon([[equilibriumQ,equilibriumP],[0,equilibriumP],[0,0]],True,color="red")
    currentAxis = plt.gca()
    currentAxis.add_patch(triangle1)
    currentAxis.add_patch(triangle2)
    
    plt.plot(demandQ,prices)
    plt.plot(supplyQ,prices)
    plt.legend(["Demand","Supply"])
    plt.plot(equilibriumQ,equilibriumP, 'ro')
    plt.xlabel("Supply and Demand Quantity")
    plt.ylabel("Price")
    plt.ylim(0, 15)
    plt.xlim(0, 10)
    plt.show()
    print("The equilibrium price is "+str(round(equilibriumP,2))+" and equilibrium quantity is "+str(round(equilibriumQ,2))+".")
    print("The consumer surplus at this equilibrium "+str((priceEnd-equilibriumP)*(equilibriumQ)*.5))
    print("The producer surplus at this equilibrium "+str((equilibriumP)*(equilibriumQ)*.5))


# you can change the range here
slider1 = widgets.IntSlider(min=5, max=15,step=1,value=10, description="Demand Intercept")
slider2 = widgets.IntSlider(min=1, max=20,step=1,value=10, description="Supply Slope")
slider3 = widgets.IntSlider(min=-5, max=5,step=1,value=0)

# Add additional text using widgets.Label or widgets.HTML
text_before = widgets.HTML(
    value="<h3>Equilibrium Simulation</h3><p>This interactive simulation allows you to explore the equilibrium price and quantity based on different demand and supply parameters.</p>"
)

text_after = widgets.HTML(
    value="<p>Adjust the <b>Demand Param</b> and <b>Supply Param</b> sliders to change the shape of the curves, and observe how the equilibrium changes.</p>"
)

# Display all elements together
ui = widgets.VBox([text_before, slider1, slider2, slider3, text_after])
out = widgets.interactive_output(Equilibrium, {'demandParam': slider1, 'supplyParam': slider2, 'priceStart': slider3})

display(ui, out)
```

## Effects of Taxes

```python
def eqSolve(eq1,eq2,tax):
    demandP = sympy.solve(eq1-q,p)[0]
    supplyP = sympy.solve(eq2-q,p)[0]
    demandP = demandP-cTax
    supplyP = supplyP+pTax

    demandQ = sympy.solve(demandP-p,q)[0]
    supplyQ = sympy.solve(supplyP-p,q)[0]
    
    return sympy.solve((demandP-supplyP, demandQ-supplyQ,tax-cTax-pTax), q,p,cTax,pTax)[q]
```

```python

p = sympy.Symbol("p")
q = sympy.Symbol("q")
cTax = sympy.Symbol("cTax")
pTax = sympy.Symbol("pTax")

def EquilibriumTax(demandParam,supplyParam,priceStart,priceEnd,tax):
    demandEquation = demandParam - p
    supplyEquation = p * (supplyParam/10)
    prices = []
    demand = []
    supply = []
    for price in range(priceStart,priceEnd+1):
        prices += [price]
        demand += [demandEquation.subs(p,price)]
        supply += [supplyEquation.subs(p,price)]
        
    
    
    nonTaxPrice = sympy.solve(demandEquation-supplyEquation)[0]
    nonTaxQ = demandEquation.subs(p,nonTaxPrice)

    
    equilibriumQ = eqSolve(demandEquation,supplyEquation,tax)
    equilibriumP1 = sympy.solve(demandEquation-equilibriumQ)[0]
    equilibriumP2 = sympy.solve(supplyEquation-equilibriumQ)[0]
    
    triangle1 = patches.Polygon([[nonTaxQ,nonTaxPrice],[equilibriumQ,nonTaxPrice],[equilibriumQ,equilibriumP1]],True,color="green")
    triangle2 = patches.Polygon([[nonTaxQ,nonTaxPrice],[equilibriumQ,nonTaxPrice],[equilibriumQ,equilibriumP2]],True)
    currentAxis = plt.gca()
    currentAxis.add_patch(triangle1)
    currentAxis.add_patch(triangle2)
    
    
    rect1 = patches.Rectangle((0,nonTaxPrice),equilibriumQ,equilibriumP1-nonTaxPrice,linewidth=1,facecolor="red")
    rect2 = patches.Rectangle((0,nonTaxPrice),equilibriumQ,equilibriumP2-nonTaxPrice,linewidth=1,facecolor="yellow")
    currentAxis.add_patch(rect1)
    currentAxis.add_patch(rect2)
    
    plt.plot(demand,prices)
    plt.plot(supply,prices)
    
    
    plt.legend([rect1,rect2,triangle1,triangle2], ["Consumer Tax","Producer Tax","Consumer Deadweight Loss","Producer Deadweight Loss"])
    plt.plot(equilibriumQ,equilibriumP1, 'ro')
    plt.plot(equilibriumQ,equilibriumP2, 'ro')
    plt.xlabel("Supply and Demand Quantity")
    plt.ylabel("Price")
    plt.ylim(0, 15)
    plt.xlim(0, 10)
    plt.show()
    print("Without Tax - the equilibrium price is "+str(round(nonTaxPrice,2))+" and equilibrium quantity is "+str(round(nonTaxQ,2)))
    print("With Tax - Price paid by consumers is "+str(equilibriumP1)+" Price received by suppliers is "+str(round(equilibriumP2,2))+" and equilibrium quantity is "+str(equilibriumQ)+".")
    print("Taxes raised from consumers equals "+str(round(equilibriumQ*(equilibriumP1-nonTaxPrice),2)))
    print("Taxes raised from producers equals "+str(round(equilibriumQ*(nonTaxPrice-equilibriumP2),2)))
    print("Total taxes raised equals "+str(equilibriumQ*tax))

slider1 = widgets.IntSlider(min=5, max=15,step=1,value=10)
slider2 = widgets.IntSlider(min=1, max=20,step=1,value=10)
slider3 = widgets.IntSlider(min=-5, max=5,step=1,value=0)
slider4 = widgets.IntSlider(min=5, max=20,step=1,value=10)
slider5 = widgets.IntSlider(min=0, max=8,step=1,value=4)
display(widgets.interactive(EquilibriumTax, demandParam=slider1, supplyParam=slider2, priceStart=slider3, priceEnd=slider4, tax=slider5))
```

## Shifts in Demand and Supply and Incidence of Tax
In this graph the second line with the suppply plus tax is not graphed but the new equilibrium point is.

## Question 3.1 -  Demand Shift - Intercept Slider
Shift the Demand Parameter - This slider is only shifting the intercept which leads to a parrallel shift in Demand. What happens to the amount of tax paid  as demand shifts out.  What happens to the ratio of tax paid from Consumer Surplus to Producer Surplus?

....  Type your answer in here

## Question 3.2 - Supply Shift - Slope Slider
Shift the Supply Parameter - This slider is shifting the slope which leads to a change in the steepness of the Supply curve. What happens to the amount of Consumer Surplus as supply becomes steeper?  What happens to the tax incidence - the ratio of tax coming from Consumer Surplus vs. Producer Surplus?

....  Type your answer in here

## Question 3.3 - Tax Shift - 
Shift the Tax Parameter - This slider is increasing or decreasing the amount of tax. 

Is it affecting the slope or the intercept? 

An increase in tax leads to and increase in the price paid by consumers. In this case how much does a given tax lead to an increase in price?


When shifting the tax alone - how does the incidence of tax between producers and consumers change?

....  Type your answer in here







--- END lec04/lec04-Supply-Demand.md ---



--- START lec04/lec04-four-plot-24.md ---

---
title: "lec04-four-plot-24"
type: lecture-notebook
week: 4
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-four-plot-24.ipynb"
---

<!--
https://pressbooks.bccampus.ca/uvicecon103/chapter/4-7-tariffs/
-->

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, EdX<br>
            Dr. Eric Van Dusen <br>
            Chris Pyles  <br>
</table>

# Lecture Notebook 4 - Tariffs: Four Plot

Play around with the sliders below to better understand how each of the variables are related!

```python
from utils24 import *
```

```python
four_plot_widget()
```





--- END lec04/lec04-four-plot-24.md ---



--- START lec04/lec04-four-plot.md ---

---
title: "lec04-four-plot"
type: lecture-notebook
week: 4
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec04/lec04-four-plot.ipynb"
---

<!--
https://pressbooks.bccampus.ca/uvicecon103/chapter/4-7-tariffs/
-->

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, EdX<br>
            Dr. Eric Van Dusen <br>
            Chris Pyles  <br>
</table>

# Lecture Notebook 4 - Tariffs: Four Plot

Play around with the sliders below to better understand how each of the variables are related!

```python
from utils import *
```

```python
four_plot_widget()
```





--- END lec04/lec04-four-plot.md ---



--- START lec05/Lec5-Cobb-Douglas.md ---

---
title: "Lec5-Cobb-Douglas"
type: lecture-notebook
week: 5
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec05/Lec5-Cobb-Douglas.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen
        </p></td></tr>
</table>

# Cobb-Douglas Regression and Penn World Table

The following code is in Pandas - a more advanced data science library you are not required to know.  
Just understanding the outputs of the following cells should be good!

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats
```

```python
pwt = pd.read_excel('pwt1001.xlsx', sheet_name='Data')
pwt
```

```python
pwt['countrycode'].unique()
```

```python
#Graphs of ln(Y/L) vs ln(K/L)
def graph_cobbs(ccodes, begin_date, end_date):
    for ccode in ccodes:
        first = pwt.loc[pwt["countrycode"] == ccode]
        second = first[ (first['year'] >= begin_date) & (first['year'] <= end_date) ]
        third = {}
        third['Y'] = second['cgdpe'] / second['cgdpe'].iloc[0]
        third['K'] = second['cn'] / second['cn'].iloc[0]
        third['L'] = second['emp'] / second['emp'].iloc[0]
        third['YL'] = third['Y'] /third['L']
        third['KL'] = third['K'] /third['L']
        third['lnYL'] = np.log(third['YL'])
        third['lnKL'] = np.log(third['KL'])
        third = pd.DataFrame(third)
        third = third.dropna()
        if(len(third)>0):
            f = plt.figure()
            ax = f.add_subplot(111)
            ax.scatter(third['lnKL'], third['lnYL'], label='')
            m, b, r_value, p_value, std_err = scipy.stats.linregress(third['lnKL'], third['lnYL'])
            ax.plot(third['lnKL'], m*third['lnKL'] + b, label='y = %.4f x + %.4f \n$R^2$ = %.4f' %(m, b, r_value**2))
            ax.legend()
            ax.set_xlabel('ln(K/L)')
            ax.set_ylabel('ln(Y/L)')
            plt.grid()
            ax.set_title(second['country'].iloc[0] + ' (' + ccode + '): '+ str(begin_date) + ' to '+ str(end_date))
            ax.text(0.0, 0.0, "Data Source: Penn World Tables", color='blue', fontstyle='italic', transform=f.transFigure)
         #   plt.savefig('Cobb-Douglas-' + ccode + '.png')
            plt.show()
```

```python
ccodes =['NER','SDN', 'IND', 'CHN', 'NOR', 'USA']
graph_cobbs(ccodes, 1994, 2017)
```

The coefficient of $x$ represents the relative impact of capital ($K$) on GDP ($Y$). Here is a brief refresher on $\alpha$ and the log form of the Cobb Douglas equation (seen above):
$$\ln{\frac{Y}{L}} = \alpha \ln{\frac{K}{L}} + A$$
$$e^{\ln{\frac{Y}{L}}} = e^{\alpha \ln{\frac{K}{L}} + A}$$
$$\frac{Y}{L} =  (\frac{K}{L})^{\alpha} * e^{A}$$
$$ Y = A K^{\alpha} L^{1 - \alpha}$$

```python
graph_cobbs(['RUS'], 1994, 2017)
graph_cobbs(['RUS'], 2003, 2017)
```

```python
#Graphs of Y and K vs time
begin_date = 1994
end_date = 2017
ccodes = pwt.countrycode.unique().tolist()
ccodes =['ZWE', 'RUS', 'CHN', 'VEN']
for ccode in ccodes:
    first = pwt.loc[pwt["countrycode"] == ccode]
    second = first[ (first['year'] >= begin_date) & (first['year'] <= end_date) ]
    third = {}
    third['year'] = second['year']
    third['Y'] = second['cgdpe'] / second['cgdpe'].iloc[0]
    third['K'] = second['cn'] / second['cn'].iloc[0]
    third['L'] = second['emp'] / second['emp'].iloc[0]
    third['YL'] = third['Y'] /third['L']
    third['KL'] = third['K'] /third['L']
    third['lnYL'] = np.log(third['YL'])
    third['lnKL'] = np.log(third['KL'])
    third = pd.DataFrame(third)
    third = third.dropna()
    if(len(third)>0):
        f = plt.figure(figsize=(10,6))
        ax = f.add_subplot(111)
        ax.scatter(third['year'], third['Y'], label='')
        m_y, b_y, r_value_y, p_value_y, std_err_y = scipy.stats.linregress(third['year'], third['Y'])
        ax.plot(third['year'], m_y*third['year'] + b_y, label='Y = %.4f x + %.4f \n$R^2$ = %.4f' %(m_y, b_y, r_value_y**2))
        ax.scatter(third['year'], third['K'], label='')
        m_k, b_k, r_value_k, p_value_k, std_err_k = scipy.stats.linregress(third['year'], third['K'])
        ax.plot(third['year'], m_k*third['year'] + b_k, label='K = %.4f x + %.4f \n$R^2$ = %.4f' %(m_k, b_k, r_value_k**2))
        ax.scatter(third['year'], third['L'], label='')
        m_l, b_l, r_value_l, p_value_l, std_err_l = scipy.stats.linregress(third['year'], third['L'])
        ax.plot(third['year'], m_l*third['year'] + b_l, label='L = %.4f x + %.4f \n$R^2$ = %.4f' %(m_l, b_l, r_value_l**2))
        ax.legend()
        ax.set_xlabel('Year')
        ax.set_ylabel('Y/K')
        plt.grid()
        ax.set_title(second['country'].iloc[0] + ' (' + ccode + '): '+ str(begin_date) + ' to '+ str(end_date))
        ax.text(0.0, 0.0, "Data Source: Penn World Tables", color='blue', fontstyle='italic', transform=f.transFigure)
       # plt.savefig('Cobb-Douglas-' + ccode + '.png')
        plt.show()
```

```python

```



--- END lec05/Lec5-Cobb-Douglas.md ---



--- START lec05/Lec5-CobbD-AER1928.md ---

---
title: "Lec5-CobbD-AER1928"
type: lecture-notebook
week: 5
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec05/Lec5-CobbD-AER1928.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
        </p></td></tr>
</table>

## Recreating Key Parts of the Cobb-Douglas Seminal Journal Article from 1928

This Jupyter Notebook aims to recreate and analyze key parts of the seminal journal article "A Theory of Production" by Charles W. Cobb and Paul H. Douglas, published in the American Economic Review in 1928. The article introduced the Cobb-Douglas production function, which has become a fundamental concept in economics.

### Overview

The Cobb-Douglas production function is a mathematical representation of the relationship between inputs (capital and labor) and output in the production process. The function is typically expressed as:
$$
Y = A \cdot K^\alpha \cdot L^\beta
$$

Where:

- \( Y \) is the total production (output),
- \( A \) is the total factor productivity,
- \( K \) is the capital input,
- \( L \) is the labor input,
- \( $\alpha$\) and \( $\beta$ \) are the output elasticities of capital and labor, respectively.
- In the simplified CD formulation \( $\alpha$\) + \( $\beta$ \) = 1

### Data and Analysis

**Dataset was manually compiled from the original article and is available in a csv file . The dataset contains historical data on labor, capital, and output for the United States from 1909 to 1929.**


In this notebook, we will:
1. Load and explore the historical data used by Cobb and Douglas.
2. Visualize the data to understand the trends in labor, capital, and output over time.
3. Estimate the parameters \( $\alpha$ \) and \( $A$ \) of the Cobb-Douglas production function using regression techniques.
4. Compare the predicted output values with the actual values to assess the model's accuracy.
5. Visualize the differences between the actual and predicted values.


### Steps

1. **Data Loading and Exploration**: We start by loading the historical data from a CSV file and exploring its structure.
2. **Data Visualization**: We create various plots to visualize the trends in labor, capital, and output over the years, seeking to make a plot that is reminiscent of the hand-drawn figure in the original article.
3. **Parameter Estimation**: Using regression techniques, we estimate the parameters \( \alpha \) and \( A \) of the Cobb-Douglas production function.
4. **Model Comparison**: We compare the predicted output values with the actual values and visualize the differences.

By following these steps, we aim to gain a deeper understanding of the Cobb-Douglas production function and its application to historical economic data.

**Please take a look at the original article for a more detailed explanation of the theory and methodology.**
From the 1928 Journal Article
https://www.jstor.org/stable/1811556


A Theory of Production
Charles W. Cobb, Paul H. Douglas
The American Economic Review, Vol. 18, No. 1, Supplement, Papers and Proceedings of the Fortieth Annual Meeting of the American Economic Association (Mar., 1928), pp. 139-165 (27 pages)

```python
from datascience import *
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker

import pandas as pd
%matplotlib inline
```

## Cobb Douglas graph of US Economic Output 1899-1922

![image.png](attachment:image.png)

```python
cobbd = Table.read_table('Cobb.csv')
cobbd
```

Step 1 - A simple plot over time using the datascience commands

```python
cobbd.select('Year', 'EmplIndex', 'CapitalIndex', 'PhProdIndex').plot('Year')
```

Step 2- Making the Y-Axis a log-scale

```python
cobbd.select('Year', 'EmplIndex', 'CapitalIndex', 'PhProdIndex').plot('Year')

plt.yscale('log')

plt.show()
```

Step 3 - Changing properties for th  Y-Axis

```python
cobbd.select('Year', 'EmplIndex', 'CapitalIndex', 'PhProdIndex').plot('Year')


plt.yscale('log')

plt.gca().set_yticks([100, 200, 300, 400, 500])  # Set specific y-tick values
plt.gca().get_yaxis().set_major_formatter(mticker.ScalarFormatter())  # Format as scalar

plt.show()
```

Step 4 - *Try to make the plot look like the original !*
- Change the properties of the lines
- add Gridlines
- Title and Legend

```python
cobbd.select('Year', 'EmplIndex', 'CapitalIndex', 'PhProdIndex').plot('Year')

plt.yscale('log')

# Customize line styles
empl_line, = plt.plot(cobbd.column('Year'), cobbd.column('EmplIndex'), label='Labor Index', color='black', marker='o', markerfacecolor='white', linewidth=2)
capital_line, = plt.plot(cobbd.column('Year'), cobbd.column('CapitalIndex'), label='Capital Index', color='black', marker='o', linewidth=2)
phprod_line, = plt.plot(cobbd.column('Year'), cobbd.column('PhProdIndex'), label='Physical Product Index', color='black', linewidth=2)

plt.gca().set_yticks([75, 100, 200, 300, 400, 500])
plt.gca().get_yaxis().set_major_formatter(mticker.ScalarFormatter())

# Add gridlines in black and white
plt.grid(True, which='both', linestyle='-', linewidth=0.5, color='gray')

# Customize title and labels
plt.title("PROGRESS IN MANUFACTURING \n 1899-1922  (1899=100)", fontsize=16)
plt.xlabel("Year", fontsize=14)

plt.legend([empl_line, capital_line, phprod_line], 
           ['LABOR FORCE', 'FIXED CAPITAL', 'PHYSICAL PRODUCT'],  # Legend labels
           loc='upper center',  # Move to center
           frameon=True)  # Optional: removes the legend box frame

plt.show()
```

##  Part 2 - Estimate Alpha and A
### Adding Columns for Ratios

1. **Add a Column for the Ratio of Capital to Labor:**
    - Use the variable names from the `cobbd` table.
    - Take the natural logarithms of the ratios
    - Add this as a new column to the `cobbd` table.

2. **Add a Column for the Ratio of Output to Labor:**
    - Use the variable names from the `cobbd` table.
    - Take the natural logarithms of the ratios
    - Add this as a new column to the `cobbd` table.

```python

cobbd = cobbd.with_column("ln(K/L)", np.log(cobbd.column("CapitalIndex")/cobbd.column("EmplIndex")))

cobbd = cobbd.with_column("ln(Y/L)", np.log(cobbd.column("PhProdIndex")/cobbd.column("EmplIndex")))
cobbd
```

### Estimating the Coefficients of the Cobb-Douglas Production Function

To estimate the coefficients of the Cobb-Douglas production function, we will use the `np.polyfit` function from the NumPy library. This function will help us perform a linear regression on the logarithmic ratios of capital to labor (\( \ln(K/L) \)) and output to labor (\( \ln(Y/L) \)).

The `log_ratios` table contains the necessary data for this estimation. By fitting a linear model to these logarithmic ratios, we can derive the coefficients \( \alpha \) and \( A \) of the Cobb-Douglas production function.

The steps involved are as follows:
1. Use `np.polyfit` to perform a linear regression on the `log_ratios` table.
2. Extract the coefficients from the regression output.
3. Interpret these coefficients in the context of the Cobb-Douglas production function.

This approach allows us to quantify the relationship between capital, labor, and output in the production process, providing valuable insights into the underlying economic dynamics.

```python
CDestimates = np.polyfit(cobbd.column("ln(K/L)"), cobbd.column("ln(Y/L)"), 1)
```

```python
alpha = CDestimates.item(0)
beta = 1-alpha
A = np.exp(CDestimates.item(1))
```

```python
print("alpha: ", alpha)
print("beta: ", beta)
print("A: ", A)
```

### Regression Analysis Using Statsmodels

In addition to using NumPy's `polyfit` function, we can also perform the regression analysis using the `statsmodels` library. This allows us to regress $ \ln(Y/L) $ on $\ln(K/L)$ and obtain detailed statistical information about the regression model.

The steps involved are as follows:
1. Use `statsmodels` to perform an Ordinary Least Squares (OLS) regression.
2. Compare the results with those obtained using `np.polyfit`.

This approach is beyond the scope of Data 8 but would be familiar to those who have taken an econometrics class

```python
import statsmodels.api as sm

sm.OLS(cobbd.column("ln(Y/L)"), sm.add_constant(cobbd.column("ln(K/L)"))).fit().summary()
```

```python
# get the coefficients for alpha from the statsmodels output
model = sm.OLS(cobbd.column("ln(Y/L)"), sm.add_constant(cobbd.column("ln(K/L)"))).fit()
alpha2 = model.params[1]
A2= np.exp(model.params[0])
print("alpha2: ", alpha2)
print("A2: ", A2)
```

What have we learned?
 - Np.polyfit returns the same output as statsmodels
 - Cobb-Douglas got the right result before computers had been invented

Now lets set this up to make  predictions for Y  and add that column to our table 
$$
A \cdot K^{\alpha} \cdot L^{1 - \alpha}
$$

or 
$$
1.01 \cdot K^{0.25} \cdot L^{0.75}
$$

or 
$$
1.01 \cdot K^{\frac{1}{4}} \cdot L^{\frac{1}{4}}
$$

```python

cobbd = cobbd.with_column("PredictedY", A*cobbd.column("CapitalIndex")**alpha*cobbd.column("EmplIndex")**(1-alpha))
cobbd
```

```python
# now we need a line plot the actual values of Y vs year and a pplot of predicted Y vs year 
cobbd.select('Year', 'PhProdIndex', 'PredictedY').plot('Year')
```

```python
# Let's add the difference between the actual and predicted values of Y to the cobbd table
cobbd = cobbd.with_column("Difference", cobbd.column("PredictedY") - cobbd.column("PhProdIndex") )
# lets add the percentage difference to the table which is actual - predicted / actual * 100
cobbd = cobbd.with_column("Percent Difference", cobbd.column("Difference")/cobbd.column("PhProdIndex")*100)
cobbd.show(24)
```

And if we plot this we can compare it to 'Chart IV' in the original paper ( page 155)

```python
cobbd.select('Year', 'Difference').plot('Year')
```

```python
cobbd.select('Year', 'Difference').plot('Year')

# Add gridlines in black and white
plt.grid(True, which='both', linestyle='-', linewidth=0.5, color='gray')

# Customize title and labels
plt.title("Percentage Deviations \n  of Computed from Actual Product\n  1899-1922", fontsize=16)
plt.xlabel("Year", fontsize=14)
```

Not quite matching the graph in the article?





--- END lec05/Lec5-CobbD-AER1928.md ---



--- START lec06/6.1-Sympy-Differentiation.md ---

---
title: "6.1-Sympy-Differentiation"
type: lecture-notebook
week: 6
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.1-Sympy-Differentiation.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen
        </p></td></tr>
</table>

# Using Sympy to take Derivatives

```python
import sympy as sp
import numpy as np
import matplotlib.pyplot as plt
```

## Let's start with the Cobb - Douglas Production Function

$Q = A  \cdot L^{\alpha} \cdot K^\beta$

$\frac{dQ}{dL} = A \cdot \alpha \cdot L^{\alpha - 1} \cdot K^\beta$

$\frac{d^2Q}{dL^2} = A \cdot \alpha (\alpha - 1) \cdot L^{\alpha - 2} \cdot K^\beta$

Define the symbols and variables
 - which symbols will we need for the Cobb Douglas formulation?
 - Declare them as symbols

```python
Q, A, L, K, alpha, beta = sp.symbols('Q A L K alpha beta')
```

Define the Cobb-Douglas production function
 - writing this equation in its Python formulation
 - using the symbols we have already defined

```python
cobb_douglas = A * L**alpha * K**beta
```

Calculate the **first derivative** with respect to **L**

- This is a *partial derivative*

- sympy.diff ( original equation, thing you are taking the derivative over ) 

- Save that as an equation for future use!

```python
first_derivative = sp.diff(cobb_douglas, L)
```

Print this result
 - you can just ask sympy to print - spprint ()
 - you can add some text "First Derivative is.."

```python
print("First Derivative:")
sp.pprint(first_derivative)
```

This doesnt Look so great 
- lets pass in our function and get sympy to simplify it
- what form you want an equaiton to be in can take many forms

```python
simplified_first_derivative = sp.simplify(first_derivative)
print(simplified_first_derivative)
```

Print Result - Variation
- you can also save as Latex !
- It won't render yet - but you can copy paste this into Markdown cell

```python
first_derivative_latex = sp.latex(first_derivative)
print("First Derivative (LaTeX):")
print(first_derivative_latex)
```

$\frac{A K^{\beta} L^{\alpha} \alpha}{L}$

Calculate the **second derivative** with respect to **L**
 - use the first derivative that you saved
 - take the derivatie again with respect to variable of interes, L
 - save that equation

```python
second_derivative = sp.diff(first_derivative, L)
```

```python
sp.pprint(second_derivative)
```

```python
second_derivative_latex = sp.latex(second_derivative)
print(second_derivative_latex)
```

Pasting this into a Markdown cell and adding "dollar signs"

$\frac{A K^{\beta} L^{\alpha} \alpha^{2}}{L^{2}} - \frac{A K^{\beta} L^{\alpha} \alpha}{L^{2}}$

First Derivative (LaTeX):
$\frac{A K^{\beta} L^{\alpha} \alpha}{L}$

Second Derivative (LaTeX):
$\frac{A K^{\beta} L^{\alpha} \alpha^{2}}{L^{2}} - \frac{A K^{\beta} L^{\alpha} \alpha}{L^{2}}$

Lets simplify both of these and generat the Latex

```python
first_div_simp_latex = sp.latex(sp.simplify(first_derivative))
print(first_div_simp_latex)
```

```python
second_div_simp_latex = sp.latex(sp.simplify(sp.diff(first_derivative, L)))
print(second_div_simp_latex)
```

$$A K^{\beta} L^{\alpha - 1} \alpha$$
$$A K^{\beta} L^{\alpha - 2} \alpha \left(\alpha - 1\right)$$

## Now Let's consider the simpler case of a Utility function 
- that has a Cobb-Douglas type formulation 
- consists of consuming two goods $X1$ and $X2$

$  U = \cdot X1^{\alpha} \cdot X2^{1-\alpha}$

```python
U, X1, X2, alpha = sp.symbols('U, X1, X2, alpha ')
```

```python
U =  X1**alpha * X2**(1-alpha)
```

```python
first_deriv_x1 = sp.diff(U,X1)
```

```python
second_deriv_x1 = sp.diff(first_deriv_x1, X1)
```

```python
sp.pprint(first_deriv_x1)
```

```python
sp.pprint(second_deriv_x1)
```

```python
sp.simplify(first_deriv_x1)
```

```python
sp.simplify( second_deriv_x1)
```

```python
first_deriv_x1_latex = sp.latex(sp.simplify(first_deriv_x1))
print(first_deriv_x1_latex)
second_deriv_x1_latex = sp.latex(sp.simplify(second_deriv_x1))
print(second_deriv_x1_latex)
```

First Derivative (LaTeX):
$X_{1}^{\alpha - 1} X_{2}^{1 - \alpha} \alpha$

Second Derivative (LaTeX):
$X_{1}^{\alpha - 2} X_{2}^{1 - \alpha} \alpha \left(\alpha - 1\right)
$

Lets get Numeric!
- Lets declare some variables as constants ( alpha, x2) 
- and pass in a range for X1

```python
alpha = 0.5  # You can change the value of alpha as needed
X1_values = np.linspace(0.1, 2.0, 100)  # Range of X1 values
X2 = 1.0  # Fixed value for X2
```

```python
X1_values
```

```python
U_values = X1_values**alpha * X2**(1 - alpha)
U_values
```

```python
first_derivative_values = alpha * X1_values**(alpha - 1) * X2**(1 - alpha)
first_derivative_values
```

```python
second_derivative_values = alpha * (alpha - 1) * X1_values**(alpha - 2) * X2**(1 - alpha)
second_derivative_values
```

```python
plt.figure(figsize=(8, 6))
plt.plot(X1_values, U_values, label='U')
plt.plot(X1_values, first_derivative_values, label='First Derivative')
plt.plot(X1_values, second_derivative_values, label='Second Derivative')
plt.xlabel('X1')
plt.ylabel('First Derivative')
plt.title('First Derivative and Second Derivative of U')
plt.legend()
plt.grid(True)
plt.show()
```

```python


# Create a figure with two subplots (one for U and one for the first derivative)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Plot the original function U on the left subplot
ax1.plot(X1_values, U_values, label='U')
ax1.set_xlabel('X1')
ax1.set_ylabel('U')
ax1.set_title('Utility Function ')

# Plot the first derivative on the right subplot
ax2.plot(X1_values, first_derivative_values, label='First Derivative', color='orange')
ax2.set_xlabel('X1')
ax2.set_ylabel('First Derivative')
ax2.set_title('First Derivative')

# Show the plot
plt.show()
```

```python

```



--- END lec06/6.1-Sympy-Differentiation.md ---



--- START lec06/6.2-3D-utility.md ---

---
title: "6.2-3D-utility"
type: lecture-notebook
week: 6
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.2-3D-utility.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, <br>
            Dr. Eric Van Dusen <br>
        Akhil Venkatesh <br>
</table>

```python
import pandas as pd
import numpy as np
#import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')
import plotly.io as pio
pio.renderers.default = 'notebook'
```

# Utility Functions and Indifference Curves

## What is Utility?

When we consume a good, we assume that the good will have some impact on our total utility. Utility is a fundamental measure that helps economists model how consumers make decisions. An assumed rule in economics is that consumers will always act rationally, which translates to the assumption that consumers will always attempt to maximize their own utility. 

It is important to note that utility doesn't have specified units and even the face value of utility doesn't have any meaning. *What does an apple providing 5 utility units even mean?* What is valuable, however, is that utility can be compared; if an apple provides 5 utility units and an orange provides 3 utility units, then we prefer apples to oranges.

As a very simple example, say Anne has 6 dollars and she can choose to buy any combination of goods A and B. If good A costs 2 dollars and provides 5 utility units per unit of A consumed, while good B costs 3 dollars and provides 6 utility units per unit of B consumed, then Anne will buy 3 units of good A, since that maximizes her utility. 

In economics, however, our models are a little more complex than that. Typically, utility is the product of the consumption of many goods; typically having a lot of one good but not another does not provide much utility. In addition, consumption of one good faces diminishing marginal returns, i.e. holding all things equal, the consumption of one additional unit of a good will provide less utility than the utility received from the previous unit. Intuitively, imagine Bob is very hungry and decides to eat slices of pizza. The first slice of pizza will bring Bob the most utility, but the 8th slice will be much less satisfying to eat.

## Utility Functions
A consumer's utility is determined by the amount of consumption from all the goods they consume. Typically, utility functions are multivariate: they take in multiple inputs (which represent the different amounts of consumption for each good, which we call a consumption bundle), and output one value, the utility. Today, we'll only look at the case where consumers can only choose between 2 goods $x_1$ and $x_2$. Hence, a utility function can be represented by: $u(x_1,x_2)$. 

With that in mind, let's start graphing some utility functions!

### Cobb-Douglas Utility Function

Consider the following utility function across $x_1$ and $x_2$:

$$u(x_1, x_2)=x_1^{\alpha}x_2^{1-\alpha}\quad\text{where } 0<\alpha<1$$

This is known as the **Cobb-Douglas utility function**. To visualize this function, we'll need a 3D plot.

```python
alpha = 0.5
def cobb_douglas(x1, x2):
    return (x1 ** alpha) * (x2 ** (1-alpha))
x1 = np.linspace(0,10,10)
x2 = np.linspace(0,10,10)
X1,X2 = np.meshgrid(x1,x2)
z = cobb_douglas(X1,X2)
data = [go.Surface(z=z, contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=True,highlightcolor="#42f462",project=dict(z=True))))]
layout = go.Layout(title='Cobb-Douglas Utility Function (alpha = 0.5)',autosize=False,width=500,height=500,margin=dict(l=65,r=50,b=65,t=90),
scene = dict(xaxis = dict(title='X1'),yaxis = dict(title=r'X2'),zaxis = dict(title='Utility'),))
fig = go.Figure(data=data, layout=layout)

plot(fig, filename="fig1.html", auto_open=False)
display(HTML("fig1.html"))
```

## Examining the Utility Function

There are 2 rules that utility functions generally follow: 

- Non-negative marginal utility: the consumption of a good will not decrease the utility. Economists generally assume that 'more is better.' If the consumption of a good decreased utility, then we would consume less of a good. 
- Diminishing marginal returns: all else equal, as consumption increases the marginal utility derived from each additional unit declines.

### Non-negative Marginal Utility
Say we are currently consuming 2 units of $x_1$ and $x_2$ each with $\alpha = \frac{1}{2}$, providing $u(2,2)=2^{0.5}2^{0.5}=2$ utility units. One additional unit of $x_1$ will provide me a higher point of utility: we can verify this result both graphically and numerically: $u(3,2)=3^{0.5}2^{0.5}\approx2.45$. Indeed, consuming one more unit of a good should increase our utility!

### Marginal Utility and the Law of Diminishing Returns
Now let's check for the second result: diminishing marginal returns. From above, we know that holding the consumption of $x_2$ constant at 2, going from 2 to 3 units of $x_1$ increases our utility by $2.45-2=0.45$. Going from 3 to 4 units of $x_1$ brings our utility to $u(4,2)=4^{0.5}2^{0.5}\approx 2.83$, an increase of $2.83-2.45=0.38$ utility units.

Using calculus, we can more formally define the marginal utility of a good. Since marginal utility is the change in utility that one additional unit of consumption provides (holding all others constant), the marginal utility with respect to $x_1$ is its partial derivative: $\frac{\partial u}{\partial x_1}$. In our case:

$$
\begin{aligned}
\textrm{Marginal Utility of } x_1: &\quad\frac{\partial u}{\partial x_1} = \frac{1}{2}x_1^{-0.5}x_2^{0.5} \\
\textrm{Marginal Utility of } x_2: &\quad\frac{\partial u}{\partial x_2} = \frac{1}{2}x_1^{0.5}x_2^{-0.5}
\end{aligned}
$$

Or, more generally,

$$\begin{aligned}
\textrm{Marginal Utility of } x_1: &\quad\frac{\partial u}{\partial x_1} = \alpha x_1^{\alpha-1}x_2^{1-\alpha} \\
\textrm{Marginal Utility of } x_2: &\quad\frac{\partial u}{\partial x_2} = (1-\alpha) x_1^{\alpha}x_2^{-\alpha}
\end{aligned}$$


With marginal utility defined, note that both conditions can be explained using the marginal utility function $\frac{\partial u}{\partial x}$: 

- Non-negative marginal utility: $\frac{\partial u}{\partial x} \geq 0$
- Diminishing marginal returns: $\frac{\partial^2 u}{\partial x^2} < 0$

## Indifference Curves

Although the utility function above in 3D is cool, you'll typically find utility graphs to be in 2D with $x_1$ and $x_2$ as the axis (eliminating the utility axis). 

To represent utility levels, we plot a set of indifference curves on the 2D graph. An indifference curve satisfies the property in which **any point on the curve has the exact same amount of utility**, so that consumers are _indifferent_ to any point on the curve. In our 3D plot, any point on the indifference curve has the exact same height, which represents the value of utility. If you're familar with contour plots, you can also think of indifference curves as following the same idea.

```python
alpha = 0.5
utilities = np.arange(1, 9)
x1_indiff_val = np.linspace(0,50,1000)
x2_indiff_vals = []
for u in utilities:
    x2_indiff_vals.append(((u/(x1_indiff_val ** alpha)) ** (1/(1-alpha))))
traces = []
colors = ['blue', 'red','green','purple'] + ['blue', 'red','green','purple']
for u, c, x2 in zip(utilities, colors, x2_indiff_vals):
    traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2,
    name = 'utility = ' + str(u),
    line = dict(color = c,width = 1)))

data = traces

# Edit the layout
layout = dict(title = 'Indifference Curves for the Cobb-Douglas Utility Function (alpha = ' + str(alpha) + ')',
              xaxis = dict(title = 'X1', range = [0,10]),
              yaxis = dict(title = 'X2', range = [0,10]),)

fig = dict(data=data, layout=layout)

plot(fig, filename="fig2.html", auto_open=False)
display(HTML("fig2.html"))
```



# Budget Constraints and Utility Maximization

In this section, we will assume that $\alpha = 0.5$ (i.e. the utility function is: $u(x_1, x_2) = x_1^{0.5}x_2^{0.5}$).

Now we introduce the concept of money into our model. Consumers face a budget constraint when choosing to maximize their utility. Given an income $M$ and prices $p_1$ for good $x_1$ and $p_2$ for good $x_2$, the consumer can at most spend up to $M$ for both goods:

$$M \geq p_1x_1 + p_2x_2$$

Since goods will always bring non-negative marginal utility, consumers will try to consume as many goods as they can. Hence, we can rewrite the budget constraint as an equality instead (since if they have more income leftover, they will use it to buy more goods).

$$M = p_1x_1 + p_2x_2$$

This means that any bundle of goods $(x_1,x_2)$ that consumers choose to consume will adhere to the equality above. What does this mean on our graph? Let's examine the indifference curve plots, assuming that $M = 32$, and $p_1 =2$ and $p_2 = 4$.

```python
M = 32
p_1 = 2
p_2 = 4

# Plot default indifference curves
utilities = np.arange(1, 9)
x1_indiff_val = np.linspace(0,50,1000)
x2_indiff_vals = []
for u in utilities:
    x2_indiff_vals.append(((u/(x1_indiff_val ** (1/2))) ** (2)))
traces = []
colors = ['blue', 'red','green','purple'] + ['blue', 'red','green','purple']
for u,c,x2 in zip(utilities,colors,x2_indiff_vals):
    traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2,
    name = 'utility = ' + str(u),
    line = dict(color = c,width = 1)))
    
# for i in range(len(traces) - 4):
#     del traces[-1] # This is a hacky method to not continually append to TRACES upon an update from the slider.
x2_bc_val = (M - (p_1*x1_indiff_val))/p_2
traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2_bc_val,
    name = 'Budget Constraint',
    line = dict(color = 'black',width = 1,dash="dot")))
data = traces
layout = dict(title = 'Budget Constraint and Indifference Curves for the Cobb-Douglas Utility Function (alpha = 0.5)',
              xaxis = dict(title = 'X1', range = [0,18]),
              yaxis = dict(title = 'X2', range = [0,10]),)
fig = dict(data=data, layout=layout)

plot(fig, filename="fig3.html", auto_open=False)
display(HTML("fig3.html"))
```

The budget constraint is like a possibilities curve: moving up or down the constraint means gaining more of one good while sacrificing the other.

Let's take a look at what this budget constraint means. Because of the budget constraint, any bundle of goods $(x_1,x_2)$ that consumers ultimately decide to consume will lie on the budget constraint line. Adhering to this constraint where $M=32, p_1 = 2, p_2 = 4$, we can see that consumers will be able to achieve 2 units of utility, and can also achieve 4 units of utility. But what is the maximum amount of utility that consumers can achieve? 

Notice an interesting property about indifference curves: **the utility level of the indifference curves gets larger as we move up and to the right.** Hence, the maximizing amount of utility in this budget constraint is the rightmost indifference curve that still touches the budget constraint line. In fact, it'll only 'touch' (and not intersect) the budget constraint and be tangential to it.

```python
M = 32
p_1 = 2
p_2 = 4

# Plot default indifference curves
utilities = np.arange(1, 9)
x1_indiff_val = np.linspace(0,50,1000)
x2_indiff_vals = []
for u in utilities:
    x2_indiff_vals.append(((u/(x1_indiff_val ** (1/2))) ** (2)))
traces = []
colors = ['blue', 'red','green','purple'] + ['blue', 'red','green','purple']
for u,c,x2 in zip(utilities,colors,x2_indiff_vals):
    traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2,
    name = 'utility = ' + str(u),
    line = dict(color = c,width = 1)))

# PLOT BC
x2_bc_val = (M - (p_1*x1_indiff_val))/p_2
traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2_bc_val,
    name = 'Budget Constraint',
    line = dict(color = 'black',width = 1,dash="dot")))


# PLOT MAX UTIL INDIFF CURVE
max_utility = ((1/2*M/p_1) ** (1/2)) * ((1/2*M/p_2) ** (1/2))
x2_max_util = (max_utility/(x1_indiff_val ** (1/2))) ** 2
x2_max_util = (max_utility/(x1_indiff_val ** (1/2))) ** 2
traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2_max_util,
    name = 'Maximized Utility = ' + str(round(max_utility, 2)),
    line = dict(color = 'black',width = 2)))
data = traces

layout = dict(title = 'Budget Constraint and Indifference Curves for the Cobb-Douglas Utility Function (alpha = 0.5)',
              xaxis = dict(title = 'X1', range = [0,20]),
              yaxis = dict(title = 'X2', range = [0,15]),)
fig = dict(data=data, layout=layout)

plot(fig, filename="fig4.html", auto_open=False)
display(HTML("fig4.html"))
```

Notice that as the price of one good increases, the indifference curve that represents the maximum attainable utility shifts towards the left (i.e. the max utility decreases). Intuitively, this makes sense. As the price of one good increases, consumers have to make adjustments to their consumption bundles and buy less of one, or both, goods. Hence, their maximum utility will decrease.

Let's visualize the budget constraint in 3D where $M=30, p_1=3, p_2=3$. Here, any point along the curve in which the 2 planes intersect represents an amount of utility in which the budget constraint holds true (i.e. where we've spent all our income). The utility maximizing quantity is a point on this intersecting curve at which the utility level is the highest.

```python
def cobb_douglas(x1, x2):
    return (x1 ** (1/2)) * (x2 ** (1/2))
x1 = np.linspace(0,10,10)
x2 = np.linspace(0,10,10)
X1,X2 = np.meshgrid(x1,x2)
z = cobb_douglas(X1,X2)

def budget_constraint(x1, x2):
    return 10000*(3*x1 + 3*x2 - 30) # We multiply this by 10000 to get a very steep plane, which should be similar to the actual BC, a vertical plane.

z2 = budget_constraint(X1, X2)

data = [go.Surface(
    z=z, contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=True,highlightcolor="#42f462",
                                                              project=dict(z=True))), name="Cobb-Douglas Utility Function"),
       go.Surface(
   z=z2, contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=False,
                              highlightcolor="#42f462",project=dict(z=True))),showscale=False, colorscale="balance", name="Budget Constraint")]
layout = go.Layout(
    title='Cobb-Douglas Utility Function with Budget Constraint', autosize=False,width=500, height=500, margin=dict(l=65,r=50,b=65,t=90),
    scene = dict(xaxis = dict(title='X1', range = [0,10]), yaxis = dict(title='X2'),
    zaxis = dict(title = 'Utility', nticks=4, range = [0,10],)))
fig = go.Figure(data=data, layout=layout)

plot(fig, filename="fig5.html", auto_open=False)
display(HTML("fig5.html"))
```





--- END lec06/6.2-3D-utility.md ---



--- START lec06/6.3-QuantEcon-Optimization.md ---

---
title: "6.3-QuantEcon-Optimization"
type: lecture-notebook
week: 6
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.3-QuantEcon-Optimization.ipynb"
---

# Optimization from QuantEcon

This is a Lecture written by the QuantEcon project
https://datascience.quantecon.org/scientific/optimization.html



**Prerequisites**

- [Introduction to Numpy](https://datascience.quantecon.org/numpy_arrays.html)  
- [Applied Linear Algebra](https://datascience.quantecon.org/applied_linalg.html)  


**Outcomes**

- Perform optimization by hand using derivatives  
- Understand ideas from gradient descent

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```

## What is Optimization?

Optimization is the branch of mathematics focused on finding extreme values (max or min) of
functions.

Optimization tools will appear in many places throughout this course, including:

- Building economic models in which individuals make decisions that maximize their utility.  
- Building statistical models and maximizing the fit of these models by optimizing certain fit
  functions.  


In this lecture, we will focus mostly on the first to limit the moving pieces, but in other lectures, we’ll discuss the second in detail.

### Derivatives and Optima

Here, we revisit some of the theory that you have already learned in your calculus class.

Consider function $ f(x) $ which maps a number into another number. We can say that any point
where $ f'(x) = 0 $ is a local extremum of $ f $.

Let’s work through an example. Consider the function

$$
f(x) = x^4 - 3 x^2
$$

Its derivative is given by

$$
\frac{\partial f}{\partial x} = 4 x^3 - 6 x
$$

Let’s plot the function and its derivative to pick out the local extremum by hand.

```python
def f(x):
    return x**4 - 3*x**2


def fp(x):
    return 4*x**3 - 6*x

# Create 100 evenly spaced points between -2 and 2
x = np.linspace(-2., 2., 100)

# Evaluate the functions at x values
fx = f(x)
fpx = fp(x)

# Create plot
fig, ax = plt.subplots(1, 2)

ax[0].plot(x, fx)
ax[0].set_title("Function")

ax[1].plot(x, fpx)
ax[1].hlines(0.0, -2.5, 2.5, color="k", linestyle="--")
ax[1].set_title("Derivative")

for _ax in ax:
    _ax.spines["right"].set_visible(False)
    _ax.spines["top"].set_visible(False)
```

If you stare at this picture, you can probably determine the the local maximum is at
$ x = 0 $ and the local minima at $ x \approx -1 $ and $ x \approx 1 $.

To properly determine the minima and maxima, we find the solutions to $ f'(x) = 0 $ below:

$$
f'(x) = 4 x^3 - 6 x = 0
$$

$$
\rightarrow x = \left\{0, \frac{\sqrt{6}}{2}, \frac{-\sqrt{6}}{2} \right\}
$$

Let’s check whether we can get the same answers with Python! To do this, we import a new
package that we haven’t seen yet.

```python
import scipy.optimize as opt
```

Then using the function definitions from earlier, we search for the minimum and maximum values.

```python
# For a scalar problem, we give it the function and the bounds between
# which we want to search
neg_min = opt.minimize_scalar(f, [-2, -0.5])
pos_min = opt.minimize_scalar(f, [0.5, 2.0])
print("The negative minimum is: \n", neg_min)
print("The positive minimum is: \n", pos_min)
```

The scipy optimize package only has functions that find minimums… You might be wondering, then, how we
will verify our maximum value.

It turns out that finding the maximum is equivalent to simply finding the minimum of the negative function.

```python
# Create a function that evaluates to negative f
def neg_f(x):
    return -f(x)

max_out = opt.minimize_scalar(neg_f, [-0.35, 0.35])
print("The maximum is: \n", max_out)
```

We won’t dive into the details of optimization algorithms in this lecture, but we’ll impart some brief
intuition to help you understand the types of problems these algorithms are good at solving and
the types of problems they will struggle with:

The general intuition is that when you’re finding a maximum, an algorithm takes a step
in the direction of the derivative… (Conversely, to find a minimum, the algorithm takes a step opposite the direction of the derivative.)
This requires the function to be relatively smooth and continuous. The algorithm also has an easier time if there is only one (or very few) extremum to be found…

For minimization, you can imagine the algorithm as a marble in a bowl.

The marble will keep rolling down the slope of the bowl until it finds the bottom.

It may overshoot, but once it hits the slope on the other side, it will continue to roll back
and forth until it comes to rest.

Thus, when deciding whether numerical optimization is an effective method for a
particular problem, you could try visualizing the function to determine whether a marble
would be able to come to rest at the extreme values you are looking for.

### Application: Consumer Theory

A common use of maximization in economics is to model
optimal consumption decisions [https://en.wikipedia.org/wiki/Consumer_choice](https://en.wikipedia.org/wiki/Consumer_choice).

#### Preferences and Utility Functions

To summarize introductory economics, take a set of
[preferences](https://en.wikipedia.org/wiki/Preference_%28economics%29) of consumers over “bundles”
of goods (e.g. 2 apples and 3 oranges is preferred to 3 apples and 2 oranges, or a 100% chance to
win $ 1 $ dollar is preferred to a 50% chance to win $ 2.10 $ dollars).

Under certain assumptions, you rationalize the preferences as a utility function over the different
goods (always remembering that the utility is simply a tool to order preferences and the numbers are
usually not meaningful themselves).

For example, consider a utility function over bundles of bananas (B) and apples (A)

$$
U(B, A) = B^{\alpha}A^{1-\alpha}
$$

Where $ \alpha \in [0,1] $.

First, let’s take a look at this particular utility function.

```python
def U(A, B, alpha=1/3):
    return B**alpha * A**(1-alpha)

fig, ax = plt.subplots()
B = 1.5
A = np.linspace(1, 10, 100)
ax.plot(A, U(A, B))
ax.set_xlabel("A")
ax.set_ylabel("U(B=1.5, A)")
```

We note that

- $ U(B,1) $ is always higher with more B, hence, consuming more bananas has a
  : positive marginal utility  i.e. $ \frac{d U(B,1)}{d B} > 0 $.  
- The more bananas we consume, the smaller the change in marginal utility, i.e.
  $ \frac{d^2 U(B,1)}{d B^2} < 0 $.  


If we plot both the $ B $ and the $ A $, we can see how the utility changes with different
bundles.

```python
fig, ax = plt.subplots()
B = np.linspace(1, 20, 100).reshape((100, 1))
contours = ax.contourf(A, B.flatten(), U(A, B))
fig.colorbar(contours)
ax.set_xlabel("A")
ax.set_ylabel("B")
ax.set_title("U(A,B)")
```

We can find the bundles between which the consumer would be indifferent by fixing a
utility $ \bar{U} $ and by determining all combinations of $ A $ and $ B $ where
$ \bar{U} = U(B, A) $.

In this example, we can implement this calculation by letting $ B $ be the variable on the
x-axis and solving for $ A(\bar{U}, B) $

$$
A(B, \bar{U}) = U^{\frac{1}{1-\alpha}}B^{\frac{-\alpha}{1-\alpha}}
$$

```python
def A_indifference(B, ubar, alpha=1/3):
    return ubar**(1/(1-alpha)) * B**(-alpha/(1-alpha))

def plot_indifference_curves(ax, alpha=1/3):
    ubar = np.arange(1, 11, 2)
    ax.plot(B, A_indifference(B, ubar, alpha))
    ax.legend([r"$\bar{U}$" + " = {}".format(i) for i in ubar])
    ax.set_xlabel("B")
    ax.set_ylabel(r"$A(B, \bar{U}$)")

fig, ax = plt.subplots()
plot_indifference_curves(ax)
```

Note that in every case, if you increase either the number of apples or bananas (holding the other
fixed), you reach a higher indifference curve.

Consequently, in a world without scarcity or budgets, consumers would consume
an arbitrarily high number of both to maximize their utility.

#### Budget Constraints

While the above example plots consumer preferences, it says nothing about what the consumers can afford.

The simplest sort of constraint is a budget constraint where bananas and apples both have a price
and the consumer has a limited amount of funds.

If the prices per banana and per apple are identical, no matter how many you consume, then the
affordable bundles are simply all pairs of apples and bananas below the line.
$ p_a A + p_b B \leq W $.

For example, if consumer has a budget of $ W $, the price of apples is $ p_A = 2 $ dollars per
apple, and the price of bananas is normalized to be $ p_B = 1 $ dollar per banana, then the consumer
can afford anything below the line.

$$
2 A + B \leq W
$$

Or, letting $ W = 20 $ and plotting

```python
def A_bc(B, W=20, pa=2):
    "Given B, W, and pa return the max amount of A our consumer can afford"
    return (W - B) / pa

def plot_budget_constraint(ax, W=20, pa=2):
    B_bc = np.array([0, W])
    A = A_bc(B_bc, W, pa)
    ax.plot(B_bc, A)
    ax.fill_between(B_bc, 0, A, alpha=0.2)
    ax.set_xlabel("B")
    ax.set_ylabel("A")
    return ax

fig, ax = plt.subplots()
plot_budget_constraint(ax, 20, 2)
```

While the consumer can afford any of the bundles in that area, most will not be optimal.

#### Optimal Choice

Putting the budget constraints and the utility functions together lets us visualize the optimal
decision of a consumer. Choose the bundle with the highest possible indifference curve within its
budget set.

```python
fig, ax = plt.subplots()
plot_indifference_curves(ax)
plot_budget_constraint(ax)
```

We have several ways to find the particular point $ A, B $ of maximum utility, such as
finding the point where the indifference curve and the budget constraint have the same slope, but a
simple approach is to just solve the direct maximization problem.

$$
\begin{aligned}
\max_{A, B} & B^{\alpha}A^{1-\alpha}\\
\text{s.t. } & p_A A + B \leq W
\end{aligned}
$$

Solving this problem directly requires solving a multi-dimensional constrained optimization problem,
where scipy [https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize)
has several options.

For this particular problem, we notice two things: (1) The utility function is increasing in both
$ A $ and $ B $, and (2) there are only 2 goods.

This allows us 1) to assume that the budget constraint holds at equality, $ p_a A + B = W $, 2) to
form a new function $ A(B) = (W - B) / p_a $ by rearranging the budget constraint at equality, and
3) to substitute that function directly to form:

$$
\max_{B}  B^{\alpha}A(B)^{1-\alpha}
$$

Compared to before, this problem has been turned into an unconstrained univariate optimization
problem.

To implement this in code, notice that the $ A(B) $ function is what we defined before
as `A_bc`.

We will solve this by using the function `scipy.optimize.minimize_scalar`, which takes a function
`f(x)` and returns the value of `x` that minimizes `f`.

```python
from scipy.optimize import minimize_scalar

def objective(B, W=20, pa=2):
    """
    Return value of -U for a given B, when we consume as much A as possible

    Note that we return -U because scipy wants to minimize functions,
    and the value of B that minimizes -U will maximize U
    """
    A = A_bc(B, W, pa)
    return -U(A, B)

result = minimize_scalar(objective)
optimal_B = result.x
optimal_A = A_bc(optimal_B, 20, 2)
optimal_U = U(optimal_A, optimal_B)

print("The optimal U is ", optimal_U)
print("and was found at (A,B) =", (optimal_A, optimal_B))
```

This allows us to do experiments, such as examining how consumption patterns change as prices or
wealth levels change.

But First, Let's fix the graph!  
Lets add UBar to the Set of Indifference Curves

```python
def A_indifference(B, ubar, alpha=1/3):
    return ubar**(1/(1-alpha)) * B**(-alpha/(1-alpha))

def plot_indifference_curves(ax, alpha=1/3):
    ubar = np.arange(1, 11, 2)
    ubar = np.append(ubar, optimal_U)
    ax.plot(B, A_indifference(B, ubar, alpha))
    ax.legend([r"$\bar{U}$" + " = {}".format(i) for i in ubar])
    ax.set_xlabel("B")
    ax.set_ylabel(r"$A(B, \bar{U}$)")

fig, ax = plt.subplots()
plot_indifference_curves(ax)
```

Then lets set the optimal level of consumpton of A and B
and add them to the graph

```python
fig, ax = plt.subplots()
plot_indifference_curves(ax)
plot_budget_constraint(ax)
ax.plot(optimal_B, optimal_A, 'go', label=f"Optimal Point (A, B) = ({optimal_A:.2f}, {optimal_B:.2f})")
ax.annotate(f"({optimal_A:.2f}, {optimal_B:.2f})", (optimal_B, optimal_A),
            textcoords="offset points", xytext=(5,5), ha='center')
```

```python

```



--- END lec06/6.3-QuantEcon-Optimization.md ---



--- START lec06/6.4-latex.md ---

---
title: "6.4-latex"
type: lecture-notebook
week: 6
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.4-latex.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong><br>
            Dr. Eric Van Dusen <br>
            Akhil Venkatesh
             </p></td></tr>
</table>

## Writing Equations in LaTeX
### Start by building up from elements

Let's start writing equations and make them more complicated little by little - let's use the $ sign to offset the equations within the cell. All of these will be in Markdown cells.  When we use two dollar signs we get the equation displayed on its own line.

Start with a function that takes two arguments K and L
$$
f(K, L) 
$$

then we want to use a greek symbol $alpha$
that we will use a \ to denote
$$
\alpha
$$

And we want to raise the value of K to the power of $\alpha$
$$
K^\alpha
$$

**Your turn!** You can go ahead and do the same thing for $L$ to the power of $\beta$
$$
.... 
$$

Let's combine these elements to be the basis of the Cobb-Douglas Production Function:
$$
f(K, L) = K^\alpha L^\beta \\
$$

For modeling National GDP we can also add in $Y$ and the technology shifter $A$
$$
Y = A \cdot f(K, L) = A K^\alpha L^\beta
$$

A common simplification is to constrain $ \alpha + \beta = 1 $ which can be rearranged to
$$
\beta = 1 - \alpha
$$

First Derivative
$$
X_{1}^{\alpha - 1} X_{2}^{1 - \alpha} \alpha
$$

Second Derivative 

$$
X_{1}^{\alpha - 2} X_{2}^{1 - \alpha} \alpha \left(\alpha - 1\right)
$$

So then we can rewrite the original equation as: 
$$
Y = A K^\alpha L^{1 - \alpha}
$$

## Writing Equations in LaTeX
Try completing some of these exercises as practice!

If you get stuck, check out the LaTeX Guide: https://math.hws.edu/gassert/LaTeX_Guide_Title.pdf!

**1. Quadratic Formula:**

![quadratic.PNG](attachment:quadratic.PNG)

**Try it out!**
$$
.... 
$$

# Lets try Markdown as well!

# Here's how you get each of the different heading sizes!
___
# Heading 1 
## Heading 2 
### Heading 3 
#### Heading 4 
##### Heading 5 
###### Heading 6

# How about a List ( unordered) 
 1. here is one item
 2. here is a second item
 3. here is a third

## Here is how to make a table

| Id | Label | Price |
|--- |----------| ------|
| 01 | Markdown |\$1600 |
| 02 | is | \$12 |
| 03 | AWESOME | \$999 |

**bold text**

*italics*

`code`

```python

```

# How about an Emoji?
## You can copy and paste  or. 

😊

&#128512;

# How about a link
[What a cool class!](https://data88e.org)



```python

```



--- END lec06/6.4-latex.md ---



--- START lec06/6.5-Edgeworth.md ---

---
title: "6.5-Edgeworth"
type: lecture-notebook
week: 6
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec06/6.5-Edgeworth.ipynb"
---

## Edgeworth Box Demo
### as another application of Cobb-Douglas Utility Function


The code is taken from "Gaby Galvan" 
https://deepnote.com/app/gaby-galvan/Edgeworth-Box-5c892517-cb51-4f3f-b20d-8e5dd8b5ac46

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

```python
out = 0
```

### Overview of the Edgeworth Box

An **Edgeworth box** is a graphical representation used in microeconomics to illustrate the distribution of resources (goods) between two consumers in a simple exchange economy. It shows all possible allocations of two goods between two consumers and helps analyze their preferences, trade possibilities, and potential for reaching Pareto-efficient outcomes.

In your scenario:

- **Two Consumers**: The consumers are trading Noodles and Cereal.
- **Objective**: To visualize how these consumers can trade to reach mutually beneficial (Pareto-efficient) allocations based on their utility preferences.

---

### Key Components of the Code

Let’s break down the main elements of your code to understand how it represents the economic concepts:

#### 1. Utility Functions (`U1` and `U2`)

- **Purpose**: Represent the satisfaction (utility) each consumer derives from consuming Noodles (X) and Cereal (Y).
- **Form**: Must be functions of two variables, e.g., `U1(x, y)` and `U2(x, y)`.
- **Interpretation**: The shape of the utility functions reflects each consumer's preferences and marginal rates of substitution (MRS) between Noodles and Cereal.

#### 2. Edgeworth Box Dimensions (`Xmax`, `Ymax`, `Xmin`, `Ymin`)

- **`Xmax` and `Ymax`**: Total available quantities of Noodles and Cereal in the economy.
- **`Xmin` and `Ymin`**: Set close to zero to avoid computational issues with logarithmic utilities or division by zero.
- **Interpretation**: The box's dimensions represent the entire feasible set of allocations between the two consumers.

#### 3. Indifference Curves

- **Definition**: Curves along which a consumer derives the same level of utility.
- **Implementation**: The code uses `plt.contour()` to draw these curves for both consumers.
- **Economic Intuition**:
    - **Consumer 1**: Indifference curves are plotted from the origin at (0,0).
    - **Consumer 2**: Indifference curves are plotted from the opposite corner (Xmax, Ymax), reflecting that any consumption by Consumer 1 reduces the amount available to Consumer 2.

#### 4. Contract Curve

- **Definition**: The set of allocations where the indifference curves of the two consumers are tangent—i.e., where their MRS are equal.
- **Implementation**:
    - Calculated by finding where the gradients of the utility functions are proportional.
    - The code computes gradients using `np.gradient()` and identifies where the determinant of the Jacobian (`out`) is zero.
    - Plotted using `plt.contour()` with specific levels.
- **Economic Intuition**: Represents all Pareto-efficient allocations—no consumer can be made better off without making the other worse off.

#### 5. Allocation Point (`AlPoint`)

- **Purpose**: Represents a specific allocation of Noodles and Cereal between the two consumers.
- **Usage**:
    - If provided, the code plots this point and the corresponding indifference curves passing through it for both consumers.
    - Helps visualize the initial endowment and potential gains from trade.
- **Economic Intuition**: Demonstrates how consumers can move from an initial allocation to a Pareto-efficient allocation through mutually beneficial trade.

#### 6. Utility Levels and Number of Indifference Curves (`Utility_Show`, `Num_ind`)

- **`Utility_Show`**: When `True`, displays utility levels on the indifference curves.
- **`Num_ind`**: Controls the number of indifference curves plotted for each consumer.
- **Economic Intuition**: More curves provide a detailed view of preferences but may clutter the graph; adjusting `Num_ind` balances detail and clarity.

#### 7. Other Parameters

- **Labels (`Xlab`, `Ylab`)**: Axis labels for clarity—here, they should be set to "Noodles" and "Cereal."
- **Colors (`colors`)**: Customize the appearance of the plot elements for better distinction.
- **Endowment (`e`)**: Number of steps used in calculations—higher values lead to smoother curves but increase computation time.

---

### Economic Intuition Behind the Code

#### Mutually Beneficial Trade

- **Starting Point**: Consumers begin with an initial allocation of Noodles and Cereal.
- **Objective**: Through trade, both aim to reach higher indifference curves, thus achieving higher utility levels.
- **Visualization**: The movement from the initial allocation point towards the contract curve represents this process.

#### Marginal Rate of Substitution (MRS)

- **Definition**: The rate at which a consumer is willing to trade one good for another while maintaining the same utility level.
- **Equal MRS**: At points on the contract curve, both consumers have equal MRS, meaning they value the trade-off between Noodles and Cereal equally.
- **Economic Intuition**: Equating MRS ensures that resources are allocated efficiently between the two consumers.

#### Pareto Efficiency

- **Definition**: An allocation is Pareto-efficient if no consumer can be made better off without making the other worse off.
- **Contract Curve**: All points along this curve are Pareto-efficient allocations.
- **Visualization**: The contract curve in the Edgeworth box shows all such allocations.

#### Utility Maximization

- **Consumers' Goal**: Each aims to reach the highest possible indifference curve given their budget constraints (the total available goods).
- **Constraints**: Limited by the total quantities of Noodles and Cereal in the economy.
- **Trade-Offs**: Consumers make choices based on their preferences, as depicted by the shapes of their indifference curves.


### Interpreting the Graph

#### Axes

- **Horizontal Axis**: Quantity of Noodles consumed by Consumer 1 (Alice).
- **Vertical Axis**: Quantity of Cereal consumed by Alice.
- **Origin for Consumer 2 (Bob)**: Located at the top-right corner

```python
 
def draw_Ed_Bow(U1,U2, Xmax, Ymax, Xmin=10**(-6), Ymin=10**(-6), Utility_Show = False, Num_ind = 10,Xlab ="X",Ylab="Y",e=200, Contract_draw=True,AlPoint = None,colors =["black","Orange", "blue","red"],Utility_draw = True):
    """
    Input : 
        U1: Utility of the 1st agent (must depend on 2 variables)
        U2: Utility of the 2nd agent (must depend on 2 variables)
        Xmax : the limit of the box
        Ymax: the limit of the box
        Xmin=10**(-6): the limit of the box (default: set to ~0 to avoid problems with log expressions)
        Ymin=10**(-6): the box limit (default: set to ~0 to avoid problems with log expressions)
        Utility_Show = False: Show utility levels on the Edgeworth box
        Num_ind = 10 : Number of indifference curves per agent
        e = 200 : Number of steps to compute the utility levels and the contract curve
        Contract_draw = True: Draw the contract curve 
        AlPoint = None : show an allocation point and his 2 indifference curves. Should be a tuple/list (x,y)
        colors = ["black","Orange", "blue","red"] : To choose the color of : [ContractCurve, endowment point, indifference curves Agent1, indifference curves agent2]
        Utility_draw = True : Draw the indefrence curves
    Output:
        None (but draws the Edgeworth box) 
    """
    delta = min((Xmax-Xmin)/e,(Ymax-Ymin)/e)

    x = np.arange(Xmin, Xmax, delta) #Tomates
    y = np.arange(Ymin, Ymax, delta) #courgettes
    X, Y = np.meshgrid(x, y)
    Z1 = lambda x,y : U1(x,y)
    Z2 = lambda x,y : U2(Xmax-x,Ymax-y)

    #the contract curve
    Num_ind_1 = Num_ind
    Num_ind_2 = Num_ind

    if Contract_draw == True:
        Z2grad = np.gradient(Z2(X,Y))
        Z1grad = np.gradient(Z1(X,Y))

        global out
        out = (Z2grad[0]*Z1grad[1]-Z2grad[1]*Z1grad[0])

        Cont = plt.contour(X,Y,out,colors=colors[0],levels=[0])
        fmt = {}
        strs = ["Contract  curve"]
        for l, s in zip(Cont.levels, strs):
            fmt[l] = s
        plt.clabel(Cont, Cont.levels, inline = True,
                fmt = fmt, fontsize = 10)
        
        C_curv = abs(pd.DataFrame(out ,index=y, columns=x))
        C_curv = C_curv.index @ (C_curv == C_curv.apply(min))
        xC_curv = np.arange(Xmin,Xmax,(Xmax-Xmin)/(Num_ind+1))
        C_curv = np.interp(xC_curv,C_curv.index,C_curv)
        Num_ind_1 = pd.Series(Z1(xC_curv,C_curv)).sort_values(ascending=True)
        Num_ind_2 = pd.Series(Z2(xC_curv,C_curv)).sort_values(ascending=True)
        
    #Draw the dotation point and his curves
    if AlPoint != None:
        plt.scatter(AlPoint[0],AlPoint[1],s=200,marker=".",color = colors[1],label="Allocation point")
        Num_ind_1 = [Z1(AlPoint[0],AlPoint[1])]
        Num_ind_2 = [Z2(AlPoint[0],AlPoint[1])]

    #draw the indifference curve
    if Utility_draw == True:
        C1 = plt.contour(X, Y, Z1(X,Y),colors = colors[2],levels=Num_ind_1)
        C2 = plt.contour(X, Y, Z2(X,Y),colors = colors[3],levels=Num_ind_2)
        if Utility_Show == True:
            fmt = {}
            strs = round(pd.Series(C1.levels[:]),1)
            for l, s in zip(C1.levels, strs):
                fmt[l] = s
            plt.clabel(C1, C1.levels, inline = True,
                    fmt = fmt, fontsize = 10)
            #Utility level2

            fmt = {}
            strs = round(pd.Series(C2.levels[:]),1)
            for l, s in zip(C2.levels, strs):
                fmt[l] = s
            plt.clabel(C2, C2.levels, inline = True,
                    fmt = fmt, fontsize = 10)


    plt.title("Edgeworth box")
    plt.xlabel(Xlab)
    plt.ylabel(Ylab)
```

## We can then set the parameters for Alpha and Beta in the Cobb - Douglas formula

##  Let's consider the simpler case of a Utility function 
- that has a Cobb-Douglas type formulation 
- consists of consuming two goods $X1$ and $X2$

$  U = \cdot X1^{\alpha} \cdot X2^{1-\alpha}$

```python
#Utility of 1st agent (depend on X,Y):
U1 = lambda c,n : c**0.5 * n**0.5
#Utility of 2nd agent (depend on X,Y):
U2 = lambda c,n : c**0.5 * n**0.5


draw_Ed_Bow(U1,U2,18,30,colors=["k","Orange", "lightblue","mistyrose"],Num_ind=3)
draw_Ed_Bow(U1,U2,18,30,Xlab="Cereal",Ylab="Noodles",AlPoint=(10,16),Contract_draw=False)
plt.show()
```

```python
#Utility of 1st agent (depend on X,Y):
U1 = lambda c,n : c**0.7 * n**0.3
#Utility of 2nd agent (depend on X,Y):
U2 = lambda c,n : c**0.5 * n**0.5


draw_Ed_Bow(U1,U2,18,30,colors=["k","Orange", "lightblue","mistyrose"],Num_ind=3)
draw_Ed_Bow(U1,U2,18,30,Xlab="Cereal",Ylab="Noodles",AlPoint=(11.5,13),Contract_draw=False)
plt.show()
```

## General parameters for $\alpha $ and $ \beta $

```python
alpha1 = 0.5
beta1 = 0.5
alpha2 = 0.7
beta2 = 0.3
```

```python
#Utility of 1st agent (depend on X,Y):
U1 = lambda c,n :n**alpha1 * c**beta1
#Utility of 2nd agent (depend on X,Y):
U2 = lambda c,n : n**alpha2 * c**beta2


draw_Ed_Bow(U1,U2,30,18,colors=["k","Orange", "lightblue","mistyrose"],Num_ind=3)
draw_Ed_Bow(U1,U2,30,18,Xlab="Cereal",Ylab="Noodles",AlPoint=(18,7),Contract_draw=False)
plt.show()
```

```python

```



--- END lec06/6.5-Edgeworth.md ---



--- START lec07/7.1-inequality.md ---

---
title: "7.1-inequality"
type: lecture-notebook
week: 7
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec07/7.1-inequality.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 23<br>
            Dr. Eric Van Dusen <br>
        Akhil Venkatesh <br>
</table>

From the Textbook Chapter
https://data-88e.github.io/textbook/content/06-inequality/inequality.html

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datascience import *
%matplotlib inline
#plt.style.use('seaborn-muted')
```

# Lec 7.1  Measurements of Inequality

## The Lorenz Curve
The Lorenz Curve visually presents income inequality by plotting household income percentile on the $x$-axis, and the cumulative income share that the bottom $x$ percentile own on the $y$-axis. The households are sorted by income, so that the first household at the 0th percentile has the least amount of income, while the household at the 100th percentile has the greatest income.

For any point $(x,y)$ on the Lorenz curve, “the bottom x percent own y% of the income”. For example, if the $x$-axis reads 0.30 and $y$-axis reads 0.10, then it means that the bottom 30% of the population received 10% of the total population's income. This yields 2 implications for the Lorenz Curve:
- The points $(0,0)$ and $(1,1)$ are always on the curve. $(0,0)$ represents the 0% of the population owning 0% of the income and $(1,1)$ represents 100% of the population owning 100% of the income.
- The slope is always increasing. This is because households are sorted by income as percentiles: for a slight increase in $x$, households become richer and hence provide a larger share of total income.

### Line of Perfect Equality
In a world of perfect equality, everyone would have the exact same income. In this case, the Lorenz curve would just be a 45-degree straight line that runs through $(0,0)$ and $(1,1)$, i.e. $y=x$. Mathematically, this is because the derivative is constant: for a slight increase in $x$, the total share of income increases at a constant rate. Another way to think about this is that the bottom 10% of the population will own 10% of the total income, the bottom 50% of the population will own 50% of the total income, and so on. This line is known as the *line of perfect equality*, and we typically display this line when plotting our Lorenz curves as a reference.

### A Toy Example
Let's suppose country 1 has the following income distribution: 
- The bottom 10% owns a cumulative 2% of total income 
- The bottom 20% owns 5% of total income
- The bottom 30% owns 9% of total income
- The bottom 40% owns 15% of total income 
- The bottom 50% owns 23% of total income
- The bottom 60% owns 33% of total income
- The bottom 70% with 45% of total income
- The bottom 80% with 59% of total income 
- The bottom 90% with 75% of total income
- The bottom 100% with 100% of total income

We will create an array of income shares and call it `Country1`.

```python
Country1 = make_array(0, 0.02, 0.05, 0.09, 0.15, 0.23, 0.33, 0.45, 0.59, 0.75, 1.0)
```

To better see this information, we will create a table containing population percentage and cumulative income share.

```python
income_distribution = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country1 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution
```

How will the Lorenz Curve for this income distribution look?

```python
income_distribution.scatter(0, 1, width=11, height=7)
plt.plot([0,100], [0,100], color='k');
```

### Comparing Lorenz Curves
Now let's compare 2 countries' Lorenz curves. Suppose country 2 has the following income distribution:
- The bottom 10% owns a cumulative 3% of total income 
- The bottom 20% owns 7% of total income
- The bottom 30% owns 13% of total income
- The bottom 40% owns 19% of total income 
- The bottom 50% owns 27% of total income
- The bottom 60% owns 37% of total income
- The bottom 70% with 50% of total income
- The bottom 80% with 65% of total income 
- The bottom 90% with 81% of total income
- The bottom 100% with 100% of total income

```python
Country2 = make_array(0, 0.03, 0.07, 0.13, 0.19, 0.27, 0.37, 0.5, 0.65, 0.81, 1.0)
income_distribution2 = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country2 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution2
```

Comparing the 2 countries' income distributions side by side:

```python
income_distribution.join(
    ["Population Percentage (%)", "Perfect Equality Income Share (%)"], 
    income_distribution2, ["Population Percentage (%)", "Perfect Equality Income Share (%)"]
).relabel(
    "Cumulative Income Share (%)", "Country 1 Cumulative Income Share (%)"
).relabel(
    "Cumulative Income Share (%)_2", "Country 2 Cumulative Income Share (%)"
)
```

Plotting both countries' Lorenz curves, can you tell which country has a higher level of income inequality?

```python
plt.figure(figsize=[7,7])
plt.plot(income_distribution.column(0), income_distribution.column(1), "-o", c = 'b')
plt.plot(income_distribution.column(0), income_distribution2.column(1), "-o", c = 'r')
plt.legend(["Country 1", "Country 2"])
plt.plot([0,100], [0,100], color='k');
```

In this case, we can see that country 2's Lorenz curve is closer to the line of equality than that of country 1, which intuitively would suggest that country 2 is more equal. If we were to look at the numbers, we see that the bottom percentiles own a higher % of total national income in country 2 than in country 1, while top percentiles own less in country 2 than in country 1. This would suggest that country 2 is more equal in income than country 1, so that country 1 has a higher level of income inequality.

But now let's consider a different case; suppose country 3 has the following distribution:

```python
Country3 = make_array(0, 0.03, 0.07, 0.12, 0.18, 0.25, 0.33, 0.42, 0.54, 0.73, 1.0)
income_distribution3 = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country3 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution.join(
    ["Population Percentage (%)", "Perfect Equality Income Share (%)"], 
    income_distribution3, ["Population Percentage (%)", "Perfect Equality Income Share (%)"]
).relabel(
    "Cumulative Income Share (%)", "Country 1 Cumulative Income Share (%)"
).relabel(
    "Cumulative Income Share (%)_2", "Country 3 Cumulative Income Share (%)"
)
```

```python
plt.figure(figsize=[7,7])
plt.plot(income_distribution.column(0), income_distribution.column(1), "-o", c = 'b')
plt.plot(income_distribution.column(0), income_distribution3.column(1), "-o", c = 'r')
plt.legend(["Country 1", "Country 3"])
plt.plot([0,100], [0,100], color='k');
```

Now, ambiguity arises; while bottom income percentiles earn a larger share of national income in country 3, top income percentiles also have a larger share. We can visualize this phenomenon by the 'crossing' of Lorenz curves on the plot. As a result, we do cannot easily tell which country has a higher level of income inequality.

As you may see, the Lorenz curve is not able to produce a 'quantitative' measure of income inequality, making the scenario above hard for us to compare the 2 countries. For this, we turn to the Gini coefficient.

## The Gini Coefficient

We can use the Gini coefficeint to quantify the level of income inequality.

!<img src="Gini.png"  />

The **Gini coefficient** is the ratio of the area between the line of equality and the Lorenz curve to the total area under the line of equality. Referring to $A$ and $B$ from {numref}`gini-coefficient`:

$$\text{Gini} = \frac{\text{Area between line of equality and Lorenz curve}}{\text{Area under line of equality}} = \frac{A}{A+B}$$

If we express the Lorenz curve as $L(x)$, we can use calculus to derive an equation for the Gini coefficient:

$$\text{Gini} = \frac{\frac{1}{2} - \int_0^1 L(x)\text{d}x}{\frac{1}{2}} = 1 - 2\int_0^1 L(x)\text{d}x$$
```

Intuitively, the closer the Lorenz curve is to the line of equality, the lower income inequality exists. Hence, the smaller the area of A, the lower the inequality. **This means that the smaller the Gini coefficient, the lower the income inequality.** Also note that the Gini coefficient will always be between 0 and 1. Mathematically, since $A$ and $B$ are both positive, $0<\frac{A}{A+B}<1$.

```python
# This function estimates the Gini coefficient. You don't have to understand how this code works below.
def gini(distribution):
    sorted_distribution = sorted(distribution)
    height = 0
    area = 0
    for i in sorted_distribution:
        height += i
        area += height - i / 2
    fair_area = height * len(distribution) / 2.
    return (fair_area - area) / fair_area
```

When we use our population as the parameter to the `gini` function, we get:

```python
gini_coefficient_country1 = gini(Country1)
gini_coefficient_country1
```

```python
gini_coefficient_country2 = gini(Country2)
gini_coefficient_country2
```

```python
gini_coefficient_country3 = gini(Country3)
gini_coefficient_country3
```

These results confirm our intuition from the analysis we did previously via Lorenz curves. Previously, we concluded that country 1 had a higher level of income inequality than country 2, and this is supported by country 1's higher gini coefficient. On the other hand, we had trouble comparing levels of inequality between country 1 and country 3. Here, the gini coefficient would indicate that country 1 has a higher level of income inequality than country 3.

# Income Inequality Historically

<!-- Written by Amal Bhatnagar -->

In the last chart on the previous page, you may have noticed that income inequality was rising in the United States in the last few decades. We will examine this in more detail, and also observe global trends in inequality.

## The United States

Let's look at historical trends of income inequality in the US over the last 100 years. The data has been collected from [The World Inequality Database](https://wid.world/), which is co-directed by Berkeley Economics professors Emanuel Saez and Gabriel Zucman. Specifically, we will observe income distributions for the bottom 50 percent, top 10 percent, and top 1 percent.

```python
us_hist = Table.read_table("US_inequality.csv")
us_hist.show(5)
```

```python
us_hist.take(np.arange(100,105))
```

Let's begin with some data cleaning: it seems like our 3 brackets are 'vertically stacked' on top of each other. Instead, we would like a table with 5 columns: `Year`, `bottom 50% income share`, `top 10% income share`, and `top 1% income share`.

```python
bottom_50_us = us_hist.where("Percentile", "p0p50").drop("Percentile").relabeled("Income Share", "Bottom 50% Share")
top_10_us = us_hist.where("Percentile", "p90p100").drop("Percentile").relabeled("Income Share", "Top 10% Share")
top_1_us = us_hist.where("Percentile", "p99p100").drop("Percentile").relabeled("Income Share", "Top 1% Share")
us_hist_joined = bottom_50_us.join("Year", top_10_us).join("Year", top_1_us)
us_hist_joined
```

Oh no, there are some `nan` values! NaN (not a number) values are very common in real world datasets: often, we may not have some observations simply because no data was collected, or perhaps the data collected was faulty. Sometimes, we can try to impute or replace NaN values in order to avoid having gaps in our data, but for now let's ignore NaNs and when plotting to see what's going on:

```python
# mpl.rcParams['figure.dpi'] = 120
us_hist_joined.plot("Year", width=11, height=7)
plt.title("Income Share over Time", fontsize = 16)
plt.ylabel("Proportion", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

# Income Inequality for the Rest of the World

Now let's examine the trends of income inequality in other parts of the world.

```python
world_hist = Table.read_table("World_Inequality.csv")
bottom_50_world = world_hist.where("Percentile", "p0p50").drop("Percentile")
top_10_world = world_hist.where("Percentile", "p90p100").drop("Percentile")
top_1_world = world_hist.where("Percentile", "p99p100").drop("Percentile")
top_10_world
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

Just like the US, it seems global inequality has been rising around the world, especially in China, India, Russia, and across Europe. However, in absolute terms, the level of income inequality in Europe is much lower than that in the United States. 

Also look at Russia: income inequality spiked up around 1991. This was likely caused by the fall of the USSR: the failing Soviet state left the ownership of state assets uncontested, which allowed former USSR officials to acquire state property through informal deals. This led to the rise of many Russian oligarchs - those who rapidly accumulated wealth during the era of Russian privatization directly follwing the dissolution of the Soviet Union.

```python
top_10_world.select("Year", "USA", "Europe").plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

```python

```

```python

```



--- END lec07/7.1-inequality.md ---



--- START lec07/7.2-historical-inequality.md ---

---
title: "7.2-historical-inequality"
type: lecture-notebook
week: 7
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec07/7.2-historical-inequality.ipynb"
---

```python
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from datascience import *
%matplotlib inline
plt.style.use('seaborn-v0_8-muted')
mpl.rcParams['figure.figsize'] = (10.0, 10.0)
```

# Income Inequality Historically

<!-- Written by Amal Bhatnagar -->

In the last chart on the previous page, you may have noticed that income inequality was rising in the United States in the last few decades. We will examine this in more detail, and also observe global trends in inequality.

## The United States

Let's look at historical trends of income inequality in the US over the last 100 years. The data has been collected from [The World Inequality Database](https://wid.world/), which is co-directed by Berkeley Economics professors Emanuel Saez and Gabriel Zucman. Specifically, we will observe income distributions for the bottom 50 percent, top 10 percent, and top 1 percent.

```python
us_hist = Table.read_table("US_inequality.csv")
us_hist.show(5)
```

```python
us_hist.take(np.arange(100,105))
```

Let's begin with some data cleaning: it seems like our 3 brackets are 'vertically stacked' on top of each other. Instead, we would like a table with 5 columns: `Year`, `bottom 50% income share`, `top 10% income share`, and `top 1% income share`.

```python
bottom_50_us = us_hist.where("Percentile", "p0p50").drop("Percentile").relabeled("Income Share", "Bottom 50% Share")
top_10_us = us_hist.where("Percentile", "p90p100").drop("Percentile").relabeled("Income Share", "Top 10% Share")
top_1_us = us_hist.where("Percentile", "p99p100").drop("Percentile").relabeled("Income Share", "Top 1% Share")
us_hist_joined = bottom_50_us.join("Year", top_10_us).join("Year", top_1_us)
us_hist_joined
```

Oh no, there are some `nan` values! NaN (not a number) values are very common in real world datasets: often, we may not have some observations simply because no data was collected, or perhaps the data collected was faulty. Sometimes, we can try to impute or replace NaN values in order to avoid having gaps in our data, but for now let's ignore NaNs and when plotting to see what's going on:

```python
# mpl.rcParams['figure.dpi'] = 120
us_hist_joined.plot("Year", width=11, height=7)
plt.title("Income Share over Time", fontsize = 16)
plt.ylabel("Proportion", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

# Income Inequality for the Rest of the World

Now let's examine the trends of income inequality in other parts of the world.

```python
world_hist = Table.read_table("World_Inequality.csv")
bottom_50_world = world_hist.where("Percentile", "p0p50").drop("Percentile")
top_10_world = world_hist.where("Percentile", "p90p100").drop("Percentile")
top_1_world = world_hist.where("Percentile", "p99p100").drop("Percentile")
top_10_world
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

Just like the US, it seems global inequality has been rising around the world, especially in China, India, Russia, and across Europe. However, in absolute terms, the level of income inequality in Europe is much lower than that in the United States. 

Also look at Russia: income inequality spiked up around 1991. This was likely caused by the fall of the USSR: the failing Soviet state left the ownership of state assets uncontested, which allowed former USSR officials to acquire state property through informal deals. This led to the rise of many Russian oligarchs - those who rapidly accumulated wealth during the era of Russian privatization directly follwing the dissolution of the Soviet Union.

```python
top_10_world.select("Year", "USA", "Europe").plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

##  The Elephant Graph

```{figure} elephant_curve.jpg
---
width: 500px
name: elephant-curve
---
The elephant curve {cite}`10inequality-elephantCurve`
```

The elephant curve is a graph that shows the real income growth per adult across each income group’s percentile around the world. 

There are 3 key features of the elephant curve: a hump for the world’s poorest, valley for the middle class, and trunk for the upper class. The thump is made of the world’s poorest countries, most likely those from developing countries. The valley comprises the working class from the developed world and upper class from developing countries. The trunk is made of people from the upper class from developed countries. The hump and valley indicate growth among emerging countries, and the top global 1%’s growth is higher than any other income group, thus explaining the positively sloped shape of the trunk. 


A study done by the Brookings Institution] found that “poorer countries, and the lower income groups within those countries, have grown most rapidly in the past 20 years” {cite}`10inequality-brookings`. This supports the World Bank’s claim that inequality between countries and within countries is decreasing. The Brookings Institution used only household surveys, however, which usually excludes the top and bottom percentile of the population, due to non-response bias. Still, the study is useful in corroborating the trends and growth in global income inequality.

# Factors that Affect Income Inequality

Economists have isolated multiple factors that influence a country's income inequality
- top marginal tax rates
- unemployment rates
- population growth

We will look at each of these scenarios independently and see its overall trends

## Top marginal tax rates

Let's also take a look at the top marginal tax rates in the United States throughout this time. Overall, the United States (and most of the rest of the world) has a progressive tax system, which means that the more income you earn, the higher percentage you will be taxed at. A good way to reduce income inequality is through progressive taxation; having the richer paying a higher portion of their income will help increase equality. Currently, the top marginal tax rate is 37%, as we can see in the table below.

```{figure} MTR.png
---
width: 500px
name: irs
---
Marginal tax rates. Image from the IRS
```

The top marginal tax rate only applies to the portion of your income above a certain income level. For example, if you earned 19501 dollars in 2019, then you will pay 1940 dollars plus 12% of $19501-19401$, i.e. 12 dollars. For another example, if you earned 80000 dollars, then you will pay $9086 + 0.22(80000-78950) = 9317$ dollars in tax, effectively a $\frac{9317}{80000} = 11.6\%$ tax rate.

In general, the idea is you will pay a lower tax rate for your first $x$ dollars, but a higher rate for dollars earned over $x$.

Now let's look at the historical trends in marginal top tax rates, which is the % taxed at the highest tax bracket.

```python
toptax = Table.read_table("toptaxrate.csv")
toptax
```

```python
# mpl.rcParams['figure.dpi'] = 120
toptax.plot(0,1, width=11, height=7)
plt.title("Top Marginal Tax Rate over Time", fontsize = 18)
plt.show()
```

```python
# mpl.rcParams['figure.dpi'] = 120
us_hist_joined.plot("Year", width=11, height=7)
plt.title("Income Share over Time", fontsize = 18)
plt.ylabel("Proportion", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

This graph depicts income inequality decreasing between 1910 and 1970 and increasing from 1970 to present. 

In 1913, Congress implemented the current income tax to promote equality. Originally meant to help compensate for revenue lost from reducing high tariffs, the new policy essentially made the top 1% start contributing to taxes. Additionally, the top marginal tax rate increased from 7% in 1913 to 73% in 1918, thus helping reduce income inequality. Right before the Great Depression, income inequality peaked, where the richest 1% possessed 19.6% of all income. During the Great Depression, top marginal tax rates increased, peaking at 94% in 1944. The top marginal tax rate decreased but remained high over subsequent decades, where it was 70% in 1965 and 50% in 1982. These high top marginal tax rates are correlated with low income inequality. During the Great Depression, the richest 1% had about 15% of total income. For the 10 years after the Great Depression, the top 1% had below 10% of total income and 8% for the 30 years afterwards. This period was known as the Great Compression, as income differentials between the top 1% and the rest of the country decreased.

In the 1970s, the economy took a turn for the worse with high unemployment and inflation (stagflation) and low growth. In order to stimulate economic growth, the government reduced top marginal tax rates (70% to 38.5% in 1980s), deregulated corporate institutions, and attacked labor union memberships (membership decreased by half within 30 years). Although these policies improved economic growth, it resulted in higher income inequality.

The graph below better shows that the share of income earned by the bottom 50% percentile steadily decreased, while the share earned by the top 1% increased steadily. This means that the top 1% has more wealth than the entire bottom 50% of the population. Suppose a class of 100 people has \$100 in aggregate. In a world with perfect equality, each person would have \$1. With this current level of income inequality, one person would have more wealth than 50 people combined. 

The media continues to report on the nation's significant income disparity. [The Washington Post wrote a story](https://www.washingtonpost.com/business/2019/09/26/income-inequality-america-highest-its-been-since-census-started-tracking-it-data-show/) and found that “The number of families earning \$15,000 or less has fallen since 2007, according to the latest census data, while the number of households bringing in \$250,000 a year or more has grown more than 15 percent.” 

Can we conclude that high marginal tax rates lead to low income inequality but slow economic growth?

## Unemployment rates

Economists believe that unemployment is one of the leading factors that leads to income inequality. When looking at what influences the Gini coefficient, a paper from [Princeton](https://rpds.princeton.edu/sites/rpds/files/media/menendez_unemployment_ar.pdf) found that the unemployment rate had the largest effect on the income inequality rates

Below, we look at the unemployment rates for the past 20 years across many different countries. These are the same countries and regions that we will further study below.

```python
unemployment = Table.read_table("Unemployment.csv")
unemployment
```

As we can see from the graph, the unemployment rates for China, India and the rest of the world have stayed somewhat steady. On the other hand, Brazil, the US, Russia and Europe are encountering drastically different unemployment situations than before.

```python
# mpl.rcParams['figure.dpi'] = 120
unemployment.plot("Year", width=11, height=7)
plt.ylabel("Unemployment Rate (%)", fontsize = 14)
plt.xlabel("Year", fontsize =14)
plt.title("Unemployment Rate over Time", fontsize = 18)
plt.show()
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", size=14)
plt.xlabel("Year", fontsize=14)
plt.title("International Income Inequality over Time", fontsize=18)
plt.show()
```

The graphs above show a positive correlation between unemployment and income inequality. As unemployment increases, income inequality also increases. In 2011, Hanan Morsy, an Egyptian economist who serves as the Director of Macroeconomic Policy, Forecasting and Research at the African Development Bank, actually researched this topic {cite}`10inequality-morsy`. Her group examined the member nations of the Organization  for  Economic  Cooperation  and  Development  (OECD) between 1980 and 2005. She found that specific groups that were vulnerable to the economic shocks that led to an increase in income inequality:
- young workers
- low-skilled workers
- workers who had been out of work for a long time

Her solution was to increase job creation opportunities for temporary, recently fired, and recently hired workers, provide job assistance and training to prevent long-term unemployment, and improve incentives for working by aligning incentives with productivity of labor. Morsy's research found that the most vulnerable groups to economic shocks were young, low-skilled and temporary workers. Creating opportunities for these different demographics would help them be more protected from potential shocks and thus decrease income inequality.

## Population growth

As the number of people in a country's population increase, it becomes more difficult for a country to distribute its public goods to everyone. This leads to many social consequences in which resources are not fairly distributed to all members of the population, cause inaccessibility for different parts of the population

The table below shows how the population growth has changed for the same countries we saw above. We are only looking at data for the past 10 years.

```python
pop_growth = Table.read_table("Population Growth.csv")
pop_growth
```

```python
pop_growth.plot("Year", width=11, height=7)
plt.title("Population Growth over Time", fontsize = 18)
plt.ylabel("Population Growth (%)", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("International Income Inequality over Time", fontsize=18)
plt.show()
```

The graphs above show that most countries high population growth between 1-2% during the 1980's. The effects of this can be seen in the rising income inequality during the 90's.  

Recent research by University of Toronto's Marijn Bolhuis and Univeristy of Oxford's Alexandra de Pleijt shows that there is a strong correlation between a country's population growth (measured by birth rates) and its income inequality {cite}`10inequality-popGrowth`. Their most recent study in 2016 analyzed income inequality and birth rates data between 1870 and 2000 across 67 countries. They concluded that if a country had 50% higher income inequality, then that country's birth rate would be about twice as high as another country with the same level of economic development. Bolhuis says that these higher birth rates mean that economic growth has to be equal to or greater than the birth rate to offset the implications of higher birth rates. 

This is part of a larger debate about the relationship between birth rates and income inequality. Economist Thomas Piketty finds that low birth rates, rather than high birth rates, are causing today's income inequality. With lower birth rates, fewer children per couple are being borne, so these children get more of their parents' inheritance.





--- END lec07/7.2-historical-inequality.md ---



--- START lec08/macro-fred-api.md ---

---
title: "macro-fred-api"
type: lecture-notebook
week: 8
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec08/macro-fred-api.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, EdX<br>
            Dr. Eric Van Dusen <br>
            Alan Liang <br> 
            Umar Maniku <br> 
            Matthew Yep <br> 
            Yiyang Chen <br> 
            Bennett Somerville <br>
        Akhil Venkatesh <br>
</table>

# Lecture Notebook 4.3: Macroeconomic Indicators

```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```

```python
# Load our API key from the .env file
def load_environment_variables(filename):
    with open(filename) as f:
        for line in f:
            (key, value) = line.replace('\n', '').split('=')
            os.environ[key] = value

load_environment_variables('.env')
```

**Learning Objectives:**  
In this notebook, we will download macroeconomic data and examine how the models and theories we discussed in lecture fit with these real life data. This notebook will introduce the basic concept of an API, the way to use it for economic research and study.

## Macroeconomic Data and Fred API

We will start by first download some macroeconomic data from FRED. In this notebook, we will introduce two ways of interacting with online sources of data with API (Application Programming Interface).

**What is an API?**  
In contrast to a user interface, which connects a computer to a person, an application programming interface (API) connects computers or pieces of software to each other. It is not intended to be used directly by a person (the end user) other than a computer programmer who is incorporating it into the software. An API is often made up of different parts which act as tools or services that are available to the programmer. A program or a programmer that uses one of these parts is said to call that portion of the API. The calls that make up the API are also known as subroutines, methods, requests, or endpoints. An API specification defines these calls, meaning that it explains how to use or implement them. 

In basic terms, APIs just allow applications to communicate with one another. For the APIs we are concerned right now--web based APIs that return data in response to a request made by us--**they allow us to get data from outside sources by sending an API a request detailing the information we want. Then the API will "respond" with the requested data to us.**

![What is an API](what_is_an_API.png)
Source: Perry Eising, "What exactly is an API?"

### General Way of Interacting with an API

First, we will use the most common way to access the data through an API without using any packages other may have already built for a particular site. Usually this is the way you would interact with an API.

#### Step 1: Get the API Key
**In most cases, you will need to get an API key in order to access an API.** For many resources, it involves some paperwork to apply and/or limited free usage, so it is good practice to keep your API keys private as long as it is possible. In this lecture notebook, we will use macroeconomic data from FRED, which is one of the most famous and convenient sources of economic data. For FRED, the process of getting an API key is simple. Request the API key [here](https://fred.stlouisfed.org/docs/api/api_key.html).

```python
# Because API keys are private, we store our API key in the .env file in this directory, which is not published on GitHub.
api_key = os.environ['API_KEY']
```

#### Step 2: Learn to use the API
Using an API is like ordering food at a restaurant with a menu. To have a delicious meal, we have to know what food the restaurant offers, and any other additional information (for example, how would you like your steak). Similarly, it is very important for us to know what requests an API take through the API documentation. **The API documentation will inform us about how we can use specific instructions to obtain the data that we want, and what the returned data would look like.** Look up the Fred API's documentation [here](https://fred.stlouisfed.org/docs/api/fred/series_observations.html).

<div class="alert alert-info">
<b> Example: "api.stlouisfed.org/fred/series/observations?series_id=GNPCA&api_key=abcdefghijklmnopqrstuvwxyz123456" <br>  
  
Endpoint: "api.stlouisfed.org/fred/series/observations"  <br>
Parameters: series_id=GNPCA, api_key=abcdefghijklmnopqrstuvwxyz123456
</div>

Try the following link, replacing REDACTED with your API key to see what the API will return!   

https://api.stlouisfed.org/fred/series/observations?series_id=GNPCA&api_key=REDACTED&file_type=json

#### Step 3: Make the fetch
Now we are ready to start writing code to fetch the data we want through the API.

```python
import requests
from urllib.parse import urlencode
endpoint = "https://api.stlouisfed.org/fred/series/observations"
```

Parameters that we would like to include in our query (these can be found in the FRED API documentations!):  
- `series_id`: The Economic data series we want (e.g. GDP)
- `observation_start`: The earliest date we want our data to include
- `observation_end`: The latest date we want our data to include
- `frequency`: The frequency of observations (e.g. Annually, Quarterly, Monthly)
- `units`: The units of observations (e.g. plain numbers, percentage change from a year ago, etc.)
- `api_key`: The API key
- `file_type`: The file data that will returned from FRED (e.g. json, xml)

```python
def fetch(series_id, unit="lin"):
    # specifies parameters
    params = {"series_id": series_id, 
              "observation_start": "1958-01-01", 
              "observation_end": "2023-10-01",
              "frequency": "q", # quarterly
              "units": unit,
              "api_key": api_key, 
              "file_type": "json"
             }
    
    # forms API request
    url_params = urlencode(params)
    url = f"{endpoint}?{url_params}"
    
    # fires off the request
    res = requests.get(url)
    
    # checks if the request encounters an error
    if res.status_code not in range(200, 299):
        raise Exception(f'Fetch request for "{series_id}" failed (Error: {res.status_code})')
    
    # return the content of the response
    return res.json()
```

Let's try downloading the real GDP data from Fred now.

```python
res_json_GDP = fetch("GDPC1")

# process the response
gdp = pd.DataFrame(res_json_GDP["observations"])[["date", "value"]]
gdp.rename(columns={"value": "GDP"}, inplace=True)
gdp['date'] = pd.to_datetime(gdp['date'], format='%Y-%m-%d')
gdp['GDP'] = pd.to_numeric(gdp['GDP'])
gdp
```

```python
plt.plot(gdp['date'], gdp['GDP'])
```

**Congratulations! We have just made our first request to Fred API and obtain some useful data.**

Now let's try download more macroeconomic indicators.

```python
# this function allows us to fetch and process the data together
def fetch_and_process(series_id, unit="lin"):
    res_json = fetch(series_id, unit)
    return pd.DataFrame(res_json["observations"])[["date", "value"]].rename(columns={"date":"DATE", "value": f"{series_id}_{unit}"})
```

```python
gdp = fetch_and_process("GDPC1")
gdp_growth = fetch_and_process("GDPC1", "pc1")
inflation = fetch_and_process("CPIAUCSL", "pc1")
unemployment = fetch_and_process("UNRATE")
ffr = fetch_and_process("DFF")
nrou = fetch_and_process("NROU")
core_inflation = fetch_and_process("CPILFESL", "pc1")
potential_gdp = fetch_and_process("GDPPOT")

gdp_growth
```

### Site-Specific Prebuilt Packages

Now we will switch gears to use some prebuilt packages to access FRED API. Notice that the prebuilt packages are not necessarily available for every API. But here we will use the "FredAPI" package developed by Mortada Mehyar. Documentation [here](https://github.com/mortada/fredapi).

```python
# may need to install the package and its dependencies
!pip install fredapi
from fredapi import Fred
fred = Fred(api_key=api_key)
GDP_fredapi = pd.DataFrame(fred.get_series('GDP'))
GDP_fredapi
```

You will get more practice with FREDAPI package in the lab!

## Macroeconomic Indicators and Data

**Now we're off to use the data we have just got!**

```python
# merge all macroeconomic data we have got!
macroeconomics = gdp.merge(gdp_growth)\
                    .merge(inflation)\
                    .merge(unemployment)\
                    .merge(ffr)\
                    .merge(core_inflation)\
                    .merge(nrou)\
                    .merge(potential_gdp)\


macroeconomics = macroeconomics.set_index("DATE").apply(pd.to_numeric).reset_index("DATE")

potential_gdp = potential_gdp[potential_gdp["DATE"] <= max(macroeconomics["DATE"])]
```

```python
macroeconomics.head()
```

---

### Macroeconomics Indicators and their Time Series
In this section, we will see how the value of each macroeconomic indicator varied from 1958 to present.

```python
series_names = {"GDPC1_lin": "GDP", 
               "GDPC1_pc1": "GDP Growth", 
               "CPIAUCSL_pc1": "Inflation", 
               "UNRATE_lin": "Unemployment Rate", 
               "DFF_lin": "Fed Funds Rate", 
               "CPILFESL_pc1": "Core Inflation Rate", 
               "NROU_lin": "Noncyclical Rate of Unemployment" # aka NAIRU, Natural Rate of Unemployment (Long-term)
              }

# generate a time series plot for "series"
def plot_time_series(data, series, title="1958 to present", figsize=(14, 8)):
    
    plt.figure(figsize=figsize)
    
    plt.plot(data["DATE"], data[series])
    
    # plot a horizontal line at zero if the variable is in percent
    if ("pc1" in series):
        plt.axhline(y=0, color='black', linestyle='-.')
    
    plt.xticks(data.index[np.arange(0, data.shape[0], 16)], rotation=45)
    plt.grid(visible=True)
    
    plt.xlabel("DATE")
    plt.ylabel(series)
    plt.title(f"{series_names[series]}: {title}", fontsize=16)
```

#### GDP

```python
plot_time_series(macroeconomics, "GDPC1_lin")
```

#### GDP Growth

```python
plot_time_series(macroeconomics, "GDPC1_pc1")
```

#### Inflation Rate

```python
plot_time_series(macroeconomics, "CPIAUCSL_pc1")
```

#### Core Inflation Rate

```python
plot_time_series(macroeconomics, "CPILFESL_pc1")
```

#### Unemployment Rate

```python
plot_time_series(macroeconomics, "UNRATE_lin")
```

#### Fed Funds Rate

```python
plot_time_series(macroeconomics, "DFF_lin")
```

#### Noncyclical Rate of Unemployment

```python
plot_time_series(macroeconomics, "NROU_lin")
```

What interesting pattern do you notice?

## Making an empirical Phillips curve

The above plots are interesting, but they can be found on the FRED website. Let's take the FRED data and do some data science with it.

```python
plt.scatter(macroeconomics['UNRATE_lin'], macroeconomics['CPILFESL_pc1'])
plt.title('Empirical Phillips Curve')
plt.xlabel('Unemployment Rate')
plt.ylabel('Inflation');
```

We can see a somewhat negative relationship, but it's not as strong as the relationship we saw during lecture, which used pre-WWI data. In the lab for this week, you'll use the FRED API to dive deeper into the relationship between the unemployment rate and inflation and see how it has changed over time.

```python
plt.scatter(macroeconomics['UNRATE_lin'], macroeconomics['GDPC1_lin'])
plt.title('Okun\'s Law With Emprirical Data')
plt.xlabel('Unemployment Rate')
plt.ylabel('Output (GDP)');
```

```python
plt.scatter(macroeconomics['DFF_lin'], macroeconomics['GDPC1_lin'])
plt.title('Empirical IS Curve')
plt.xlabel('Real Interest Rate')
plt.ylabel('Output (GDP)');
```



--- END lec08/macro-fred-api.md ---



--- START lec09/lecNB-prisoners-dilemma.md ---

---
title: "lecNB-prisoners-dilemma"
type: lecture-notebook
week: 9
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec09/lecNB-prisoners-dilemma.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, EdX<br>
            Dr. Eric Van Dusen <br>
            Chris Pyles <br>
        Akhil Venkatesh <br>
</table>

```python
from players import *
```

# Lecture Notebook: Iterated Prisoner's Dilemma

## Iterated Prisoner's Dilemma

```python
def human_play(self, opponent):
    if len(opponent.history):
        print(f"Your opponents last move: {'D' if opponent.history.item(-1) else 'C'}")
    move = input("Your move: (d/c) ").strip().upper()
    assert move in ["D", "C"], f"invalid move: {move}"
    return move == "D"

Human = create_player_class("Human", human_play)
```

```python
n_rounds = input("How many rounds would you like to play? [10] ")
if n_rounds:
    n_rounds = int(n_rounds)
else:
    n_rounds = 10

show_opponent = input("Show opponent? (y/n) [n] ").strip().upper() == "Y"

human = Human()
opponent = np.random.choice(make_array(
    Alternator(), Backstabber(), Bully(), Desperate(), FoolMeOnce(), Forgiver(), 
    Grudger(), OnceBitten()
))

if show_opponent:
    print(f"Your opponent is: {opponent}")

winner = run_match(human, opponent, n_rounds)
if winner == Human():
    winner = "you!"
print(f"The winner is: {winner}")
```

### Using `create_player_class`

**From the project:**

In this project, we will be creating some custom Python classes to represent different playing strategies. We'll be using the `create_player_class` function (provided) to create these classes for us. Here are a few important things to know about the classes:

Consider a player class instance `player = Player()`. 
* `player.play()` is a method that represents a single move. It returns `True` if the player **defects** and `False` if they cooperate.
* `player.history` is an array of the previous moves of the player instance. `player.history.item(-1)`, for example, is the most recent move (the last element of the array).

Don't worry about the other methods that are defined for players; these are there for the code below to work but you don't need to concern yourself with them. Here's how the `create_player_class` function works:

1. Define a function, `f`, that takes two arguments: `self` and `opponent` (more about these later), and returns `True` if the player defects and `False` otherwise.
2. Call `create_player_class` with two arguments: a string for the class name, e.g. `"Player"`, and the play method `f`.

Let's illustrate this by creating `Defector`, a player that always defects. The `defector_play` function below always returns `True`, since the player always defects. We then create `Defector` using `create_player_class`.

**Example:** Defector

```python
def defector_play(self, opponent):
    return True

Defector = create_player_class("Defector", defector_play)
```





--- END lec09/lecNB-prisoners-dilemma.md ---



--- START lec10/Lec10.2-waterguard.md ---

---
title: "Lec10.2-waterguard"
type: lecture-notebook
week: 10
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec10/Lec10.2-waterguard.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen</p></td></tr>
</table>

# Lec 9 : Water Guard Randomized Controlled Trial

This Lecture Notebook is an adaptation from a set of notebooks developed for a full semester Data Science Connector Course taught in Fall 2017, entitled "Behind the Curtain in Economic Development".  This dataset come from a randomized controlled trial household survey carried out in Eastern Kenya in 2007-2008. 

The purpose of the study was to understand how to promote the use of WaterGuard, a dilute sodium hypochlorite solution that was promoted for Point-of-use household water disinfection.  There were seven arms in the study, which will be more fully described in the following chart:

<img src="Slide1.png"  />

Within this table you can see the seven treatments arms -  control plus three treatments -  in the bolded boxes in the middle with the number of springs and households. The study was carried out as a part of a study of households who gather drinking water from springs in a rural area.  The three boxes at the bottom describe the three rounds of data collection - a baseline before the treatment, and a short term and long term follow-up.

<!-- **Notebook Outline**

1. [Mapping](#Mapping)
2. [Balance Check](#Balance)
3. [Baseline and a Randomly Selected Compound](#Baseline)
4. [Chlorine Usage outcome variables](#Chlorine)
5. [Graph of outcomes by Treatment Arm](#Graph)  -->

```python
from datascience import *
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import pandas as pd
from pandas import read_stata
```

<!-- END QUESTION -->



<div id="Balance"></div>

## Balance Check and Variable Names

### Baseline Survey 
This is our first look at the survey dataset.  These are a limited set of questions/answers from a simple and short baseline survey. However it is a lot bigger and messier than the datasets we have seen so far and in Data 8. 

Data variable names follow along with the survey below, referred to by the section, a,b,c... number, 1,2,3... and a few words about the question. 

The purpose of this section will be: 
* to get a familiarity with the dataset, 
* to look at some background descriptor variables of the households, 
* to start to think about missing values and coding of subsets of the data.  
* to check the randomization of households by seeeing if the different arms of the study are balanced across some of the key baseline variables.  

**The surveys that illustrate the raw data names are in a file linked [here](https://drive.google.com/open?id=1UVoiVn7LJ4rn7WEb-9BJ96jmdJ2FBk60). You have to go and look through this survey to understand the variables.**

**The code sheet that has the codes for some of the possible answers are in a file linked [here](https://drive.google.com/file/d/1iinJXExeVKV4Dm7tRKOiotoYUDSXMyqc). You have to go and look through this code sheet in a later section.**

```python
WGP_baseline = Table.read_table("WGP_baseline_Data8.csv")
WGP_baseline
```

```python
baseline = pd.read_csv("WGP_baseline_Data8.csv")
baseline
```

```python
baseline.dropna(axis = 0)
```

### Misssing values 

If you look through the dataset above, and scroll to the right a ways to some of the last variables, you will notice that that there are a lot of cells with NaN, which means a missing value. For these cells no data was entered at the time of data entry. In some cases it may be appropriate to enter a zero and carry on with the analyis.

```python
WGP_base_dfna = WGP_baseline.to_df().fillna(0)
WGP_table = Table.from_df(WGP_base_dfna)
WGP_table
```

Look at the variable names, and then look at the survey form to find the concordance of codes

```python
# Here is a list of all of the possible categories / columns
list(WGP_table)
```

### What are some Variables that we want to specifically look at? ###

There are a lot of variables here and it can be kind of overwhelming, but it is good to see how many columns there can be in a comprehensive survey dataset.

#### Front Page information - A variables

- household id
- spring id
- interviewer id

#### Information about respondent - B variables 

- tribe
- education
- age
- gender 
- group membership

#### Water Guard Use - C variables

For Waterguard (WG) usage

- `c1a` - Whether the respondent has ever heard of WG
- `c2a` - Whether the respondent has ever used WG
- `c3a` - Whether the respondent's water is currently treated with WG
- `c4a` - Whether the respondent has used WG in the past month

#### Durable / Capital Goods - D variables

- Whether the respondent has electricity / latrine / iron roof
- Number of of bicycle / radio / hoe / beds owned
- Number of animals owned

#### Child Health - E variables

- `e1_num_kids_under_5`: Number of kids under 5
- `e2_`:  This table becomes tricky because it has a different format. Each kid in the table is numbered 01, 02 and so on, and then the subsequent questions are keyed to that child number. e.g. `e2e_01_d_diarrhea`, `e2e_02_d_diarrhea` represent whether child 1 and 2 respectively have diarrhea. In total, four diseases are recorded:
    - Cough
    - Diarrhea
    - Malaria
    - Vomiting

### The Treatment Arm 

In the study, arm 1 is control, while Arms 2-7 are different types of treatment interventions:
 
- Arm 1 - Control
- Arm 2 - Household Script
- Arm 3 - Community Script
- Arm 4 - HH + Community Script
- Arm 5 - Flat-Fee Promoter + Coupons
- Arm 6 - Incentivized Promoter + Coupons
- Arm 7 - Incentivized Promoter + Dispenser at Spring

Let's check how many households are in each treatment arm.

```python
WGP_table.group("treatment_arm")
```

### Baseline Check - Exposure to Water Guard Use 

Let's see how many households have ever used Water Guards.

The data is currently Coded as 1 = Yes and 2 = No, so we can't really make sense of the Mean of the variable in its current form. Instead, we will make a new column/variable with the 1 or 2 answers translated into Yes or No.
Notably, we must first filter out respondents that had missing values (with value 0) for this question.

```python
WGP_ever = WGP_table.where('c2a_wg_used_ever', are.above(0))
WGP_ever.group("c2a_wg_used_ever")
```

```python
#This helper function goes through a column of choice, and spits out yes or no based off each value in the column. It returns an array of these yes and no's
def translate_to_yesno(table, col):
    dummy=[]
    table=table.where(col, are.above(0))
    for i in np.arange(table.num_rows):
        if table.column(col).item(i) == 1:
            dummy.append('Yes')
        else: #if not 1 then its 2 and 2 means no
            dummy.append("No")
    return dummy
```

```python
new = translate_to_yesno(WGP_ever, 'c2a_wg_used_ever')
WGP_ever = WGP_ever.with_column('c2a_wg_used_ever',new)
WGP_ever.group('c2a_wg_used_ever')
```

### Pivoting and Balance Checks

Now we will use a command called **Pivot** to create a new table that has the percent of households who have ever used Water Guard within each Treatment Arm. 

We can first use it to do a  **balance check** for Water Guard use across Arms.

```python
ever_yesno = WGP_ever.pivot('c2a_wg_used_ever','treatment_arm')
ever_yesno
```

Converting to percentages...

```python
total = ever_yesno.column(1) + ever_yesno.column(2)
ever_yesno = ever_yesno.with_columns('Percent No',ever_yesno.column(1) / total * 100, 
                                     'Percent Yes', ever_yesno.column(2) / total * 100)
ever_yesno
```

Let's also repeat the process for the variable of whether the households are currently using Water Guard, `c3a_wg_water_currently_treat`.

```python
WGP_current = WGP_table.where('c3a_wg_water_currently_treat',are.not_equal_to(0))
new2 = translate_to_yesno(WGP_current,'c3a_wg_water_currently_treat')
WGP_current = WGP_current.with_column('c3a_wg_water_currently_treat',new2)
WGP_current.group("c3a_wg_water_currently_treat")
```

Do you notice a problem here? Look at the total numbers reported in the output above.

We can do the same percentage tables for the balance check but maybe there's a problem. 
Look at the total number of households answering the question and compare that to the total number from the previous section.

```python
current_yesno = WGP_current.pivot('c3a_wg_water_currently_treat','treatment_arm')
total = current_yesno.column(1) + current_yesno.column(2)
current_yesno = current_yesno.with_columns('Percent No',current_yesno.column(1)/total * 100, 
                                           'Percent Yes', current_yesno.column(2)/total * 100)
current_yesno
```

This seems like a really high usage, but **maybe this is due to missing values**. 

Let's now also include the 0 (missing) values in our analysis.

```python
current_yesnomissing = WGP_table.pivot('c3a_wg_water_currently_treat','treatment_arm')
total = current_yesnomissing.column(1) + current_yesnomissing.column(2) + current_yesnomissing.column(3)
current_yesnomissing = current_yesnomissing.with_columns(
                                     'Percent Missing',current_yesnomissing.column("0.0") / total * 100, 
                                     'Percent No',current_yesnomissing.column("2.0") / total * 100, 
                                     'Percent Yes', current_yesnomissing.column("1.0") / total * 100)
current_yesnomissing
```

<!-- END QUESTION -->



<div id="Baseline"></div>

## Baseline and a Randomly Selected Compound

Let's describe a household selected at random.

First, we will extract the household/compound id into an array.

```python
hhld_array = WGP_table.column('a1_cmpd_id')
hhld_array
```

Next, we will draw randomly from this array.

```python
randomhh = np.random.choice(hhld_array)
print("My randomly selected household is household number", randomhh)
```

Then, let's look at the data for our randomly selected household:

```python
myfamily = WGP_table.where("a1_cmpd_id",randomhh)
myfamily
```

Some of the variables may need some manipulation. 
Let's start with the age of the respondent:

```python
birthyear = myfamily.column("b3_birth_year").item(0)
surveyyear = myfamily.column("a5_date_interview_year").item(0)
agecalc = surveyyear-birthyear  # 
agecalc
```

And their tribe:

```python
print("Survey respondent Tribe", myfamily.column("b5_tribe").item(0))
print("Respondent Spouse Tribe", myfamily.column("b7_tribe_spouse").item(0))
```

Lastly, whether they have a latrine:

```python
print("Does the household have a latrine?", myfamily.column("d3_latrine").item(0))
```

Remember in the answer above it is coded so that 1=Yes and 2=No.

<!-- BEGIN QUESTION -->

**Question 3:** Describe your randomly selected household and the respondent who is answering the survey. Please remember you can find the code sheet under the section of Baseline Survey.

1. Age
2. Tribe
3. Education 
4. Member of any groups b11-b15?
5. Occupation
6. Religion 
7. A summary of D variables, iron roof, floor materials, latrine, cattle, and others
8. Have they ever used WG?
9. Their treatment arm assignment
10. How many children do they have  
11. Gender and Age of children
12. Have any of the children been sick?

<!--
BEGIN QUESTION
name: q3
manual: true
-->

_Type your answer here, replacing this text._

<!-- END QUESTION -->



<div id="Chlorine"></div>

## Water Guard Usage outcome variables

### WGP Followup - Variability
The purpose of this section will be to continue on with the follow-up rounds of the Water Guard Promotion study.   In this section we have both the household reported use, and the use validated by checking the chlorine content of the water using a test kit.

```python
WGP3rds_table = Table.read_table('WGP_3waves_Data8.csv')
WGP3rds_table
```

This is a large dataset, basically three datasets merged together, one for baseline, one for short term follow up and one for long term followup. The column `round` describes these 3 time steps:

- Round = 1 : baseline
- Round = 2 : 3 week followup
- Round = 3 : 3 month followup

Notably, many of the variables are only asked in one of the three rounds. For example, the chlorine use variables are:

- The variable for self reported chlorine use was `c6n` in Round 2, and `c5n` in Round 3.
- The variable for chlorine use is `c12n21pnk` in Round 2 and `c15npt2or1pnk` in Round 3.

Instead, the following variables have been combined across rounds for the ease of programming:

- `Selfrptpct` is self reported chlorine use in both round 2 and round 3
- `Vldclpct`  is validated chlorine use in both rounds

```python
WGP3rds_table.group("treatment_arm")
```

```python
WGP3rds_table.group('round')
```

### Grouping by round + treatment arm

We want to create a multi-level group: each group should be a unique combination of the survey round and the treatment arm.

```python
WGP_3rds_outcomesonly= WGP3rds_table.select("round", "treatment_arm", "Selfrptpct", "Vldclpct")
WGP_3rds_outcomesonly.group(["round","treatment_arm"], np.mean).show(30)
```

### Making a smaller dataset

Lets break out a smaller dataset of the variables we want to focus on; just for Round 2 and the outcome variables.

```python
WGPRd2 = WGP3rds_table.where("round", 2).select("a1_cmpd_id","treatment_arm",
                                           "c6_current_water_treated_wg", 
                                           'c6_curr_water_treat_other_c',
                                           'c12_chlorine_meter_reading',
                                           'c11_chlorine_color','c12n21pnk', 'c6n'
                                          )
WGPRd2
```

A quick examination of the estimated Water Guard usage in Round 2 across all treatment arms:

```python
np.mean(WGPRd2.column('c12n21pnk'))
```

```python

```

```python

```



--- END lec10/Lec10.2-waterguard.md ---



--- START lec10/lec10.1-mapping.md ---

---
title: "lec10.1-mapping"
type: lecture-notebook
week: 10
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec10/lec10.1-mapping.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024
        <br>
            Dr. Eric Van Dusen</p></td></tr>
</table>

# Lec9: Water Guard Randomized Controlled Trial

This notebook is an adaptation from a set of notebooks developed for a full semester Data Science Connector Course taught in Fall 2017, entitled "Behind the Curtain in Economic Development".  This dataset come from a randomized controlled trial household survey carried out in Eastern Kenya in 2007-2008. 

The purpose of the study was to understand how to promote the use of WaterGuard, a dilute sodium hypochlorite solution that was promoted for Point-of-use household water disinfection.  There were seven arms in the study, which will be more fully described in the following chart:

<img src="Slide1.png"  />

Within this table you can see the seven treatments arms -  control plus three treatments -  in the bolded boxes in the middle with the number of springs and households. The study was carried out as a part of a study of households who gather drinking water from springs in a rural area.  The three boxes at the bottom describe the three rounds of data collection - a baseline before the treatment, and a short term and long term follow-up.

<!-- **Notebook Outline**

1. [Mapping](#Mapping)
2. [Balance Check](#Balance)
3. [Baseline and a Randomly Selected Compound](#Baseline)
4. [Chlorine Usage outcome variables](#Chlorine)
5. [Graph of outcomes by Treatment Arm](#Graph)  -->

```python
from datascience import *
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import pandas as pd
from pandas import read_stata
from ipyleaflet import Map, basemaps, Marker, AwesomeIcon
```

## Mapping

<div id="Mapping"></div>

This first section works with a package in Jupyter called ipyleaflet.

`ipyleaflet`;
the documentation is [here](https://ipyleaflet.readthedocs.io/en/latest/)
and it is worth a short read through if you are interested.


We want to use two different base maps - one is a satellite layer and oen is the Open Street Map layer.

We will start by reading in a dataset of the coordinates of the springs that are used in the WaterGuard Promotion (WGP) study.  These springs were randomized into seven different treatment arms.  The springs are identified by a unique numerical id tag, and the common name in the local language.

```python
springsGPS = Table.read_table('WGPgps_forData8.csv')
springsGPS
```

```python
# make a table wth just the North and East Gps columns 
locations = springsGPS.select("gpsn1", "gpse1")
locations
```

Where in the world are we?

First of all lets look at the mean for the Lat and Long and we can center our map there

```python

mean_longitude = springsGPS.column('gpse1').mean()
mean_latitude = springsGPS.column('gpsn1').mean()

print("Mean of 'gpse1':", mean_longitude)
print("Mean of 'gpsn1':", mean_latitude)
```

The code cell below should display a map. However, it may not run the first time you click it - if this happens, try running all the cells above this one and then refreshing your browser. After a few refreshes, the maps should load.

```python

center = [0.4, 34.4]
zoom = 12
basemap=basemaps.Esri.WorldImagery
layout={'width': '800px', 'height': '600px'}

Map(basemap=basemap, center=center, zoom=zoom, layout=layout)
```

Lets make a map of our sample sites ( springs)

```python
m = Map(basemap=basemap, center=center, zoom=zoom, layout=layout)

# Iterate through the rows in the dataset
for row in springsGPS.rows:
    latitude = row.item('gpsn1')
    longitude = row.item('gpse1')
    marker = Marker(location=(latitude, longitude))
    m.add_layer(marker)

m
```

Now the most interesting bit of data is still not being used, the Treatment Arm. Let's assign different colors to the different treatment arms so that when we map it we can see if the arms appear to be randomly distributed.

The following is function assigns the 7 different treatment arms to a set of colors. [Here](https://www.w3.org/TR/css3-color/#html4) is the colors reference if you are interested!

```python
def color(arm):
    if arm == 1:
        return 'black'
    elif arm == 2:
        return 'red'
    elif arm == 3:
        return 'purple'
    elif arm == 4:
        return 'green'
    elif arm == 5:
        return 'blue'
    elif arm == 6:
        return 'pink'
    elif arm == 7:
        return 'orange'
```

```python
# Using the .apply method, you can apply any function to a data frame
colors = springsGPS.apply(color, "treatment_arm")
springsGPS = springsGPS.with_column("color", colors)
springsGPS
```

```python

m = Map( center=center, zoom=zoom, layout=layout)

for row in springsGPS.rows:
    latitude = row.item('gpsn1')
    longitude = row.item('gpse1')
    color = row.item('color')
    
    marker = Marker(
        location=(latitude, longitude),
        draggable=False,  # Set to True if you want to make the markers draggable
        title=color,      # Set the marker title to the color for tooltip
        alt=color         # Set the alt text to the color
    )
    
    # Apply the specified color to the marker
    marker.icon = AwesomeIcon(name='circle', marker_color=color)
    
    m.add_layer(marker)

m
```

```python

m=Map(basemap=basemap, center=center, zoom=zoom, layout=layout)

for row in springsGPS.rows:
    latitude = row.item('gpsn1')
    longitude = row.item('gpse1')
    color = row.item('color')
    
    marker = Marker(
        location=(latitude, longitude),
        draggable=False,  # Set to True if you want to make the markers draggable
        title=color,      # Set the marker title to the color for tooltip
        alt=color         # Set the alt text to the color
    )
    
    marker.icon = AwesomeIcon(name='circle', marker_color=color)
    
    m.add_layer(marker)

m
```

Do the colors seem randomly distributed?

In fact, the randomization was performed on just a list of the springs using a random number generator. 
It did not take spatial distribution into effect.

```python

```

```python

```



--- END lec10/lec10.1-mapping.md ---



--- START lec11/11.1-slr.md ---

---
title: "11.1-slr"
type: lecture-notebook
week: 11
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec11/11.1-slr.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24<br>
            Dr. Eric Van Dusen <br>
            Vaidehi Bulusu <br>
        <br>
</table>

# Lecture Notebook 11.1: Simple Linear Regression

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import patches
import statsmodels.api as sm
import warnings
warnings.simplefilter("ignore")
from datascience import *
%matplotlib inline
from sklearn.metrics import mean_squared_error
import ipywidgets as widgets
from ipywidgets import interact
```

# Simple Linear Regression

Let's learn about simple linear regression using a dataset you're already familiar with from week 2: the dataset on the price and quantity demanded of avocados. Recall that in week 2, we used `np.polyfit()` to find the slope and intercept. Here, we'll use regression to calculate the slope and intercept from scratch and get the same results!

Go over this lecture notebook along with the slides!

```python
av_df = Table.read_table("avocados.csv")
av_df.show(5)
```

## Scatter Plot

To investigate the relationship between price and quantity demanded, we can start by creating a scatter plot:

```python
av_df.scatter("Average Price", "Total Volume")
plt.ylabel("Quantity Demanded")
plt.xlabel("Price")
plt.title("Demand Curve")
plt.show()
```

By looking at the scatter plot, we get a sense that there's a somewhat strong negative relationship between price and quantity demanded. Notice that the points don't all lie on a straight line as it's not a perfect association. This is because there is some random variation in the data plus there are factors other than price that affect the quantity demanded (we'll talk more about this later).

## Correlation Coefficient

To quantify the association and give it a numerical value, we can calculate the correlation coefficient between price and quantity demanded. Recall from DATA 8 that the correlation coefficient only measures **linear** association so for the purpose of this class, we are assuming that the relationship between the price and quantity demanded of avocados is linear (in reality, it may be nonlinear).

Let's start by defining a function to calculate the correlation coefficient.

```python
# Assumes that array_x and array_y are in standard units
def correlation(array_x, array_y):
    return np.mean(array_x * array_y)
```

The first step in calculating the correlation coefficient is converting the data to standard units. When you express a value in standard units, you are basically standardizing the variable by expressing it in terms of the number of standard deviations it is above or below the mean (you can think of this as the "distance" from the mean). For example, the first value of the price shown below is around 1.2 standard deviations below the mean price.

Expressing the data in standard units removes the measured units (e.g. dollars) from the data. So, the correlation coefficient is not affected by a change in the units/scale (e.g. if you convert the prices from USD to CAD).

```python
# Expressing price in standard units
price_su = (av_df["Average Price"] - np.mean(av_df["Average Price"]))/np.std(av_df["Average Price"])
price_su[:5]
```

```python
# Expressing quantity in standard units
quantity_su = (av_df["Total Volume"] - np.mean(av_df["Total Volume"]))/np.std(av_df["Total Volume"])
quantity_su[:5]
```

Now we can use the function we defined to calculate the correlation coefficient:

```python
r = correlation(price_su, quantity_su)
r
```

As we expected, there is a pretty strong negative correlation between price and demand! It aligns with what we saw in the scatter plot.

## Regression Equation

Let's use the correlation coefficient to build the regression equation. The first step is to calculate the slope and intercept:

```python
slope = r * np.std(av_df["Total Volume"])/np.std(av_df["Average Price"])
slope
```

```python
intercept = np.mean(av_df["Total Volume"]) - (slope * np.mean(av_df["Average Price"]))
intercept
```

We can now write the regression equation for the relationship between price and quantity demanded.

$$\hat{quantity} = 1446951.641 -476412.719\hat{price}$$

This equation allows you to do 2 things:

- Quantify the association between price and quantity demanded: as price increases by $1, the quantity demanded decreases by around 476,000 units (avocados)


- Predict values of quantity demanded: this is especially useful for values of price that are not already in the dataset

Note that the regression line is the line of best fit for your data (more on this later). We can plot this line of best fit on the scatter plot from before:

```python
av_df.scatter("Average Price", "Total Volume")
plt.plot(av_df["Average Price"], slope * av_df["Average Price"] + intercept, color='tab:orange')
plt.show()
```

Does this graph look familiar? You've already done a similar analysis in week 2 when you used `np.polyfit` to find the slope and intercept of the line of best fit!

```python
params = np.polyfit(av_df["Average Price"], av_df["Total Volume"], 1)
params
```

The slope and intercept you get from `np.polyfit` should be very close to the slope and intercept from before (there might be some tiny variation). This is because under the hood, np.polyfit is using simple linear regression to calculate the parameters.

```python
slope_param = params[0]
np.round(slope_param, 5) == np.round(slope, 5)
```

```python
intercept_param = params[1]
np.round(intercept_param, 5) == np.round(intercept, 5)
```

## RMSE

Let's use the slope and intercept to generate predictions for the quantity demanded:

```python
predicted_quantity = intercept + (slope * av_df["Average Price"])
predicted_quantity[:5]
```

We want our predictions to be as accurate as possible. One metric we can use to measure the accuracy of our predictions is the root mean squared error (RMSE) which gives us a sense of how far our predictions are from the actual values, on average.

Let's define a function for calculating the RMSE of our data (note that this function is specific to the avocados dataset).

```python
def rmse_p_q(slope, intercept):
    predicted_y = intercept + (slope * av_df["Average Price"])
    return (np.mean((av_df["Total Volume"] - predicted_y)**2))**0.5
```

```python
rmse_1 = rmse_p_q(slope, intercept)
rmse_1
```

On average, our predictions are off by about $132,000. This is a pretty high RMSE! This shows that even though price and quantity demanded have a pretty high correlation (about 0.72), it doesn't necessarily mean that price is a good predictor for the quantity demanded.

We said earlier that the regression line is the line of best fit. The reason why is that the regression line minimizes the RMSE of your data. In other words, among all the lines you can use to model your data, the regression line is the one that is the most accurate as it minimizes RMSE – this is why this technique is called (ordinary) least squares regression.

We can test this using the minimize function. Recall that minimize takes a function as the input as gives you values of the arguments of that function that minimize it. Below, we would expect the output of the minimize function to be similar to the slope and intercept we calculated:

```python
minimum = minimize(rmse_p_q)
minimum
```

The slope and intercept from the minimize function is very similar to the slope and intercept we calculated, as expected!

When you perform regression in Python (e.g. when you use `np.polyfit`), it tries to find the values of the slope and intercept that minimize the RMSE. You can build your intuition for how it does this by playing around with this slider from [last semester's notebook](https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fdata-88e%2Fecon-sp21&urlpath=tree%2Fecon-sp21%2Flec%2Flec12%2Flec12.ipynb&branch=main) (you can also find the link in on last semester's [website](https://data-88e.github.io/sp21/), in econometrics week). See if you can find values for the slope and intercept that minimize the squared error area.

```python
d = Table().with_columns(
    'x', make_array(0,  1,  2,  3,  4),
    'y', make_array(1, .5, -1,  2, -3))

def rmse(target, pred):
    return np.sqrt(mean_squared_error(target, pred))

def plot_line_and_errors(slope, intercept):
    print("RMSE:", rmse(slope * d.column('x') + intercept, d.column('y')))
    plt.figure(figsize=(5,5))
    points = make_array(-2, 7)
    p = plt.plot(points, slope*points + intercept, color='orange', label='Proposed line')
    ax = p[0].axes
    
    predicted_ys = slope*d.column('x') + intercept
    diffs = predicted_ys - d.column('y')
    for i in np.arange(d.num_rows):
        x = d.column('x').item(i)
        y = d.column('y').item(i)
        diff = diffs.item(i)
        
        if diff > 0:
            bottom_left_x = x
            bottom_left_y = y
        else:
            bottom_left_x = x + diff
            bottom_left_y = y + diff
        
        ax.add_patch(patches.Rectangle(make_array(bottom_left_x, bottom_left_y), abs(diff), abs(diff), color='red', alpha=.3, label=('Squared error' if i == 0 else None)))
        plt.plot(make_array(x, x), make_array(y, y + diff), color='red', alpha=.6, label=('Error' if i == 0 else None))
    
    plt.scatter(d.column('x'), d.column('y'), color='blue', label='Points')
    
    plt.xlim(-4, 8)
    plt.ylim(-6, 6)
    plt.gca().set_aspect('equal', adjustable='box')
    
    plt.legend(bbox_to_anchor=(1.8, .8))
    plt.show()

interact(plot_line_and_errors, slope=widgets.FloatSlider(min=-4, max=4, step=.1), intercept=widgets.FloatSlider(min=-4, max=4, step=.1));
```

## Regression in Python

So far, we've looked at the DATA 8 version of regression in which we calculated the correlated coefficient and used it to find the slope and intercept of our regression line.

Now we can look at how regression is actually performed in Python using a library called `statsmodels`.

```python
# import statsmodels.api as sm – see first cell in the notebook
x_1 = av_df.column("Average Price") # Selecting the independent variable(s)
y_1 = av_df.column("Total Volume") # Selecting the dependent variable
model_1 = sm.OLS(y_1, sm.add_constant(x_1)) # Performing OLS regression – make sure to include intercept using sm.add_constant
result_1 = model_1.fit() # Fitting the model
result_1.summary() # Showing a summary of the results of the regression
```

Let's look at the parameters (slope and intercept) of the regression and see if they are close to the parameters we calculated before:

```python
type(result_1.params)
```

```python
np.round(result_1.params[0], 5) == np.round(intercept, 5)
```

```python
np.round(result_1.params[1], 5) == np.round(slope, 5)
```

They are similar, as we expected!

## Hypothesis Testing

Since we are interested in whether or not there is a substantial association between x and y (in this case, price and quantity demanded), we can frame this as a hypothesis testing question:

- The null hypothesis would be that there isn't a substantial association between x and y ($\beta = 0$)

- The alternative hypothesis would be that there is a substantial association between x and y ($\beta ≠ 0$)

Go back to the regression output from before: notice that it shows a 95% confidence interval for the slope. Let's try to visualize this confidence interval by creating a distribution of the regression slopes:

```python
slopes_array = make_array()
for i in np.arange(5000):
    bootstrapped_sample = av_df.select("Average Price", "Total Volume").sample()
    x = bootstrapped_sample.column("Average Price")
    y = bootstrapped_sample.column("Total Volume")
    results = sm.OLS(y, sm.add_constant(x)).fit()
    slopes_array = np.append(results.params[1], slopes_array)
Table().with_column("Slopes", slopes_array).hist()
print("Center of the distribution (mean):", np.mean(slopes_array))
print("Standard deviation:", np.std(slopes_array))
```

```python
ci_lower_bound = percentile(2.5, slopes_array)
ci_lower_bound
```

```python
ci_upper_bound = percentile(97.5, slopes_array)
ci_upper_bound
```

Notice that the center of the distribution (average of the slopes) is similar to the slope from the regression output. The lower and upper bounds of the confidence interval are also similar.

Recall that if the confidence interval contains 0, it means that there is evidence for the null hypothesis at the 5% significance level. If it doesn't contain 0, it means that there is evidence against the null hypothesis at the 5% level.

The confidence interval in the regression output above doesn't contain 0 so there's evidence that price and quantity demanded have a significant association. This makes sense as they have a relatively high correlation coefficient.

```python

```



--- END lec11/11.1-slr.md ---



--- START lec11/11.2-mlr.md ---

---
title: "11.2-mlr"
type: lecture-notebook
week: 11
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec11/11.2-mlr.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24 <br>
            Dr. Eric Van Dusen <br>
            Vaidehi Bulusu <br>
        <br>
</table>

# Lecture Notebook 11.2: Multiple Linear Regression

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import patches
import statsmodels.api as sm
import warnings
warnings.simplefilter("ignore")
from datascience import *
%matplotlib inline
from sklearn.metrics import mean_squared_error
import ipywidgets as widgets
from ipywidgets import interact
```

# Multiple Linear Regression

In simple linear regression, we're only considering one independent variable. We're essentially assuming that this is the only variable that affects our dependent variable. Going back to the price and quantity demanded example, while price does have a huge effect on demand, there are other factors that also affect demand including income taxes, sales taxes, advertising, prices of related goods, etc. In the context of regression, because we are not including these variables in our model, they are called **omitted variables**.

Omitted variables cause 2 main problems:

- The regression parameters end up being inaccurate (biased) because of something called omitted variable bias. Your regression estimates for the slope and intercept are higher or lower than the actual values because of omitted variables (we are generally only concerned about the slope).


- They prevent you from inferring a causal association between the independent and dependent variables. In other words, you can't say that it's *because* of an increase in price that your quantity demanded decreased as there are so many other factors – like the change in the price of a related good – that could be causing the decrease in demand.

To try to eliminate omitted variable bias from our model, we take simple linear regression a step further: to multiple linear regression. In multiple linear regression, we are including more indepedent variables – variables we think are confounding variable that we've omitted – to reduce omitted variable bias.

Let's look at multiple linear regression in Python using a new dataset on earnings and various other factors (check out the [data description](https://www.princeton.edu/~mwatson/Stock-Watson_3u/Students/EE_Datasets/CPS12_Description.pdf)) [Stock and Watson datasets](https://www.princeton.edu/~mwatson/Stock-Watson_3u/Students/Stock-Watson-EmpiricalExercises-DataSets.htm).

```python
cps = Table.read_table("CPS.csv")
cps.show(5)
```

Say we want to look at the relationship between age and earnings (the `ahe` column which is the average hourly earnings). We would expect whether or not a person has a bachelor's degree to be a confounding variable as those with a bachelor's degree typically earn more than those with only a high school degree. This is how we would do multiple linear regression:

```python
x_2 = cps.select("bachelor", "age").values # This is how we include multiple independent variables in our model
y_2 = cps.column("ahe")
model_2 = sm.OLS(y_2, sm.add_constant(x_2))
result_2 = model_2.fit()
result_2.summary()
```

## Dummy Variables

One type of variable commonly used in econometrics are dummy variables. These are variables that take a value of either 0 or 1 to indicate the presence of absence of a category. For example, take col: it takes the value of 1 to indicate that a person went to college and 0 to indicate that a person didn't go to college.

Let's do a regression of `ahe` on only `bachelor` (the dummy variable).

```python
x_3 = cps.select("bachelor").values
y_3 = cps.column("ahe")
model_3 = sm.OLS(y_3, sm.add_constant(x_3))
result_3 = model_3.fit()
result_3.summary()
```

The coefficient (or slope) on `bachelor` is this:

```python
result_3.params[1]
```

This means that the people in our sample with a bachelor's degree earn around $6.583 more per hour than those with only a high school degree.

An interesting fact about dummy variables is that we can calculate this coefficient another way:

```python
# Filter for bachelor = 1 and find mean earnings
b_1_mean = np.mean(cps.where("bachelor", 1).column("ahe"))

# Filter for bachelor = 0 and find mean earnings
b_0_mean = np.mean(cps.where("bachelor", 0).column("ahe"))

# Take the difference in the mean earnings
diff = b_1_mean - b_0_mean
diff
```

```python
np.round(result_3.params[1], 5) == np.round(diff, 5)
```

These two values are pretty much the same! This is because the coefficient on a dummy x-variable is just equal to the difference of the mean of the y-variable when x = 1 and x = 0.

### `pd.get_dummies()`

We can convert categorical variables in our dataset to dummy variables using the pd.get_dummies() function. This function gives you a table showing the presence or absence of dummy variables for that category for each observation in the dataset. Here's an example of converting the year values to dummies. 1 indicates that the person was surveyed in that year (e.g. the first 4 people in the dataset were surveyed in 1992 according to the table below).

```python
year = cps.column("year")
np.unique(year)
```

```python
year_dummies = pd.get_dummies(year)
year_dummies.head()
```

Let's do the same for the age variable. Take the first row as an example: since 1 is under 29, it means that that person is 29 years old.

```python
age = cps.column("age")
np.unique(age)
```

```python
age_dummies = pd.get_dummies(age)
age_dummies.head()
```

### Dummy Variable Trap

One problem you may run into when using dummy variables is called the dummy variable trap. This happens when you include variables for all the values of the dummy variable: for example, you include col, which takes on a value of 1 if the person went to college, as well as notcol, which takes on a value of 1 if the person didn't go to college.

This causes redundancy as you can express one independent variable as a linear combination of another independent variable. In this case:

$$ notcol = 1 - col$$

This means that there is a perfect correlation between these two independent variables which reuslts in perfect multicollinearity. In general, multicollinearity occurs any time there is a high correlation between the independent variables in your model. It causes your regression estimates to be highly inaccurate.

Let's see what happens in Python when multicollinearity happens:

```python
year_dummies["ahe"] = np.array(cps.column("ahe"))
year_dummies.head()
```

```python
x_3 = year_dummies[[1992, 2008]] # We are including dummy variables for each value of year
y_3 = year_dummies["ahe"]
model_3 = sm.OLS(y_3, sm.add_constant(x_3))
result_3 = model_3.fit()
result_3.summary()
```

Python detected the multicollinearity and gave you a warning. A solution is to just drop one of the dummy variables.

```python
x_3 = year_dummies[[1992, 2008]] # We are including dummy variables for each value of year
y_3 = year_dummies["ahe"]
model_3 = sm.OLS(y_3, x_3)
result_3 = model_3.fit()
result_3.summary()
```

```python

```



--- END lec11/11.2-mlr.md ---



--- START lec12/Lec12-4-PersonalFinance.md ---

---
title: "Lec12-4-PersonalFinance"
type: lecture-notebook
week: 12
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec12/Lec12-4-PersonalFinance.ipynb"
---

<table style="width: 100%;" id="nb-header">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 24 <br>
            Dr. Eric Van Dusen 
        <br>
</table>

```python
from datascience import Table
import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display
%matplotlib inline
```

```python
try: import yfinance as yf
except: 
    !pip install yfinance
    import yfinance as yf
```

## Personal Finance Demo 

### This notebook will demonstrate some basic concepts in personal finance, over time, using Python.
 - Rate of Savings
 - Impact of Fees on Returns over Time
 - Monte Carlo Simulation of Retirement Savings
 - Historical Returns of Stocks and Bonds


 The notebook will also motivate the use of widgets in Jupyter notebooks to interact with the data and see the impact of different parameters on the results.

 ### Part 1: Rate of Savings

```python
annual_income_widget = widgets.FloatText(
    value=50000,
    description='Annual Income:',
    style={'description_width': 'initial'}
)

savings_rate_widget = widgets.FloatSlider(
    value=0.2,
    min=0,
    max=1,
    step=0.01,
    description='Savings Rate (%):',
    style={'description_width': 'initial'}
)

years_widget = widgets.IntSlider(
    value=30,
    min=1,
    max=50,
    step=1,
    description='Years:',
    style={'description_width': 'initial'}
)

annual_return_widget = widgets.FloatSlider(
    value=0.05,
    min=0,
    max=0.2,
    step=0.01,
    description='Annual Return (%):',
    style={'description_width': 'initial'}
)
display(annual_income_widget, savings_rate_widget, years_widget, annual_return_widget)
```

Save these values as variables

```python
annual_income = annual_income_widget.value
savings_rate = savings_rate_widget.value
years = years_widget.value
annual_return = annual_return_widget.value
```

Set a formula for the rate of savings

```python
annual_savings = annual_income * savings_rate
```

Create an array of years

```python
years_array = np.arange(1, years + 1)
savings_growth = []
```

Calculate the savings for each year, create total savings appending each year's savings to the previous year's savings

```python
total_savings = 0
for year in years_array:
    # Add annual savings and apply growth
    total_savings += annual_savings
    total_savings *= (1 + annual_return)
    savings_growth.append(total_savings)
```

Create a Table to store the results

```python
savings_table = Table().with_columns(
    "Year", years_array,
    "Total Savings ($)", savings_growth
)

savings_table
```

```python

plt.figure(figsize=(10, 6))
plt.plot(years_array, savings_growth, marker='o')
plt.title("Savings Growth Over Time")
plt.xlabel("Years")
plt.ylabel("Total Savings ($)")
plt.grid(True)
plt.show()
```

## Part 2 - Retirement Planning & Fees on Investments

```python
initial_investment_widget = widgets.FloatText(
    value=10000,
    description='Initial Investment ($):',
    style={'description_width': 'initial'}
)

annual_return_widget_fee = widgets.FloatSlider(
    value=0.07,
    min=0,
    max=0.2,
    step=0.01,
    description='Annual Return (%):',
    style={'description_width': 'initial'}
)

years_widget_fee = widgets.IntSlider(
    value=30,
    min=1,
    max=50,
    step=1,
    description='Investment Duration (Years):',
    style={'description_width': 'initial'}
)

fee_percentage_widget = widgets.FloatSlider(
    value=0.01,
    min=0,
    max=0.05,
    step=0.005,
    description='Annual Fee (%):',
    style={'description_width': 'initial'}
)
second_fee_percentage_widget = widgets.FloatSlider(
    value=0.02,
    min=0,
    max=0.05,
    step=0.005,
    description='Annual Fee Comparison (%):',
    style={'description_width': 'initial'}
)

display(initial_investment_widget, annual_return_widget_fee, years_widget_fee, fee_percentage_widget, second_fee_percentage_widget)
```

Retrieve widget values for calculations

```python
initial_investment = initial_investment_widget.value
annual_return = annual_return_widget_fee.value
years = years_widget_fee.value
annual_fee = fee_percentage_widget.value
annual_fee_2 = second_fee_percentage_widget.value
```

Initialize arrays for growth with and without fees

Set starting values for investments

```python
years_array = np.arange(1, years + 1)
growth_without_fees = []
growth_with_fee_1 = []
growth_with_fee_2 = []

investment_without_fees = initial_investment
investment_with_fee_1 = initial_investment
investment_with_fee_2 = initial_investment
```

Build a growth path for the three options

```python
for year in years_array:
    # Growth without fees
    investment_without_fees *= (1 + annual_return)
    growth_without_fees.append(investment_without_fees)

    # Growth with fee 1
    net_return_1 = annual_return - annual_fee
    investment_with_fee_1 *= (1 + net_return_1)
    growth_with_fee_1.append(investment_with_fee_1)

    # Growth with fee 2
    net_return_2 = annual_return - annual_fee_2
    investment_with_fee_2 *= (1 + net_return_2)
    growth_with_fee_2.append(investment_with_fee_2)
```

Create a Table to store the results

```python
fees_table = Table().with_columns(
    "Year", years_array,
    "Growth Without Fees ($)", growth_without_fees,
    "Growth With Fees ($)", growth_with_fee_1,
    "Growth With Fees 2 ($)", growth_with_fee_2
)


fees_table.show()
```

```python
# Plot the growth over time
plt.figure(figsize=(10, 6))
plt.plot(years_array, growth_without_fees, label="Without Fees", marker='o')
plt.plot(years_array, growth_with_fee_1, label=f"With Fees ({annual_fee*100:.2f}%)", marker='o', linestyle='--')
plt.plot(years_array, growth_with_fee_2, label=f"With Fee 2 ({annual_fee_2*100:.2f}%)", marker='o', linestyle=':')
plt.title("Investment Growth Over Time with and without Fees")
plt.xlabel("Years")
plt.ylabel("Investment Value ($)")
plt.legend()
plt.grid(True)
plt.show()
```

```python

```

## Part 3: Allocation of Assets 

Let's simulate the impact of different asset allocations on the growth of investments over time.

In this example we will consider two asset classes: Stocks and Bonds.

We will use a simple model of the growth of investments in each asset class over time but with variable returns in each time period.

This model will be for a one time initial investment, but we will simulate the growth over multiple time periods.

Set up the widgets

```python
initial_investment_widget_alloc = widgets.FloatText(
    value=10000,
    description='Initial Investment ($):',
    style={'description_width': 'initial'}
)

years_widget_alloc = widgets.IntSlider(
    value=30,
    min=1,
    max=50,
    step=1,
    description='Investment Duration (Years):',
    style={'description_width': 'initial'}
)

stock_allocation_widget = widgets.FloatSlider(
    value=0.6,
    min=0,
    max=1,
    step=0.05,
    description='Stock Allocation (%):',
    style={'description_width': 'initial'}
)


display(initial_investment_widget_alloc, years_widget_alloc, stock_allocation_widget)
```

Set up average returns and standard deviations for stocks and bonds

( these are hard coded for now - but could be widgets as well)

```python
average_stock_return = 0.08 # 8% average annual return for stocks
average_bond_return = 0.03 # 3% average annual return for bonds
std_dev_stock = 0.15  # 15% annual standard deviation for stocks
std_dev_bond = 0.05   # 5% annual standard deviation for bonds
```

```python
initial_investment = initial_investment_widget_alloc.value
years = years_widget_alloc.value
stock_allocation = stock_allocation_widget.value
bond_allocation = 1 - stock_allocation
```

```python
years_array = np.arange(1, years + 1)
growth_allocation = []
```

Calculate growth based on random returns derived from asset allocation

This will use the command 'np.random.normal' to generate random returns based on the average and standard deviation of returns for stocks and bonds

The arguments to 'np.random.normal' are the average return, the standard deviation of returns, that we hard coded above.

Think about it as each year the return is a random number drawn from a normal distribution with the average return and standard deviation we set above.

This will create a random path of returns for each year, as we rerun the cell, we will get a different path of returns.  We could set a seed to get the same path of returns each time.

```python
#set_seed(42)
investment_value = initial_investment
for year in years_array:
    stock_return = np.random.normal(average_stock_return, std_dev_stock)
    bond_return = np.random.normal(average_bond_return, std_dev_bond)
    
    # Calculate the portfolio's annual return based on the allocation
    portfolio_return = (stock_allocation * stock_return) + (bond_allocation * bond_return)
    
    # Update the investment value based on the portfolio return
    investment_value *= (1 + portfolio_return)
    growth_allocation.append(investment_value)
```

Create a Table to store the results

```python
allocation_table = Table().with_columns(
    "Year", years_array,
    f"Growth with {stock_allocation*100:.0f}% Stocks / {bond_allocation*100:.0f}% Bonds ($)", growth_allocation
)

# Display the table
allocation_table.show()
```

```python

plt.figure(figsize=(10, 6))
plt.plot(years_array, growth_allocation, label=f"{stock_allocation*100:.0f}% Stocks / {bond_allocation*100:.0f}% Bonds", marker='o')
plt.title("Investment Growth Over Time with Asset Allocation (Including Volatility)")
plt.xlabel("Years")
plt.ylabel("Investment Value ($)")
plt.legend()
plt.grid(True)
plt.show()
```

## Part 4 Monte Carlo Simulation
What is the role of uncertainty in financial planning?

In the previous example, we have a single path for the growth of investments. However, in reality, the growth of investments is more complicated! 

We can use Monte Carlo simulation to simulate the growth of investments over time. This would allow us to see the range of possible outcomes for our investments.  Think of it as generating many paths of returns and seeing the range of outcomes.  This is a powerful tool for financial planning.

Step 1 - Set up the widgets

We can add a widget for the number of simulations we want to run.  This will allow us to see the range of outcomes for our investments.

```python
initial_investment_widget_alloc = widgets.FloatText(
    value=10000,
    description='Initial Investment ($):',
    style={'description_width': 'initial'}
)

years_widget_alloc = widgets.IntSlider(
    value=30,
    min=1,
    max=50,
    step=1,
    description='Investment Duration (Years):',
    style={'description_width': 'initial'}
)

stock_allocation_widget = widgets.FloatSlider(
    value=0.6,
    min=0,
    max=1,
    step=0.05,
    description='Stock Allocation (%):',
    style={'description_width': 'initial'}
)

simulations_widget = widgets.IntSlider(
    value=1000,
    min=100,
    max=5000,
    step=100,
    description='Number of Simulations:',
    style={'description_width': 'initial'}
)

# Display the widgets
display(initial_investment_widget_alloc, years_widget_alloc, stock_allocation_widget, simulations_widget)
```

Again we will hard code the average returns and standard deviations for stocks and bonds. ( for a future project we could make these widgets as well! )

```python
average_stock_return = 0.08
average_bond_return = 0.03
std_dev_stock = 0.15  # 15% annual standard deviation for stocks
std_dev_bond = 0.05   # 5% annual standard deviation for bonds
```

```python
# Retrieve widget values for calculations
initial_investment = initial_investment_widget_alloc.value
years = years_widget_alloc.value
stock_allocation = stock_allocation_widget.value
bond_allocation = 1 - stock_allocation
num_simulations = simulations_widget.value

# Initialize a list to store the final investment values of each simulation
final_values = []
```

Build a loop to simulate the growth of investments over time.  Running the simulaton multiple times is called a *Monte Carlo Simulation*.

This will be similar to the previous example, but we will run the simulation multiple times to see the range of outcomes.  The inner loop is the same as the previous example, but in the outer loop we will run the simulation multiple times.

Create a Table to store the results `final-values`

Calculate the average and standard deviation of the results `final-values`

This entire function is wrapped in a plot function that will plot the results of each run of the Monte Carlo simulation.  Think of it as 1000 different plots that map out the range of outcomes for the investments.  

In the plot we have set the alpha to 0.1 so that we can see the overlap of the different paths.  Alpha is a parameter that sets the transparency of the plot.  So, the lower the alpha, the more transparent the plot.  


We will also plot the average and standard deviation of the results.  This will give us a sense of the range of outcomes for our investments.

```python

plt.figure(figsize=(12, 8))
#  Outer loop for running multiple simulations
for _ in range(num_simulations):
    investment_value = initial_investment
    growth_path = []

    # Inner loop for each simulation
    for year in range(years):
        # Generate random returns for stocks and bonds based on mean and standard deviation
        stock_return = np.random.normal(average_stock_return, std_dev_stock)
        bond_return = np.random.normal(average_bond_return, std_dev_bond)
        
        portfolio_return = (stock_allocation * stock_return) + (bond_allocation * bond_return)
      
        investment_value *= (1 + portfolio_return)
        growth_path.append(investment_value)

    # Store the final investment value for this simulation
    final_values.append(investment_value)

    # Plot this simulation's growth path
    plt.plot(range(1, years + 1), growth_path, color='blue', alpha=0.05) # alpha is the transparency level

# Calculate summary statistics of the final values across all simulations
mean_final_value = np.mean(final_values)
median_final_value = np.median(final_values)
percentile_10 = np.percentile(final_values, 10)
percentile_90 = np.percentile(final_values, 90)

# Plot the summary statistics on the chart
plt.title("Monte Carlo Simulation of Investment Growth with Asset Allocation")
plt.xlabel("Years")
plt.ylabel("Investment Value ($)")
plt.grid(True)
plt.axhline(mean_final_value, color='green', linestyle='--', label=f"Mean Final Value: ${mean_final_value:,.2f}")
plt.axhline(median_final_value, color='orange', linestyle='--', label=f"Median Final Value: ${median_final_value:,.2f}")
plt.axhline(percentile_10, color='red', linestyle='--', label=f"10th Percentile: ${percentile_10:,.2f}")
plt.axhline(percentile_90, color='purple', linestyle='--', label=f"90th Percentile: ${percentile_90:,.2f}")
plt.legend()
plt.show()
```

## Appendix (Part 4)  - get the parameters for return and sd from the markets

We can get the average returns and standard deviations for stocks and bonds from the markets.  This will allow us to use real data in our simulations.  

We have hard coded the values for now, but we could use the market values to update our hard coded values above   This would allow us to use real data in our simulations.

We will use the `yfinance` package to get the data from the markets.  This package allows us to get data from Yahoo Finance in a simple way, no API key, and in the format we want.  

We will use the `yfinance` package to get the data for the **S&P 500** and the **10 year treasury** bond.  We will call the tickers for these assets `^GSPC` and `^TNX` respectively. 

We will use the data to calculate the average annual returns and standard deviations for stocks and bonds. Using annual returns and standard deviations is clearly one limitation of this model, but it is a simple way to get a sense of the range of outcomes for our investments.

There are two important choices in the following code:
- The time period for the data
- The index for the data

*Note for Data 88E: There is a little Pandas manipulaton in the code to calculate the average annual returns and standard deviations for stocks and bonds.  We will use the `ffill` and `resample` methods to fill in any missing values and to resample the data to get the annual returns and standard deviations.  We will use the `dropna` method to remove any missing values from the data.*

```python
start_date = "2000-01-01"
end_date = "2023-01-01"
```

```python
# Fetch historical data for S&P 500 (stocks) and TLT (bonds) using yfinance
stock_data = yf.download('^GSPC', start=start_date, end=end_date)
bond_data = yf.download('TLT', start=start_date, end=end_date)

# Convert to annual adjusted close prices by resampling to year-end and forward-filling
stock_data = stock_data['Adj Close'].resample('Y').ffill()
bond_data = bond_data['Adj Close'].resample('Y').ffill()

# Calculate annual returns as percentage change for stocks and bonds
stock_returns = stock_data.pct_change().dropna()
bond_returns = bond_data.pct_change().dropna()
```

```python
# Create datascience Tables for annual returns
stock_table = Table().with_columns("Year", stock_data.index.year[1:], "Stock Returns", stock_returns.values)
bond_table = Table().with_columns("Year", bond_data.index.year[1:], "Bond Returns", bond_returns.values)
```

```python
# Calculate mean and standard deviation using the Table methods
average_stock_return_data = stock_table.column("Stock Returns").mean()
std_dev_stock_data = stock_table.column("Stock Returns").std()
average_bond_return_data = bond_table.column("Bond Returns").mean()
std_dev_bond_data = bond_table.column("Bond Returns").std()
```

```python
# Display the results
print(f"Average Stock Return (Market Data): {average_stock_return_data * 100:.2f}%")
print(f"Stock Return Standard Deviation (Market Data): {std_dev_stock_data * 100:.2f}%")
print(f"Average Bond Return (Market Data): {average_bond_return_data * 100:.2f}%")
print(f"Bond Return Standard Deviation (Market Data): {std_dev_bond_data * 100:.2f}%")
```

## Updating the Monte Carlo Simulation with Market Data !?

- How do these results compare to the parameters that we hard coded above?
- What are the implications for financial planning?
- How do the results change if we use real data from the markets?

```python
print( " Actual - Data")
print(f"Difference in Average Stock Return: {(average_stock_return - average_stock_return_data) * 100:.2f}%")
print(f"Difference in Stock Return Standard Deviation: {(std_dev_stock - std_dev_stock_data) * 100:.2f}%")
print(f"Difference in Average Bond Return: {(average_bond_return - average_bond_return_data) * 100:.2f}%")
print(f"Difference in Bond Return Standard Deviation: {(std_dev_bond - std_dev_bond_data) * 100:.2f}%")
```

Looks like our hard-coded values were a bit optimistic?  
- We overestimanted returns
- We underestimated volatility

```python

```

```python

```

```python

```



--- END lec12/Lec12-4-PersonalFinance.md ---



--- START lec12/lec12-1_Interest_Payments.md ---

---
title: "lec12-1_Interest_Payments"
type: lecture-notebook
week: 12
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec12/lec12-1_Interest_Payments.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
      </p></td></tr>
</table>

```python
import numpy as np
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display
import plotly.graph_objects as go
```

## Mortgage Calculator

Lets build some widgets to allow you to select the 
- Principal - amount of loan
- Interestes Rate - annual interest rate
- Years:  - Years of Loan term ( usually 30 or 15) 

You can consult BankRate to look up resonable rates
https://www.bankrate.com/mortgages/mortgage-rates/

```python

principal_widget = widgets.FloatSlider( value=200000, min=0, max=1000000, step=100, description='Principal:', readout_format='.0f', )
annual_interest_rate_widget = widgets.FloatSlider( value=0.05, min=0, max=0.25, step=0.001, description='Interest Rate:', readout_format='.2f', )
years_widget = widgets.IntSlider( value=30, min=1, max=50, step=1, description='Years:', readout_format='.0f', )

display(principal_widget, annual_interest_rate_widget, years_widget)
```

```python
#principal = 200000  
#annual_interest_rate = 0.04  
#years = 30  

principal = principal_widget.value
annual_interest_rate = annual_interest_rate_widget.value
years = years_widget.value
```

# Formula for Mortgage Payment 

$$ \text{Monthly Payment} = \frac{{\text{Principal} \cdot \left(\text{Monthly Interest Rate} \cdot \left(1 + \text{Monthly Interest Rate}\right)^{\text{Number of Payments}}\right)}}{{\left(1 + \text{Monthly Interest Rate}\right)^{\text{Number of Payments}} - 1}}
$$

## This is what we need to program into Python

```python
# Calculate monthly interest rate and number of payments
monthly_interest_rate = annual_interest_rate / 12
num_payments = years * 12

#Calculate monthly mortgage payment
monthly_payment = principal * (monthly_interest_rate * (1 + monthly_interest_rate)**num_payments) / ((1 + monthly_interest_rate)**num_payments - 1)
print('Monthly payment: $%.2f' % monthly_payment)
print('Total payment: $%.2f' % (monthly_payment * num_payments))
print('Total interest: $%.2f' % (monthly_payment * num_payments - principal))
print('Total Principal: $%.2f' % principal)
print('Interest to principal ratio: %.2f' % ((monthly_payment * num_payments - principal) / principal))
```

## Lets Build a dataset of the payments over time
- Each Monthly Payment has a share 
- Principal paid each month
- Interest Payement eah month

### if we build this right we can graph the payments over the life of the loan

```python
payment_schedule = []
principal_schedule = []
interest_schedule = []


remaining_balance = principal

# This function will calculate the payments and share of interest and pricipal paid
for month in range(1, num_payments + 1):
    interest_payment = remaining_balance * monthly_interest_rate
    principal_payment = monthly_payment - interest_payment
    remaining_balance -= principal_payment
    payment_schedule.append(monthly_payment)
    principal_schedule.append(principal_payment)
    interest_schedule.append(interest_payment)
```

## Let's look at the composition of the first 5 and last 5 payments

```python
print("\nPayment Schedule for the first 5 months:")
for month in range(5):
    print(f"Month {month + 1}: Payment ${payment_schedule[month]:.2f} (Principal ${principal_schedule[month]:.2f}, Interest ${interest_schedule[month]:.2f})")
print("\nPayment Schedule for the last 5 months:")
for month in range(num_payments - 5, num_payments):
    print(f"Month {month + 1}: Payment ${payment_schedule[month]:.2f} (Principal ${principal_schedule[month]:.2f}, Interest ${interest_schedule[month]:.2f})")
```

## Now we will plot these payments over the 360 months of payback

```python
plot = go.Figure()
plot.add_trace(go.Scatter(y=principal_schedule, name="Principal"))
plot.add_trace(go.Scatter(y=interest_schedule, name="Interest"))
plot.add_trace(go.Scatter(y=payment_schedule, name="Payment"))
plot.update_layout(title="Mortgage Amortization Schedule", xaxis_title="Payment Number", yaxis_title="Payment")
plot.show()
```

## Let's do this for an investment instead of a mortgage
### We will calculate the future value of an investment
### Create widgets to specify the rates
### Slightly different than the Lab

```python

principal_widget = widgets.FloatSlider(value=1000, min=1, max=10000, step=1, description='Principal')
rate_widget = widgets.FloatSlider(value=0.05, min=0.01, max=0.2, step=0.01, description='Interest Rate')
time_widget = widgets.IntSlider(value=10, min=1, max=30, step=1, description='Time (Years)')
n_widget = widgets.IntSlider(value=12, min=1, max=365, step=1, description='Compounding Frequency')
display(principal_widget, rate_widget, time_widget, n_widget)
```

# Define a function to calculate future value

```python
def calculate_future_value(principal, rate, time, n):
    years = np.arange(0, time + 1)
    future_values = principal * (1 + rate / n) ** (n * years)
    return years, future_values

# Calculate and plot the future value of the investment
years, future_values = calculate_future_value(principal_widget.value, rate_widget.value, time_widget.value, n_widget.value)

print("At time = %d years, the future value of the investment is $%.2f" % (time_widget.value, future_values[-1]))
```

```python
# Plot the future value of the investment over time

fig = go.Figure()
fig.add_trace(go.Scatter(x=years, y=future_values, mode='lines'))
fig.update_layout(title="Future Value Over Time", xaxis_title="Years", yaxis_title="Future Value")
fig.show()
```

## Here is a couple more functions to compare mortgage terms

### First to compare two different interest rates

You can consult BankRate to look up resonable rates
https://www.bankrate.com/mortgages/mortgage-rates/

```python
principal_amount = 200000  # Principal amount of the loan
loan_term_years = 30  # Loan term in years
interest_rates_to_compare = [0.04, 0.08]  # Two different interest rates
```

```python
def compare_interest_rates(principal, years, interest_rates):
    results = []
    
    for rate in interest_rates:
        monthly_interest_rate = rate / 12
        num_payments = years * 12
        monthly_payment = principal * (monthly_interest_rate * (1 + monthly_interest_rate)**num_payments) / ((1 + monthly_interest_rate)**num_payments - 1)
        total_payment = monthly_payment * num_payments
        total_interest_paid = total_payment - principal
        interest_as_percent_of_payment = (total_interest_paid / total_payment) * 100

        results.append({
            "interest_rate": rate,
            "monthly_payment": monthly_payment,
            "total_payment": total_payment,
            "total_interest_paid": total_interest_paid,
            "interest_as_percent_of_payment": interest_as_percent_of_payment
        })
    
    return results

results = compare_interest_rates(principal_amount, loan_term_years, interest_rates_to_compare)

for result in results:
    print(f"Interest Rate: {result['interest_rate'] * 100}%")
    print(f"Monthly Payment: ${result['monthly_payment']:.2f}")
    print(f"Total Payment: ${result['total_payment']:.2f}")
    print(f"Total Interest Paid: ${result['total_interest_paid']:.2f}")
    print(f"Interest as % of Payment: {result['interest_as_percent_of_payment']:.2f}%")
    print()
```

## Redo to can compare two different loan terms

```python
principal_amount = 200000  # Principal amount of the loan
interest_rate = 0.05  # Annual interest rate (5%)
loan_terms_to_compare = [15, 30] # Two most typical loan terms ( lengths)
```

```python
def compare_loan_terms(principal, years, interest_rate):
    results = []
    
    for loan_term in years:
        # Calculate monthly interest rate and number of payments
        monthly_interest_rate = interest_rate / 12
        num_payments = loan_term * 12

        # Calculate monthly mortgage payment
        monthly_payment = principal * (monthly_interest_rate * (1 + monthly_interest_rate)**num_payments) / ((1 + monthly_interest_rate)**num_payments - 1)

        # Calculate total payment and total interest paid
        total_payment = monthly_payment * num_payments
        total_interest_paid = total_payment - principal

        # Calculate the total interest as a percentage of the total payment
        interest_as_percent_of_payment = (total_interest_paid / total_payment) * 100

        results.append({
            "loan_term": loan_term,
            "monthly_payment": monthly_payment,
            "total_payment": total_payment,
            "total_interest_paid": total_interest_paid,
            "interest_as_percent_of_payment": interest_as_percent_of_payment
        })
    
    return results

# Example usage


results = compare_loan_terms(principal_amount, loan_terms_to_compare, interest_rate)

for result in results:
    print(f"Loan Term: {result['loan_term']} years")
    print(f"Monthly Payment: ${result['monthly_payment']:.2f}")
    print(f"Total Payment: ${result['total_payment']:.2f}")
    print(f"Total Interest Paid: ${result['total_interest_paid']:.2f}")
    print(f"Interest as % of Payment: {result['interest_as_percent_of_payment']:.2f}%")
    print()
```

```python

```

```python

```

```python

```



--- END lec12/lec12-1_Interest_Payments.md ---



--- START lec12/lec12-2-stocks-options.md ---

---
title: "lec12-2-stocks-options"
type: lecture-notebook
week: 12
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec12/lec12-2-stocks-options.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Dr. Eric Van Dusen <br>
        Sreeja Apparaju <br>
        Kidong Kim</p></td></tr>
</table>

# Lecture 12: Finance

## This notebook takes a look at some simple tools for looking at the stock market
 - Previously Yahooo finance had a free API for reading in historical data on stocks
 - However the Yahoo API got discontiued
 - An awesome quant made a python package that recreated this functionality by scraping the information
 
Check out the documentation for [Yfinance package](https://pypi.org/project/yfinance/)

The package - called yfinance is not on the datahub so first we need to install it

```python
try:
    import yfinance as yf
except:
    !pip install yfinance
    import yfinance as yf
```

```python
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from datetime import timedelta, date, datetime
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
import warnings
from datascience import *
warnings.filterwarnings('ignore')
%matplotlib inline
```

## S&P 500 and the Nasdaq

The yfinance package allows us to download by stock ticker and make a Pandas Dataframe - here we will pull in by the market-wide tickers for the S&P 500 and the Nasdaq

```python
data_SPNQ = yf.download(("^GSPC", '^IXIC'), start="1993-01-29", end="2022-04-05")
```

The following section uses the dataframe to build out a new dataframe with returns - the amount earned each day on the previous days close

```python
data_SN = data_SPNQ.iloc[:, [2,3]]
data_SP =data_SPNQ.iloc[:, 0]
data_NQ = data_SPNQ.iloc[:, 1]
dSP = np.array(len(data_SP)-1)
for i in range(len(data_SP)-1):
    dat = ((data_SP[i] - data_SP[i+1])/data_SP[i])*100
    dSP = np.append(dSP,dat)
dNQ = np.array(len(data_NQ)-1)
for i in range(len(data_NQ)-1):
    dat = ((data_NQ[i] - data_NQ[i+1])/data_NQ[i])*100
    dNQ = np.append(dNQ,dat)
data_SN['SP Returns'] = dSP
data_SN['NQ Returns'] = dNQ
```

```python
data_SN.iloc[:,[0,1]].plot(color = ('blue', 'red'), figsize=(10,8), alpha =0.3);
```

```python
data_SN[['SP Returns', 'NQ Returns']].iloc[1:].plot(color = ('blue', 'red'), figsize=(10,8), alpha = 0.3);
```

```python
data_SN.iloc[:,[0,1]].plot(color = ('blue', 'red'), figsize=(10,8), alpha =0.3);
```

```python
data_SN[['SP Returns', 'NQ Returns']].iloc[1:].plot(color = ('blue', 'red'), figsize=(10,8), alpha = 0.3);
```

## Let's dive deeper into the Yfinance API and and work with the data

First we will define three stocks that we want to look at more closely, and examine what sort of information we can get for each stock.  

Lets look at 
 - Twitter
 - Tesla
 - USO - an ETF (exchange traded fund) that tracks the price of oil

```python
twitter_ticker = yf.Ticker("twtr")
tesla_ticker = yf.Ticker("tsla")
uso_ticker = yf.Ticker("uso")
```

There is actually a lot of information that yfinance API can provide for any equity.  In the example above we only downloaded the closing price for each of the indexes.

```python
#tesla_ticker.info
```

## Out of all this info - let's extract the stock prices

This will put the dates, prices, and volumes into a *Pandas* dataframe with the name of the stock

```python
#twitter = twitter_ticker.history(period="max")
tesla = tesla_ticker.history(period="max")
uso = uso_ticker.history(period="max")
```

```python
tesla
```

## Lets look at the market for Options for Twitter 
 - This will show us the possible strike dates for different options
 - From short term - this week - to long term - in two years

```python
tesla_ticker.options
```

## Downloading Calls and Puts 
Let's download all of the Calls and Puts for Tesla  into two tables

```python
tesla_options = tesla_ticker.option_chain(tesla_ticker.options[0])
```

```python
tesla_options.calls
```

```python
# Returns two table 0 : calls, 1 : puts
tesla_options = tesla_ticker.option_chain(tesla_ticker.options[0])
tesla_put , tesla_call = tesla_options.puts, tesla_options.calls
relevant_columns = ['lastPrice', 'change', 'percentChange', 'volume', 'strike']
tesla_put, tesla_call = tesla_put[relevant_columns], tesla_call[relevant_columns]
tesla_put.describe(), tesla_call.describe()
```

## Let's put these together into a joint table that are joined by the strike price

```python
tesla_option = pd.merge(tesla_put, tesla_call, how='inner', on = "strike")
#tesla_option = pd.merge(tesla_put, tesla_call, how='outer', on = "strike")
tesla_option[12:32]
```

## Now lets code up a graph of the puts and calls for Tesla

```python
current_tesla_p = tesla.iloc[-1]['Close']
current_tesla_p

plt.figure().set_size_inches(15, 5)

plt.title("Tesla calls vs puts",  fontsize=15)

plt.plot(tesla_option['strike'], tesla_option['lastPrice_x'], color='r', label='call', linewidth=3)
plt.plot(tesla_option['strike'], tesla_option['lastPrice_y'], color='b', label='put', linewidth=3)
plt.axvline(x = current_tesla_p, color = 'g', label = 'Current Price', linewidth=4) #Current Price

plt.axis([tesla_option['strike'].iloc[0], tesla_option['strike'].iloc[-1], 0, max(max(tesla_option['lastPrice_x']), max(tesla_option['lastPrice_y']))])
plt.legend()
```

```python
def option(ticker):
  options_obj = ticker.option_chain(ticker.options[0])
  put, call = options_obj.puts, options_obj.calls
  relevant_columns = ['lastPrice', 'change', 'percentChange', 'volume', 'strike']
  put, call = put[relevant_columns], call[relevant_columns]
  option_sheet = pd.merge(put, call, how='inner', on = "strike")
  return option_sheet

option(uso_ticker)
```

## QuantStats Package
The same developer made a more recent package that draws on Yfinance but makes a whole set of summary tables 

Check out the documentation for the [QuantStats Package](https://pypi.org/project/QuantStats/)

```python
try:
    import quantstats as qs
except:
    !pip install quantstats
    import quantstats as qs
```

```python
import quantstats as qs

# extend pandas functionality with metrics, etc.
qs.extend_pandas()

# fetch the daily returns for a stock
stock = qs.utils.download_returns('TSLA')

# show sharpe ratio
qs.stats.sharpe(stock)

# or using extend_pandas() :)
stock.sharpe()
```

### QuantStats can make a "Snapshot" of stock performance

```python
qs.plots.snapshot(stock, title='Tesla Performance')
```

## Relevant materials and sources

https://algotrading101.com/learn/yfinance-guide/ <br>
https://pypi.org/ <br>
https://pypi.org/project/QuantStats/



--- END lec12/lec12-2-stocks-options.md ---



--- START lec13/Co2_ClimateChange.md ---

---
title: "Co2_ClimateChange"
type: lecture-notebook
week: 13
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/Co2_ClimateChange.ipynb"
---

## Carbon Dioxide Emissions and Global Climate Change

This notebook is a very simple analysis of global carbon emissons and global temperature change.

- The first point is to load the data directly from the government agencies that manage it.   
- The second is to plot the data and see that we are at the hightest levels of carbon emissions in history.  
- The third is to plot the data and see that we are at the highest levels of global temperature in history.  
- The fourth is to plot the two data series and see that there is a correlation between the two.

```python
import pandas as pd
import plotly.express as px
```

## Global Atmospheric Carbon Levels

The CO2 data is from the Global Monitoring Laboratory of the National Oceanic and Atmospheric Administration (NOAA).  The most frequently cited data is the Mauna Loa Observatory in Hawaii.  

![NOAA](https://upload.wikimedia.org/wikipedia/commons/1/11/NOAA_logo_mobile.svg)

The data is available at https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html.

There are many data series, but we have selected monthly mean data, which are available as a csv.  

Mauna Loa Obervatory is at the top of a volcano in the middle of the Pacific Ocean.  It is a good place to measure CO2 because it is far from any major sources of CO2.  The data is available from 1958 to the present.  The data is in parts per million (ppm) of CO2 in the atmosphere.

```python
url_mlo_co2="https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv"

df_mlo_co2 = pd.read_csv(url_mlo_co2, comment='#')

df_mlo_co2
```

### Let's graph the monthly level of Co2 over time to see the trend.

This is potentially one of the most famous graphs in the world.  It is called the Keeling Curve, after Charles David Keeling, who started the measurements in 1958.  The graph shows the monthly mean level of CO2 in the atmosphere.

```python


fig = px.line(df_mlo_co2, x='decimal date', y='average', title='Mauna Loa CO₂ daily means')
fig.show()
```

## GISS Surface Temperature Analysis (GISTEMP v4) ##

This dataset is the GISS Surface Temperature Analysis (GISTEMP v4) provided by NASA's Goddard Institute for Space Studies. The dataset is updated on a monthly basis and is available from 1880 to the present. The dataset is a combination of land-surface air temperatures and sea-surface water temperatures. The data is provided in tabular format and is available for global, hemispheric, and zonal means. The data is provided as temperature anomalies, which are deviations from the 1951-1980 means. The data is available in csv files and is updated through the most recent month.

![NASA](https://www.giss.nasa.gov/gfx/2013/nasa_x2.png)

The data is available from the following URL:
https://data.giss.nasa.gov/gistemp/

We have chosen the following datasets for this analysis:

**Global-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month**

```python
url_giss_temp = 'https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv'

giss_temp = pd.read_csv(url_giss_temp, skiprows=1)

giss_temp
```

Lets plot the data to see the trend in global temperature over time.

MAM is the variable name for Monthly Anomaly Means.  The data is in degrees Celsius.

```python
fig = px.line(giss_temp, x='Year', y='MAM', title='Global temperature anomaly')
fig.show()
```

## Correlation between CO2 and Global Temperature ##

Let's check the correlation between the two datasets.  

First we need to cut the temperatture anomoly data to match the CO2 data.  The CO2 data is from 1958 to the present.  The temperature data is from 1880 to the present.  We will cut the temperature data to match the CO2 data.  

Then we will make an annual CO2 dataset by taking the average of the monthly data.

Finally we will merge the two datasets on the year.

```python
# cut temp  data to 1958 
giss_temp = giss_temp[giss_temp['Year'] >= 1958]
```

```python
# make an annual graph for the Co2 data
#df_mlo_co2['year'] = df_mlo_co2['decimal date'].apply(lambda x: int(x))
annual_co2 = df_mlo_co2.groupby('year').mean().reset_index()
annual_co2
```

```python
#merge with co2 data
merged_co2_temp = pd.merge(annual_co2, giss_temp, left_on='year', right_on='Year')
```

```python
merged_co2_temp
```

```python
#graph over time using plotly for the merged data
fig = px.scatter(merged_co2_temp, x='average', y='MAM', 
    title='CO₂ vs global temperature anomaly',
    labels={'average':'CO₂ (ppm)', 'MAM':'Temperature anomaly (°C)'},
    trendline='ols')
# add a trendline


fig.show()
```

```python
#calculate the correlation
correlation = merged_co2_temp['average'].corr(merged_co2_temp['MAM'])
print(correlation)
```





```python

```



--- END lec13/Co2_ClimateChange.md ---



--- START lec13/ConstructingMAC.md ---

---
title: "ConstructingMAC"
type: lecture-notebook
week: 13
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/ConstructingMAC.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Peter Grinde-Hollevik<br>
            Dr. Eric Van Dusen
        </p></td></tr>
</table>

```python
from datascience import *
import matplotlib.pyplot as plt
%matplotlib inline 

import numpy as np
import pandas as pd
from matplotlib import patches
```

# Constructing the Marginal Abatement Cost Curve for Methane Gas

## Learning objectives

* Understanding of how to construct the of The McKinsey Greenhouse Gas (GHG) Marginal Abatement Cost Curve (MAC).
* Understanding of how to compare different MAC curves across industries and regions.

![image.png](attachment:image.png)

**This Notebook: Methane Illustration of Global MAC curve**

Please refer to the "Marginal Abatement Cost Curves" chapter for a thorough introduction to abatement curves. This is an empirical application of what you have learned in lecture and through your readings.

The data behind in the 2009 McKinsey Carbon Dioxide abatement curve is difficult to reproduce, and is out of date technologically.  For this empirical case study we are relying on a dataset published by the International Energy Association for methane emissions from the oil and gas sector.  One important starting point is that Methane is more powerful than CO2 in trapping heat in the atmosphere, 80 times more potent in the first 20 years and 20 times more powerful when average over 100 years. Methane emissions from the energy sector contribute 1/3 of all human caused methane emissions.  

**This dataset and documentation are from**: 

- Methane Emissions from Oil and Gas Report from IEA (https://www.iea.org/reports/methane-emissions-from-oil-and-gas)
- IEA Methane Tracker (2021) (https://www.iea.org/articles/methane-tracker-database).



The chart on the left offers indications of where these methane emissions occur within the oil and gas industry.  The chart on the right indicates where the IEA estimates fall in respect to other recent studies.  This clearly illustrates the enormous potential of methane emission abatement in the world's oil and gas sector. With high abatement potential, it is important to understand the various abatement technologies and their costs.

We start by importing a dataset on methane abatement from the International Environmental Agency (IEA):

```python
abatement_table = Table.read_table("abatement_data.csv").where('Cost',are.between(-10.1,10)).where('Possible Savings', are.below(200)).drop('Emissions').relabel('Possible Savings', 'Abatement Potential').relabel('Cost','Abatement Cost')
abatement_table
```

##### The Second column is Region. To begin with, let's select the Asia Pacific region.
The first MAC is drawn for the Asia Pacific region.  This is approximately 1/4 of all emissions.  Later, our analysis for the Asia Pacific Region can compared to North America.

```python
selection = 'Asia Pacific'
abatement_table_ap = abatement_table.where('Region', selection)
abatement_table_ap
```

#### Plotting Functions
Building the MAC requires some data manipulations to build this specific visualization.  The first is that the width of the columns will be the quantity of abatement.  The height ( whether positive or negative) will be the cost.  

For drawing the plot we need to actually find the middle point in each column so that we place each abatement technology at the right place on the MAC. The seconds part runs the function on our x and y axis.

```python
# Making columnn widths for the Asia Pacific _ap Region
def find_x_pos(widths):
    cumulative_widths = [0]
    cumulative_widths.extend(np.cumsum(widths))
    half_widths = [i/2 for i in widths]
    x_pos = []
    for i in range(0, len(half_widths)):
        x_pos.append(half_widths[i] + cumulative_widths[i])
    return x_pos

#Prepare the data for plotting
width_group = abatement_table_ap.column('Abatement Potential')
height_group = abatement_table_ap.column('Abatement Cost')
new_x_group = find_x_pos(width_group)
```

### Policy Analysis Tool
Now lets add a tool in to see the effects of a tax.  With the following function, we introduce level of taxation to measure the total abatement outcome later.

```python
def methane_tax(tax, table):
    if tax < min(table.column('Abatement Cost')):
        print("No Abatement")
    else:
        abatement = table.where('Abatement Cost', are.below_or_equal_to(tax))
        total_abatement = sum(abatement.column('Abatement Potential'))
        abatement_technologies = abatement.column('Abatement technology')
        
        print('Total Abatement (kt CH4): ', np.round(total_abatement,2))
        print("")
```

#### Plot the graph, with tax line
The next function takes the methane_tax function and does plots all the possible abatement opportunities. 

Try to understand what each "plt" part does if you're interested!

```python
def group_plot(tax):
    print(f"Methane Tax ($/MBtu): ${tax}")
    methane_tax(tax, abatement_table_ap)
    plt.figure(figsize=(16,10))
    plt.bar(new_x_group, height_group,width=width_group,edgecolor = 'black')
    plt.title(selection)
    plt.xlabel('Abatement Potential KtCH4')
    plt.ylabel('Abatement Cost $/MBtu')
    plt.axhline(y=tax, color='r',linewidth = 2)
    
group_plot(4)
```

This is an applied MAC for Methane in Asia Pacific. 


Let's make the plot even more instructive by creating a color mapping of the different abatement technologies.
The solution to this is giving each a different color from a Python dictionary.

```python
#Prepare data for plotting (second round)
width = abatement_table_ap.column('Abatement Potential')
height = abatement_table_ap.column('Abatement Cost')
new_x = find_x_pos(width)

#Let's give each type of technology a different color!
abatement_colors_dict = {}
count = 0
colors = ['#EC5F67', '#F29056', '#F9C863', '#99C794', '#5FB3B3', '#6699CC', '#C594C5','#85E827','#F165FD','#1F9F7F','#945CF8','#ff3a1d','#2a8506']
for i in set(abatement_table_ap['Abatement technology']):
    abatement_colors_dict[i] = colors[count]
    count += 1

colors_mapped = list(pd.Series(abatement_table_ap['Abatement technology']).map(abatement_colors_dict))
abatement_table_ap = abatement_table_ap.with_column('Color', colors_mapped)
```

```python
#The Methane curve plot - function!
def mckinsey_curve(tax, abatement_table):
    print(f"Methane Tax: ${tax}")
    methane_tax(tax, abatement_table_ap)
    plt.figure(figsize=(18,12))
    plt.bar(new_x, height, width=width, linewidth=0.1, color=abatement_table['Color'], edgecolor = "black")
    plt.title('Methane Abatement Cost Curve (MAC)')
    plt.xlabel('Abatement Potential KtCH4')
    plt.ylabel('Abatement Cost  $/MBtu')
    plt.axhline(y=tax, color='r', linewidth = 2)

    plt.figure(figsize=(5,1))
    plt.bar(abatement_colors_dict.keys(), 1, color = abatement_colors_dict.values())
    plt.xticks(rotation=60)
    plt.title('Legend')
    
mckinsey_curve(3, abatement_table_ap)
```

#### Color MAC plot for Asia Pacific Region
*Nice Plot!*   From here, we can differentiate the multiple methane abatement technologies on a cost basis, finding the most efficient ways of reducing methane emissions from gas production.

Looking at the Asia Region we can see that most of the opportunities on this graph have a negative cost- meaning that it makes economic sense to make the technological improvements.

We also observe the result of introducing a tax: With a tax of $3 per ton, we expect the total abatement to be almost 4300 tons within this industry.

### Now, repeating the process for North America:

```python
selection = 'North America'
abatement_table_us = abatement_table.where('Region', selection)
abatement_table_us

#Prepare data for plotting (North America. _us)
width = abatement_table_us.column('Abatement Potential')
height = abatement_table_us.column('Abatement Cost')
new_x = find_x_pos(width)

colors_mapped = list(pd.Series(abatement_table_us['Abatement technology']).map(abatement_colors_dict))
abatement_table_us = abatement_table_us.with_column('Color', colors_mapped)

mckinsey_curve(3, abatement_table_us)
```

### Finally, lets build the table for the Global emissions (all regions) :

```python
abatement_table = Table.read_table("abatement_data.csv").where('Cost',are.between(-10.1,10)).where('Possible Savings', are.below(200)).drop('Emissions').relabel('Possible Savings', 'Abatement Potential').relabel('Cost','Abatement Cost')
abatement_table

width = abatement_table.column('Abatement Potential')
height = abatement_table.column('Abatement Cost')
new_x = find_x_pos(width)

colors_mapped = list(pd.Series(abatement_table['Abatement technology']).map(abatement_colors_dict))
abatement_table = abatement_table.with_column('Color', colors_mapped)

mckinsey_curve(3, abatement_table)
```

### Challenge for the student:

You might have noticed how the author of this chapter (Peter) repeats certain lines of code. My challenge to you would be to find these lines and write one big function (or multiple small ones) that streamlines the whole process. Let me know at hollevik@berkeley.edu when you figure it out!



--- END lec13/ConstructingMAC.md ---



--- START lec13/EmissionsTracker.md ---

---
title: "EmissionsTracker"
type: lecture-notebook
week: 13
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/EmissionsTracker.ipynb"
---

## Climate Action Tracker - Country Assessments 


The Climate Action Tracker develops national greenhouse gas emission scenarios to assess countries' mitigation efforts. 
These scenarios include both pledges, such as NDCs and net zero commitments, and emission trajectories under currently adopted policies and actions.
For more details please visit the methodology section on our website: https://climateactiontracker.org/methodology/estimating-national-emissions/

Here, we present the latest version of the scenarios developed for each country. Please note that the assessment date varies per country and that some updates from the past month may not yet be reflected in this table.

Values in the table below are in MtCO2e/year and AR4 GWPs.

Please reference as: 'Climate Action Tracker, Country Assessments | November 2024 - http://climateactiontracker.org'

Copyright © 2024 Climate Action Tracker by NewClimate Institute and Climate Analytics. All rights reserved. The content provided by this website is protected by copyright. 
You are authorised to view, download, print and distribute the content from this website subject to the following condition: Any reproduction, in full or in part, must credit Climate Analytics and NewClimate Institute and must include a copyright notice.

```python
import pandas as pd
import openpyxl
import plotly.express as px
```

```python
url='https://climateactiontracker.org/documents/1289/CAT_14112024_CountryAssessmentData_DataExplorer.xlsx'
```

```python
file_path = 'CAT_14112024_CountryAssessmentData_DataExplorer.xlsx'
sheet_name = 'Assessment'
```

```python
# Reload the data with the first row as headers using openpyxl engine
emissions = pd.read_excel(url, sheet_name=sheet_name, skiprows=18, engine='openpyxl')
emissions
```

LULUCF stands for Land Use, Land-Use Change, and Forestry. In emissions tracking, it refers to the greenhouse gas (GHG) emissions and removals resulting from human activities in forests, croplands, grasslands, wetlands, and other land-use types.

LULUCF plays a dual role in emissions accounting:

Source of Emissions: When forests are cleared, burned, or degraded, or when land is converted to agricultural or other uses, it releases stored carbon into the atmosphere.

Carbon Sink: LULUCF activities can also act as a carbon sink by removing CO₂ from the atmosphere, for example, through reforestation, afforestation, or improved land management practices.

In climate agreements and national emissions inventories, LULUCF is often included separately due to its unique nature of both emitting and absorbing

```python
emissions_cleaned = emissions.dropna(subset=["Country"])
emissions_cleaned
# i want to replace the value in the column "Sector" with "Emissions-exLU" where the value is "Economy-wide, excluding LULUCF"
emissions_cleaned['Sector'] = emissions_cleaned['Sector'].replace("Economy-wide, excluding LULUCF", "Emissions-exLU")
#emissions_cleaned.loc[emissions_cleaned['Sector'] == "Economy-wide, excluding LULUCF", 'Sector'] = "Emissions-exLU"

emissions_cleaned
```

```python
# lets look at China
china = emissions_cleaned[emissions_cleaned['Country'] == 'CHN']
china
```

### Lets looks at China Historical Emissions

Scenario = "Historical" and sector = "Emissions-exLU"

```python
china_historical = china[(china['Scenario'] == 'Historical') & (china['Sector'] == 'Emissions-exLU')]
china_historical
```

###  The data are in the wrong shape for analysis.
- The years are spread out as columns
- Let's  `melt` the DataFrame so that years become rows under a 'Year' column

```python
china_historical_melted = pd.melt(
    china_historical, 
    id_vars=['Version', 'Country', 'Scenario', 'Sector', 'Indicator', 'Unit'],  
    var_name='Year',
    value_name='Emissions'  
)

china_historical_melted['Year'] = pd.to_numeric(china_historical_melted['Year'], errors='coerce')

china_historical_melted.head()
```

```python
fig = px.line(china_historical_melted, x='Year', y='Emissions', title='China Emissions-exLU Historical')
fig.show()
```

##  Now let's get the Forecast data 

In this dataset these are called "current policy scenario,max" and "current policy scenario,min".  
- We will use the same process 'melt' as above to get the data in the right shape for analysis.

```python
china_current_policy = china[china['Scenario'].str.contains('Current Policy')]

china_current_policy_melted = pd.melt(
    china_current_policy_both, 
    id_vars=['Version', 'Country', 'Scenario', 'Sector', 'Indicator', 'Unit'], 
    var_name='Year',  
    value_name='Emissions' 
)

china_current_policy_melted
```

Now we need to pivot the data so that we have the following columns:
- Year
- Max Emissions
- Min Emissions

```python
data_melted = china_current_policy_melted
china_current_policy_melted['Year'] = pd.to_numeric(china_current_policy_melted['Year'], errors='coerce')

china_forecast_pivot = china_current_policy_melted.pivot_table(
    index=['Version', 'Country', 'Sector', 'Indicator', 'Unit', 'Year'],  
    columns='Scenario',  
    values='Emissions'  
).reset_index()  

# Rename the columns for clarity
china_forecast_pivot.columns.name = None  
china_forecast_pivot.rename(columns={'Current Policy, Max': 'Max_Forecast', 'Current Policy, Min': 'Min_Forecast'}, inplace=True)

china_forecast_pivot
```

Let's plot the data to see the trends in the emissions

```python
fig = px.line(china_forecast_pivot, x='Year', y='Max_Forecast', title='China Emissions-exLU Current Policy')
fig.add_scatter(x=china_forecast_pivot['Year'], y=china_forecast_pivot['Min_Forecast'], mode='lines', name='Forecast Min')
fig.show()
```

Let's add historical data to the plot

```python
fig = px.line(china_historical_melted, x='Year', y='Emissions', title='China Emissions-exLU Historical')
fig.add_scatter(x=china_forecast_pivot['Year'], y=china_forecast_pivot['Max_Forecast'], mode='lines', name='Max Emissions')
fig.add_scatter(x=china_forecast_pivot['Year'], y=china_forecast_pivot['Min_Forecast'], mode='lines', name='Min Emissions')
fig.show()
```

```python

```

```python

```

```python

```



--- END lec13/EmissionsTracker.md ---



--- START lec13/KuznetsHypothesis.md ---

---
title: "KuznetsHypothesis"
type: lecture-notebook
week: 13
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/KuznetsHypothesis.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Peter Grinde-Hollevik<br>
            Dr. Eric Van Dusen
        </p></td></tr>
</table>

```python
from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import patches
%matplotlib inline
```

### What is the Environmental Kuznets Curve Hypothesis?

The Environmental Kuznets curve hypothesis that the economic development of a nation is associated with a downward-facing U-shape.

The Y-axis is in terms of the level of environmental degradation (e.g pollution, water quality, deforestation.  The X-axis would be the GDP/capita.  

The idea is that the environmental degradation worsens, until a certain level of income, and after which it gets better. In the US this could be seen in terms of air or water quality, where the skies or rivers were very polluted in the 1960s, until the Clean Air Act and Clean Water Act were passed and Air Quality and Water Quality improved.

Another motivation for the downward slope would be the idea that at some point a wealthier society demands environmental improvements.  

However, could this hold for the potentially most important Pollutant $CO_2$, the main driver of anthropogenic climate change? Controversially, the impacts of global CO2 pollution are not experienced *locally*, but are experienced as *global* effects. So it is not clear whether the Environmental Kuznets hypothesis will hold.  

Today, we'll look to build an $CO_2$ Kuznets curve for an *association* between the amount of $CO_2$ emitted per capita (t/CO2) and the growing GDP per capita (USD). This dataset is collected from Our World in Data, a great source of all sorts of data types!

![kuznets.png](attachment:kuznets.png)

### Building our own Environmental Kuznets Curve

We start by importing data on GDP per capita and Per Capita $CO_2$ emissions for every country in the world for as long as it has been recorded.

Source: Gapminder Foundation

```python
co2_table = Table.read_table('co2-emissions-vs-gdp.csv').drop('145446-annotations','Total population (Gapminder, HYDE & UN)','Code')
co2_table = co2_table.relabeled('Entity', 'Country')
co2_table
```

#### Low Income Countries
Let's start by selecting a set of Low Income Countries to graph the movement of $CO_2$ intensity

```python
#Low-Income Nations
LIH_array = make_array('Haiti', 'Afghanistan','Rwanda','Pakistan', 'Nicaragua')
LIH_table = co2_table.where('Country', are.contained_in(LIH_array))
LIH_table = LIH_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
plt.figure(figsize = (8,6), dpi=250)
LIH_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country');
```

Note that each dot represents a nation at a given level of emissions and GDP per capita

With these three countries we see a few different stories:
 - In Afghanistan, Haiti and Rwanda we see little income growth and little $CO_2$ intensity growth with a slight upward trend
 - In Nicaragua we see some jumping around, in fact Nicaragua GDP per capita has gone up and down, as has $CO_2$ per capita
 - In Pakistan, a larger and more populous country we see strong linear upward growth in both GDP per capita and $CO_2$ per capita, with no signs of turning down

In these countries it is hard to tell the complete story without the exact time trend.

#### BRICS 
Lets look at the BRICS countries, the rapidly growing upper middle income countries

```python
BRICS_array = make_array('Brazil','Russia','India','China','South Africa')
BRICS_table = co2_table.where('Country', are.contained_in(BRICS_array))
BRICS_table = BRICS_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
BRICS_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

The BRICS nations seem to have a variety of development pathways but all show the linear trend of increasing emissions as wealth grows.  
- Russia has an interesting dip while GDP per capita decrease and then increase again
- South Africa has a recent period where growth in both GDP per capita and $CO_2$ per capita have stagnated
- China and India show linearly increasing trends, with China both wealthier and more $CO_2$ intensive

### Individual country graphs
Lets look at some individual countries, starting with the US
We can plot both total and logged quantities

```python
US_table = co2_table.where('Country', 'United States').where('Year', are.between(1800,2018))
US_table = US_table.with_column('LogGDP', np.log(US_table.column('GDP per capita'))).with_column('LogCO2',np.log(US_table.column('Per capita CO2 emissions')))
US_table.scatter('GDP per capita', 'Per capita CO2 emissions')
US_table.scatter('LogGDP', 'LogCO2')
```

**Looks like we have a curve!**

In the case of the US it does indeed look like the C02 per capita does indeed start to deline after about USD 40,000.

Somewhere around 2000-2004 the $CO_2$ emissions leveled off and then began to decline

### Let's look at China
In COP26 and 27 (this year), China has been a large part of the discussion (and emissions). Let's have a look at their curve!

```python
#China Example + LOG
NO_table = co2_table.where('Country', 'China').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
```

Indeed it appears that China has an inflection point and is starting to level off in the $CO_2$ intensity per capita.

What about India?

```python
#India Example + LOG
NO_table = co2_table.where('Country', 'India').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
```

As I'm Norwegian, I thought it might be cool to see how things are going back home as well:

```python
#Norway Example + LOG
NO_table = co2_table.where('Country', 'Norway').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
```

Turns out we're ahead of the US in $CO_2$ emissions per capita, but there's still a long way to go until our development resembles a full Kuznets curve. 

However, it certainly looks like something! An almost vertical linear growth in terms of per capita $CO_2$ emissions in the early economic stages stagnated into a period of fluctuations. As of now, it looks like it’s heading in a downward trend.

Let's look at a set of other High Income Nations:

```python
HIN_array = make_array('United States', 'Netherlands', 'United Kingdom','Germany','Canada')
HIN_table = co2_table.where('Country', are.contained_in(HIN_array))
HIN_table = HIN_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
HIN_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

```python
HMLIN_array = make_array('United States', 'China', 'India','Germany')
HMLIN_table = co2_table.where('Country', are.contained_in(HMLIN_array))
HMLIN_table = HMLIN_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
HMLIN_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

As in the US and Norway, these nations have experienced a boom, stagnation, and now to some extent a downward trend. Let's finally plot all the previously observed nations together:

```python
ALL_array = np.append((np.append(LIH_array,BRICS_array)), HIN_array)
ALL_table = co2_table.where('Country', are.contained_in(ALL_array))
ALL_table = ALL_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
ALL_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
#What do we see? Can we spot the Environmental Kuznets Curve?
```

Here we see potential evidence for an Environmental Kuznets Curve.

It seems, at least to some extent, that as nations develop economically, the level of environmental degradation reaches a peak and then declines, mapping a downward-facing U-curve.

### [BONUS] Criticism of the Environmental Kuznets Curve Hypothesis

Some questions we ought to ask ourselves in the end are:
* Do all types of environmental degradation follow the curve? What if we plot Energy, Land, & Resource usage?
* What we plotted today shows the ratio between GDP and CO2 per capita, but what about the *absolute* numbers of emissions?
* What is the true long-term shape of the curve? Could it reshape itself to an "N" as an economy passes a certain threshold?
* What about its applicability on a global scale? Knowing that the HINs have a habit of exporting pollution to LINs, what will happen as LIN grow economically?

These are just some questions environmental economists have asked themselves throughout the years since the curve was hypothesized in 1955. Some, including Perman and Stern (2003) conclude that the level of environmental degradation has much more to do with a constant "battle" between scale and time than income alone. As nations scale up (BRICS, for instance) the growth results in higher emissions, while countries with lower growth (LIN & HIN) seem more influenced by the "time-effect", which results in lower emissions. Others, among Krueger & Grossman, argue that there is "no evidence that environmental quality deteriorates steadily with economic growth."

More on these theories can be found in the recommended readings below.

As data scientists motivated to help heal the plant with the tools of environmental economics, we can help to find these answers!

#### What's next?

If you are interested in this area, there are even more fascinating applications of Data Science to environmental topics such as: finding the social cost of carbon, the valuation of our environment, and the economics of emissions trading. Besides purely economical modeling, the field of environmental data science is rapidly growing as we collect more and more data on our planet and its resources. Applying the power of Satellite Imagery, Machine Learning, and Geographic Information Systems (GIS), one can follow both technology and policy-based paths, both ensured to have a positive impact in shaping a data-driven, sustainable future.

### If you liked this, check out these readings!

Levelized Cost of Carbon Abatement: An Improved Cost-Assessment Methodology for a Net-Zero Emissions World (also the main source of this Jupyter Notebook)

https://www.energypolicy.columbia.edu/sites/default/files/file-uploads/LCCA_CGEP-Report_101620.pdf

Dynamic vs. Static costs are described further in K.Gillingham & J.H Stock's The Cost of Reducing Greenhouse Gas Emissions (italic) from 2018. - A highly recommended reading out of scope for this class.

https://scholar.harvard.edu/files/stock/files/gillingham_stock_cost_080218_posted.pdf

Goldman Sachs Research: Carbonomics: The Future of Energy in the Age of Climate Change
  
https://www.goldmansachs.com/insights/pages/carbonomics.html

EPA article on the Economics of Climate Change:
https://www.epa.gov/environmental-economics/economics-climate-change

Draw your own curve program:
https://tamc.github.io/macc/

Abatement curve for crops:
https://github.com/aj-sykes92/ggmacc/blob/main/README_files/figure-gfm/full-macc-1.png


Aalborg University's software:
https://github.com/matpri/EPLANoptMAC



--- END lec13/KuznetsHypothesis.md ---



--- START lec13/RoslingPlots.md ---

---
title: "RoslingPlots"
type: lecture-notebook
week: 13
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec13/RoslingPlots.ipynb"
---

<table style="width: 100%;">
    <tr style="background-color: transparent;"><td>
        <img src="https://data-88e.github.io/assets/images/blue_text.png" width="250px" style="margin-left: 0;" />
    </td><td>
        <p style="text-align: right; font-size: 10pt;"><strong>Economic Models</strong>, Fall 2024<br>
            Peter Grinde-Hollevik<br>
            Dr. Eric Van Dusen
        </p></td></tr>
</table>

```python
from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import patches
%matplotlib inline 
from matplotlib.pyplot import figure
pd.plotting.register_matplotlib_converters()
import seaborn as sns
from pylab import *
from ipywidgets import interact
from IPython.lib.display import YouTubeVideo
```

### Plotting Global Emissions with "Rosling"-style Plots

This notebook is inspired by the late Professor Hans Rösling's wonderful contributions to the world of data visualization, which can be seen at the Gapminder website. 

[You can see the Gapminder version of this visualization here](https://www.gapminder.org/tools/#$model$markers$bubble$encoding$y$data$concept=co2_emissions_tonnes_per_person&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2018;;;;;&chart-type=bubbles&url=v1).  

This notebook aims to reproduce the "bubble charts" in understanding the historical, global emissions of the last century.

As a warm-up exercise, feel free to check out his famous ["200 Countries, 200 years, 4 minutes"](https://www.youtube.com/watch?v=jbkSRLYSojo) from the BBC series *The Joy of Stats*. This video plots life expectancy over changing per capita income. After watching this, you'll understand why the arguments made by Professor Rösling's visualization have had such an impact.

```python
YouTubeVideo('jbkSRLYSojo')
```

### Building Our Own Rosling Plot

The goal of this exercise is to gain some experience in data cleaning, making sense of it with basic plots, and present it in the most "friendly" manner for scientific communication. The two first will be accomplished by using tools from DATA 8, while the latter requires the usage of methods you might not be familiar with yet. However, with this walkthrough, you ought to be more comfortable with building a Rosling Plot of your own in the future.

We start off by importing a Gapminder Foundation Dataset containing per capita GDP and per capita emissions of $CO_2$.

```python
co2_table = Table.read_table('co2-emissions-vs-gdp.csv')
co2_table
```

Our first Rosling Plot will focus on the latest year of data: 2018. Hence, we select only the rows where 'Year' = 2018.

```python
co2_table = co2_table.where('Year', 2018)
co2_table
```

Now, let's consider what we would like to plot for year 2018: Following Rosling's philosophy we should consider how emissions change over economic development. Hence, our x-axis should be GDP per capita. Our y-axis could be a new column: Total Emissions, made by the product of Per Capita $CO_2$ emissions and the total population of each country.

```python
#Make the 'Total CO2 Emissions' column
total_co2_emissions = co2_table.column('Per capita CO2 emissions') * co2_table.column('Total population (Gapminder, HYDE & UN)')
co2_table = co2_table.with_column('Total CO2 Emissions', total_co2_emissions )
co2_table.sort("GDP per capita", descending=True)
```

Observe the 'nan' values spread across almost all columns. These are 'empty' data points, where data has simply not been collected. To ensure the validity of our analysis, we decide to remove all rows with 'per capita $CO_2$ emissions' and 'total $CO_2$ emissions' are empty. For this, we use the **.where** and **.are.above(0)** methods from DATA 8. Furthermore, we select the columns relevant to our inquiry. We also sort the table to have the biggest emitters at the top and remove any continent from the 'entity column'.

```python
#Selecting relevant columns, removing 'NaN' values, & sorting descending
co2_table = co2_table.select('Entity', 'Total CO2 Emissions', 'GDP per capita')
co2_table = co2_table.sort('Total CO2 Emissions', descending=True)
co2_table = co2_table.where('Total CO2 Emissions', are.above(0))
co2_table = co2_table.where('GDP per capita', are.above(0))
continents = make_array('World', 'Asia', 'Europe', 'Africa', 'North America', 'Latin America', 'Oceania')
co2_table = co2_table.where('Entity', are.not_contained_in(continents))
co2_table
```

Having cleaned our data, look at a scatter plot and see if we can make sense of it. What might we expect to see plotting Total CO2 emissions over GDP per capita? Do you expect any outliers? If so, what countries do you think that would be?

```python
#Let's scatter plot our data. See any association between GDP per capita and Total CO2 Emissions?
co2_table.scatter('GDP per capita', 'Total CO2 Emissions')
```

This plot is hard to read, and is dominated by a few outliers (large emitters and very wealthy nations). Hence, let us take the log of each axis. Percentage changes in GDP per capita should now refer to percentage changes in total emissions in each country.

```python
# use np.log on each column
gdp_log = np.log(co2_table.column('GDP per capita'))
co2_log = np.log(co2_table.column('Total CO2 Emissions'))

#Log-plot. Easier to see the association?
log_co2_table = co2_table.with_columns('Log GDP per capita', gdp_log, 'Log Total CO2 Emissions', co2_log)
log_co2_table.scatter('Log GDP per capita', 'Log Total CO2 Emissions', fit_line = True)
```

To find out the slope and intercept of that line fitted through the data, we run a simple Linear Regression model using Numpy.

```python
x = gdp_log
y = co2_log
np.polyfit(x, y,1)
```

### Part 2: A Historical View on $CO_2$ emissions

As of now, it seems like we have a strong linear association between log total $CO_2$ emissions and log GDP per capita. To explore this association further, let us view this from a historical perspective by answering the following question: How has the relationship between the two variables changed over time? To do so, we build the Rosling Plot we sought out in the first place; one for each year the last 120 years.

```python
#For this, we need the population & continent of each country.
gapminder = Table.read_table('gapminder - gapminder.csv')
gapminder
```

We use the DATA 8 method **.join** you are familiar with by now to add continent and population for each nation. The color of the "bubbles" we are making will represent the continent, while its size represents the population.

```python
bubble_table = co2_table.join('Entity', gapminder, 'country')
bubble_table = bubble_table.select('Entity', 'Total CO2 Emissions', 'GDP per capita','population','continent')
bubble_table.sort('Total CO2 Emissions', descending=True)
```

Using this newly created table, we can visualize the log-plot from above in a "friendlier" manner using an imported method called Seaborn. Feel free to check out its documentation here: https://seaborn.pydata.org/generated/seaborn.scatterplot.html

```python
population = bubble_table.column('population')
plt.figure(figsize = (8,6), dpi=250)
#sns.scatterplot(bubble_table.column('GDP per capita'), bubble_table.column('Total CO2 Emissions'), hue = bubble_table.column('continent'), size= population, sizes=(20,400), legend=False);
sns.scatterplot(
    x=bubble_table.column('GDP per capita'), 
    y=bubble_table.column('Total CO2 Emissions'), 
    hue=bubble_table.column('continent'), 
    size=population, 
    sizes=(20, 400), 
    legend=False
)
plt.grid(True)
plt.xscale('log')
plt.yscale('log')
plt.xlabel('GDP per capita [USD]')
plt.ylabel('Total CO2 Emissions [Tons]')
plt.title('World CO2 Emission in 2018')
plt.xticks([1000, 10000, 100000],['1k', '10k', '100k'])
plt.show();
```

So, this was our Rosling Plot for 2018. From here, we aim to create a function that we can apply to each year in our original dataset. Remember the one with 50.000+ entries, each representing a country in a given year?

One thing to note is that our could be changed a tad: Would it not make sense to plot $CO_2$ emissions per capita over GDP per capita? Doing so, we get a sense of the $CO_2$ intensity of GDP.

```python
def the_bubble_plot(emissions_data, population_data, income_data, geography_data, year):
    # Selecting data for given year & relabel columns
    emissions_data = emissions_data.select('country', f"{year}").relabel(1, 'CO2 per capita [tons]')
    population_data = population_data.select('country', f"{year}").relabel(1, 'Population')
    income_data = income_data.select('country', f"{year}").relabel(1, 'GDP per capita [USD]')

    # Creating the 'the_bubble_table' with emission, population, income, continent for given year
    the_bubble_table = emissions_data.join('country', population_data)
    the_bubble_table = the_bubble_table.join('country', income_data)
    the_bubble_table = the_bubble_table.join('country', geography_data)

    # Filter to ensure we have valid emissions data for the given year
    the_bubble_table = the_bubble_table.where('CO2 per capita [tons]', are.above(0))

    # Prepare data for Seaborn (convert to NumPy arrays)
    gdp_per_capita = the_bubble_table.column('GDP per capita [USD]')
    co2_per_capita = the_bubble_table.column('CO2 per capita [tons]')
    population = the_bubble_table.column('Population')
    continent = the_bubble_table.column('continent')

    # Create the scatter plot
    plt.figure(figsize=(8, 6), dpi=250)
    sns.scatterplot(
        x=gdp_per_capita,
        y=co2_per_capita,
        hue=continent,
        size=population,
        sizes=(20, 400),
        legend=False
    )
    
    # Adjusting the plot for better visualization
    plt.grid(False)
    plt.xscale('log')
    plt.yscale('log')
    plt.xlabel('GDP per capita [USD]', fontsize='large')
    plt.ylabel('CO2 per capita [tons]', fontsize='large')
    plt.title(f"World CO2 Emissions in {year}", fontsize='x-large')
    plt.xticks([10**0, 10**1, 10**2], ['1k', '10k', '100k'])
    plt.yticks([10**-2, 10**-1, 10**0, 10**1], ['0.01', '0.1', '1', '10'])
    plt.show()


# Importing data from the Gapminder Foundation
emissions_data = Table.read_table('co2_emissions_tonnes_per_person.csv')
population_data = Table.read_table('population_total.csv')
income_data = Table.read_table('income_per_person_gdppercapita_ppp_inflation_adjusted.csv')
geography_data = Table.read_table('gapminder - gapminder.csv').select('country', 'continent')

# Call the function for the year 2017
the_bubble_plot(emissions_data, population_data, income_data, geography_data, 2017)
```

```python

```



--- END lec13/RoslingPlots.md ---



--- START lec15/vibecession.md ---

---
title: "vibecession"
type: lecture-notebook
week: 15
source_path: "/Users/ericvandusen/Documents/Data88E-ForTraining/F24Lec_NBs/lec15/vibecession.ipynb"
---

##  Vibecession - The Notebook!
The purpose of this notebook is to get the data and check the vibes behind the post-COVID drop in Consumer Sentiment

The concept of vibecession and the testing it in a notebook folllows from a couple key sources
 - A Tweet by **Quantian** that [showed a test of the hypothesis](https://threadreaderapp.com/thread/1688397994821873664.html#google_vignette) - that this notebook is recreating

- Which was followed up by a FT recreation - for multiple countries  - and how partisan this gap is !  You can read about it here [Should we believe Americans when they say the economy is bad? John Burn-Murdoch](https://www.ft.com/content/9c7931aa-4973-475e-9841-d7ebd54b0f47)

### But our starting point can be a more accessible commentator 


[Kyla Scanlon](https://kylascanlon.com/), an online content creator and independent economics educator, coined the term "vibecession" to describe a phenomenon where public sentiment about the economy is overwhelmingly negative, despite relatively positive economic indicators like GDP growth and low unemployment rates. The term is a portmanteau of "vibes" and "recession," suggesting an economic downturn driven primarily by negative public sentiment rather than direct economic metrics.

The concept of the vibecession was introduced by Scanlon in a June 2022 newsletter, amidst observations that, while hard economic data was showing signs of stability and growth, the general sentiment among the public remained pessimistic. This disconnect, where the public feels economically insecure despite positive indicators, has been a central theme in Scanlon's discussions on platforms like TikTok, where she aims to make complex economic concepts more accessible and engaging to a broader audience, particularly younger people.


[Kyla Scanlon](https://www.marketplace.org/2022/09/07/for-tiktok-maker-kyla-scanlon-its-about-making-finance-fun-and-a-bit-chaotic/) on Marketplace

```python
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error
import plotly.express as px
```

```python
# try to import fredapi
try:
    from fredapi import Fred
except ImportError:
    !pip install fredapi
    from fredapi import Fred

# try to import yfinance
try:
    import yfinance as yf
except ImportError:
    !pip install yfinance
    import yfinance as yf
fred = Fred(api_key='e3053cdc3e94dfb2b73c5945b0d1b1f7')
```

##  Part  1 - Gather the Data 
 - The following sections are how we source the data
  - This may be too boring and pedantic  - but they are related to the learnign outcomes of the class 
  - We will use a combination of the python package `fredapi` to dowload a few time series from the Federal Reserve Bank of St Louis
  - And python package `yfinance` (derived from Yahoo Finance)  to get additional series
  - An enterprising student could add additional series to the modeling process
*Skip to Part 2 if you just want to see the results*

## Consumer Sentiment 

The [University of Michigan Consumer Sentiment Index](http://www.sca.isr.umich.edu/) is an economic indicator that assesses the confidence, conditions, and expectations of U.S. consumers regarding their financial situation and the general state of the economy. The index is based on a monthly survey of approximately 500 households regarding their personal finances, business conditions, and buying conditions. It is divided into two parts: the Index of Consumer Expectations and the Current Economic Conditions Index.

This data is significant as it can provide insights into consumer behavior, which helps in predicting changes in spending and saving habits. Higher consumer confidence typically indicates that people feel secure in their personal financial situation and thus are more likely to increase spending, which is a key driver of economic growth.

The index is often used by analysts and policymakers to understand consumer sentiment and its potential impact on the economy. For instance, rising sentiment can suggest increased consumer spending and economic expansion, while declining sentiment might indicate economic slowdowns or recessions.

```python
# This is the data we want to model, we are going to get the UMCSENT series from FRED
UMCSENT = fred.get_series('UMCSENT', observation_start='1979-01-01', observation_end='2024-03-01')
UMCSENT
print(UMCSENT.index.tzinfo)
UMCSENT
```

Lets take a look at the data that we are trying to model 

The time range is roughly the last 40 years

```python
fig = px.line(UMCSENT, title='Consumer Sentiment Index')
fig.update_yaxes(range=[0, 120])
fig.show()
```

###  Get some Features eg Explanatory Variables for our Model

Now we want to search for some basic economics variables that could explain how Consumers are Feeling 

Variables that were suggested by Quantian on Twitter were the following 

- Inflation rate
- Inflation rate change
- Unemployment
- Unemployment change
- Housing prices
- Real wages
- Dollar strength
- Interest rates
- Stock prices

For the first set of Series we will go FRED and download them according to the series name

```python
#Unemployment rate
UNRATE=fred.get_series('UNRATE', observation_start='1979-01-01', observation_end='2024-03-01')
#Inflation rate
CPIAUSCL= fred.get_series('CPIAUCSL', observation_start='1979-01-01', observation_end='2024-03-01')
#GDP
GDP=fred.get_series('GDP', observation_start='1979-01-01', observation_end='2024-03-01')
#housing price change
USSTHPI=fred.get_series('USSTHPI', observation_start='1979-01-01', observation_end='2024-03-01')
#intereste rate
FEDFUNDSS=fred.get_series('FEDFUNDS', observation_start='1979-01-01', observation_end='2024-03-01')
#Real Wages
WAGES=fred.get_series('LES1252881600Q', observation_start='1979-01-01', observation_end='2024-03-01')

# The following Series ARE available on FRED but not with the time range we need
#S&P 500
#SP500=fred.get_series('SP500', observation_start='1979-01-01', observation_end='2024-03-01')
#Dolar index monthly
#DTWEXM=fred.get_series('DTWEXM', observation_start='1979-01-01', observation_end='2024-03-01')
# inflation change quarter to quarter
#INFCHANGE=fred.get_series('BPCCRO1Q156NBEA', observation_start='2019-01-01', observation_end='2024-03-01')
```

### Processing the data 

- In the next few cells I will take a look at each series we have downloaded 
- Some have to check the Time Zone - I am going to go with making them all "time zone naive"
- Some data are monthly, some are quarterly - I am going to adjust everything to monthly by filling in quarterly data for the first of each month in that quarter

```python
#Inflation rate
print(CPIAUSCL.index.tzinfo)
CPIAUSCL
```

```python
#housing price change index is quarterly, we need to resample it to monthly
USSTHPI=USSTHPI.resample('MS').ffill()
print(USSTHPI.index.tzinfo)

USSTHPI
```

```python
# remove this series 
#DTWEXM=DTWEXM.resample('MS').ffill()
#print(DTWEXM.index.tzinfo)

#DTWEXM
```

```python
# Wage series is quarterly, we need to resample it to monthly
WAGES=WAGES.resample('MS').ffill()
print(WAGES.index.tzinfo)

WAGES
```

```python
#convert gdp to monthly by using the quarterly data
GDP=GDP.resample('MS').ffill()
print(GDP.index.tzinfo)

GDP
```

```python
# Make a new variable thats the percent change in CPI
cpichange = CPIAUSCL.pct_change()
print(cpichange.index.tzinfo)

cpichange
```

### For the next couple of series we can get a longer time series by going to YFinance



```python
#download S&P 500 data close price only
SP500 = yf.download('^GSPC', start='1979-01-01', end='2024-03-01')
#convert daily data to monthly data
SP500=SP500.resample('MS').mean()
#drop all columns except close price
SP500=SP500['Close']
#make time zone naive
SP500.index = SP500.index.tz_localize(None)

SP500
```

```python
# Get the Dollar Index data
DXY = yf.download('DX-Y.NYB', start='1979-01-01', end='2024-03-01')
DXY=DXY.resample('MS').mean()
DXY=DXY['Close']
DXY.index = DXY.index.tz_localize(None)
DXY
```

Now Lets Combine these series into a Dataframe called Vibes
- save it as a csv so we can skip all the data processing in the future
- Drop missing values for now (What gets dropped!?!) ( eg data before Nov 2023)
- check the data visually

```python
# create a dataframe using pd.concat
vibes = pd.concat([UMCSENT, UNRATE, CPIAUSCL, GDP, USSTHPI, FEDFUNDSS, SP500, DXY, cpichange,WAGES], axis=1) 
vibes.columns = ['UMCSENT', 'UNEMPLOYMENT', 'CPI', 'GDP', 'HOUSINGPRICE', 'FEDFUNDS', 'SP500', 'DOLLAR', 'CPICHANGE','WAGES'] 
# drop any rows with missing values
vibes.dropna(inplace=True)
# save vibes to a csv file
vibes.to_csv('vibes.csv')
vibes
```

## Part 2  - Modeling Consumer Sentiment with Macroeconomic Data Series 

In the next section we will be modeling consumer sentiment and using the macroeconomcis time series 
- split up the data into before and after covid
- run a regression to predict consumer sentiment
- Compare predicted to actual outcomes
- Look at the residuals
- Show how to run the model in SKLearn instead of statsmodels

```python
#read in vibes.csv
vibes = pd.read_csv('vibes.csv', index_col=0)
# Y = the variable we want to predict, the Target in ML
# X = the explanatory variabbles we use to predicy Y , or features in ML
X = vibes[['UNEMPLOYMENT', 'CPI', 'GDP', 'HOUSINGPRICE', 'FEDFUNDS', 'SP500', 'DOLLAR', 'CPICHANGE','WAGES']]
Y = vibes['UMCSENT']  # Make sure this is the correct column name for Consumer Sentiment
```

The first model will use Statsmodels to run a simple linear regression 

*I know there are problems with this model specification!*

```python
X = sm.add_constant(X)
# Fit the model
model = sm.OLS(Y, X).fit()
# Print out the statistics
print(model.summary())
```

### Redo the Model as before and after COVID

The idea here is to train the model toon data up until 2019 and then use it to predict 2020-2024

```python
# Split the data into training and testing sets before and after Dec 2019
vibes_train = vibes.loc[:'2019-12-01']
vibes_test = vibes.loc['2020-01-01':]

X_train = vibes_train[['UNEMPLOYMENT', 'CPI', 'GDP', 'HOUSINGPRICE', 'FEDFUNDS', 'SP500', 'DOLLAR', 'CPICHANGE','WAGES']]
Y_train = vibes_train['UMCSENT']

X_test = vibes_test[['UNEMPLOYMENT', 'CPI', 'GDP', 'HOUSINGPRICE', 'FEDFUNDS', 'SP500', 'DOLLAR', 'CPICHANGE','WAGES']]
Y_test = vibes_test['UMCSENT']

X_train = sm.add_constant(X_train)
X_test = sm.add_constant(X_test)
```

```python
# Fit the model on the training data
model = sm.OLS(Y_train, X_train).fit()
# Summary of the model
print(model.summary())
# Predict the test data
Y_pred = model.predict(X_test)
# Calculate the MAE
mae = mean_absolute_error(Y_test, Y_pred)
print(f'The Mean Absolute Error of the model is {mae}')
```

```python
#plot the actual vs predicted values over the testing data
plt.figure(figsize=(12, 6))
plt.plot(Y_train, label='Train')
plt.plot(Y_test, label='Test')
plt.plot(Y_pred, label='Predicted')
plt.ylabel('Consumer Sentiment Index')
plt.legend()
plt.show()
```

```python
# Plot the data and the model's prediction for the entire time period
#X = vibes[['UNEMPLOYMENT', 'CPI', 'GDP', 'HOUSINGPRICE', 'FEDFUNDS', 'SP500', 'DOLLAR', 'CPICHANGE','WAGES']]
#X = sm.add_constant(X)
#Y = vibes['UMCSENT']
Y_pred_all = model.predict(X)
plt.figure(figsize=(12, 6))
plt.plot(Y, label='Actual')
plt.plot(Y_pred_all, label='Predicted')
# add a line at March 2020 
plt.axvline('2020-03-01', color='red', linestyle='--')
#Label the Y axis
plt.ylabel('Consumer Sentiment Index')
# label the x axis by every 5 years
plt.xticks(['1980-01-01', '1985-01-01', '1990-01-01', '1995-01-01', '2000-01-01', '2005-01-01', '2010-01-01', '2015-01-01', '2020-01-01'])

plt.legend()
plt.show()
```

```python
# Plot the data and the model's prediction for the entire time period using plotly
# Plot actual and predicted values for Y
fig = px.line(vibes, y=['UMCSENT'], title='Consumer Sentiment Index')
fig.add_scatter(x=vibes.index, y=Y_pred_all, mode='lines', name='Predicted')
fig.update_yaxes(range=[0, 120])
# add a line at March 2020
fig.add_vline(x='2020-03-01', line_dash='dash', line_color='red')
# add y axis label
fig.update_yaxes(title_text='Consumer Sentiment Index')
fig.show()
```

```python
#plot the actual vs predicted values
plt.plot(Y_test.index, Y_test, label='Actual')
plt.plot(Y_test.index, Y_pred, label='Predicted')
plt.legend()
plt.show()
```

```python
#plot the residuals
residuals = Y_test - Y_pred
plt.plot(Y_test.index, residuals)
plt.axhline(0, color='red', linestyle='--')
plt.show()
```

```python
#plot the residuals over the entire time period 

residuals_all = Y - Y_pred_all
plt.plot(Y.index, residuals_all)
plt.axhline(0, color='red', linestyle='--')
# label the x axis by every 5 years
plt.xticks(['1980-01-01', '1985-01-01', '1990-01-01', '1995-01-01', '2000-01-01', '2005-01-01', '2010-01-01', '2015-01-01', '2020-01-01'])

plt.show()
```

```python
# Lets make a model using sklearn
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
```

```python
# SKlearn Linear Regression Model
# Create a linear regression model
lr = LinearRegression()

# Fit the model
lr.fit(X_train, Y_train)

# Predict on the test set
Y_pred = lr.predict(X_test)

# Calculate the MAE
mae = mean_absolute_error(Y_test, Y_pred)
print(f'The Mean Absolute Error of the model is {mae}')

# print the R^2 value
print(f'The R^2 value of the training model is {lr.score(X_train, Y_train)}')
print(f'The R^2 value of the model on test data is {lr.score(X_test, Y_test)}')
```

```python
#plot the actual vs predicted values
plt.plot(Y_test.index, Y_test, label='Actual')
plt.plot(Y_test.index, Y_pred, label='Predicted')
plt.legend()
plt.show()
```

```python
# Fit the model
model = LinearRegression()
model.fit(X_train, Y_train)
# Make predictions
y_pred = model.predict(X_test)
# Evaluate the model
mae = mean_absolute_error(Y_test, y_pred)
# Print the MAE
print(f'The Mean Absolute Error of the model is {mae}')
# print the R^2
print(f'The R^2 of the model on test data is {model.score(X_test, Y_test)}')
```

```python
#Plot y  over time with years on the x-axis
plt.figure(figsize=(12,6))
plt.plot(Y.index, Y, label='Actual')
plt.plot(Y_test.index, y_pred, label='Predicted')
plt.ylabel('Consumer Sentiment Index')
plt.legend()
plt.show()
```

##  SKlearn - Let's try a different ML model
Now that we have the ML model all set up, we can try a different ML model.  One that has been suggested for time series for improved fits is the Random Forest Regressor

```python
from sklearn.ensemble import RandomForestRegressor

# Create the model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Fit the model
model.fit(X_train, Y_train)

# Predict on the test set
RFRpred = model.predict(X_test)

#Evaluate the model on traning data
r2 = model.score(X_train, Y_train)
print("R-squared on training data :", r2)

# Calculate the MAE
mae = mean_absolute_error(Y_test, RFRpred)
print(f'The Mean Absolute Error of the model is {mae}')

# Evaluate the model (using R^2 score here)
r2 = model.score(X_test, Y_test)
print("R-squared on test data :", r2)
```

Let's look a the plots as we did for the regressions above

```python
#Plot y  over time with years on the x-axis
plt.figure(figsize=(12,6))
plt.plot(Y.index, Y, label='Actual')
plt.plot(Y_test.index, RFRpred, label='Predicted')
plt.ylabel('Consumer Sentiment Index')
plt.legend()
plt.show()
```

```python
#Plot the Y_test and the predicted values
plt.plot(Y_test.index, Y_test, label='Actual')
plt.plot(Y_test.index, RFRpred, label='Predicted')
plt.legend()
plt.show()
```

```python
# plot the Y and the predicted values over the entire time period
RFRpred_all = model.predict(X)
plt.figure(figsize=(12, 6))
plt.plot(Y, label='Actual')
plt.plot(RFRpred_all, label='Predicted')
plt.ylabel('Consumer Sentiment Index')
plt.legend()
plt.show()
```

```python
# Plot the feature importances
importances = model.feature_importances_
plt.bar(X.columns, importances)
plt.ylabel('Importance')
plt.xticks(rotation=45)
plt.show()
```



```python

```



--- END lec15/vibecession.md ---

