

--- SUMMARY for TEXTBOOK ---

- chapter: 0
  count: 2
  files:
  - file: 00-intro/index.md
    title: index
    source_path: content/00-intro/index.ipynb
    type: textbook
    order: 1
  - file: 00-intro/.ipynb_checkpoints/index-checkpoint.md
    title: index-checkpoint
    source_path: content/00-intro/.ipynb_checkpoints/index-checkpoint.ipynb
    type: textbook
    order: 2
- chapter: 1
  count: 5
  files:
  - file: 01-demand/index.md
    title: index
    source_path: content/01-demand/index.md
    type: textbook
    order: 1
  - file: 01-demand/01-demand.md
    title: 01-demand
    source_path: content/01-demand/01-demand.ipynb
    type: textbook
    order: 2
  - file: 01-demand/02-example.md
    title: 02-example
    source_path: content/01-demand/02-example.ipynb
    type: textbook
    order: 3
  - file: 01-demand/03-log-log.md
    title: 03-log-log
    source_path: content/01-demand/03-log-log.ipynb
    type: textbook
    order: 4
  - file: 01-demand/04-elasticity.md
    title: 04-elasticity
    source_path: content/01-demand/04-elasticity.ipynb
    type: textbook
    order: 5
- chapter: 2
  count: 6
  files:
  - file: 02-supply/index.md
    title: index
    source_path: content/02-supply/index.md
    type: textbook
    order: 1
  - file: 02-supply/01-supply.md
    title: 01-supply
    source_path: content/02-supply/01-supply.ipynb
    type: textbook
    order: 2
  - file: 02-supply/02-eep147-example.md
    title: 02-eep147-example
    source_path: content/02-supply/02-eep147-example.ipynb
    type: textbook
    order: 3
  - file: 02-supply/03-sympy.md
    title: 03-sympy
    source_path: content/02-supply/03-sympy.ipynb
    type: textbook
    order: 4
  - file: 02-supply/04-market-equilibria.md
    title: 04-market-equilibria
    source_path: content/02-supply/04-market-equilibria.ipynb
    type: textbook
    order: 5
  - file: 02-supply/.ipynb_checkpoints/index-checkpoint.md
    title: index-checkpoint
    source_path: content/02-supply/.ipynb_checkpoints/index-checkpoint.md
    type: textbook
    order: 6
- chapter: 3
  count: 6
  files:
  - file: 03-public/index.md
    title: index
    source_path: content/03-public/index.md
    type: textbook
    order: 1
  - file: 03-public/govt-intervention.md
    title: govt-intervention
    source_path: content/03-public/govt-intervention.ipynb
    type: textbook
    order: 2
  - file: 03-public/.ipynb_checkpoints/index-checkpoint.md
    title: index-checkpoint
    source_path: content/03-public/.ipynb_checkpoints/index-checkpoint.md
    type: textbook
    order: 3
  - file: 03-public/surplus.md
    title: surplus
    source_path: content/03-public/surplus.ipynb
    type: textbook
    order: 4
  - file: 03-public/.ipynb_checkpoints/surplus-checkpoint.md
    title: surplus-checkpoint
    source_path: content/03-public/.ipynb_checkpoints/surplus-checkpoint.ipynb
    type: textbook
    order: 5
  - file: 03-public/taxes-subsidies.md
    title: taxes-subsidies
    source_path: content/03-public/taxes-subsidies.ipynb
    type: textbook
    order: 6
- chapter: 4
  count: 3
  files:
  - file: 04-production/index.md
    title: index
    source_path: content/04-production/index.md
    type: textbook
    order: 1
  - file: 04-production/production.md
    title: production
    source_path: content/04-production/production.ipynb
    type: textbook
    order: 2
  - file: 04-production/shifts.md
    title: shifts
    source_path: content/04-production/shifts.ipynb
    type: textbook
    order: 3
- chapter: 5
  count: 3
  files:
  - file: 05-utility/index.md
    title: index
    source_path: content/05-utility/index.md
    type: textbook
    order: 1
  - file: 05-utility/budget-constraints.md
    title: budget-constraints
    source_path: content/05-utility/budget-constraints.ipynb
    type: textbook
    order: 2
  - file: 05-utility/utility.md
    title: utility
    source_path: content/05-utility/utility.ipynb
    type: textbook
    order: 3
- chapter: 6
  count: 5
  files:
  - file: 06-inequality/index.md
    title: index
    source_path: content/06-inequality/index.md
    type: textbook
    order: 1
  - file: 06-inequality/historical-inequality.md
    title: historical-inequality
    source_path: content/06-inequality/historical-inequality.ipynb
    type: textbook
    order: 2
  - file: 06-inequality/.ipynb_checkpoints/historical-inequality-checkpoint.md
    title: historical-inequality-checkpoint
    source_path: content/06-inequality/.ipynb_checkpoints/historical-inequality-checkpoint.ipynb
    type: textbook
    order: 3
  - file: 06-inequality/inequality.md
    title: inequality
    source_path: content/06-inequality/inequality.ipynb
    type: textbook
    order: 4
  - file: 06-inequality/.ipynb_checkpoints/inequality-checkpoint.md
    title: inequality-checkpoint
    source_path: content/06-inequality/.ipynb_checkpoints/inequality-checkpoint.ipynb
    type: textbook
    order: 5
- chapter: 7
  count: 10
  files:
  - file: 07-game-theory/index.md
    title: index
    source_path: content/07-game-theory/index.md
    type: textbook
    order: 1
  - file: 07-game-theory/bertrand.md
    title: bertrand
    source_path: content/07-game-theory/bertrand.ipynb
    type: textbook
    order: 2
  - file: 07-game-theory/cournot.md
    title: cournot
    source_path: content/07-game-theory/cournot.ipynb
    type: textbook
    order: 3
  - file: 07-game-theory/.ipynb_checkpoints/cournot-checkpoint.md
    title: cournot-checkpoint
    source_path: content/07-game-theory/.ipynb_checkpoints/cournot-checkpoint.ipynb
    type: textbook
    order: 4
  - file: 07-game-theory/equilibria-oligopolies.md
    title: equilibria-oligopolies
    source_path: content/07-game-theory/equilibria-oligopolies.ipynb
    type: textbook
    order: 5
  - file: 07-game-theory/.ipynb_checkpoints/equilibria-oligopolies-checkpoint.md
    title: equilibria-oligopolies-checkpoint
    source_path: content/07-game-theory/.ipynb_checkpoints/equilibria-oligopolies-checkpoint.ipynb
    type: textbook
    order: 6
  - file: 07-game-theory/expected-utility.md
    title: expected-utility
    source_path: content/07-game-theory/expected-utility.ipynb
    type: textbook
    order: 7
  - file: 07-game-theory/.ipynb_checkpoints/expected-utility-checkpoint.md
    title: expected-utility-checkpoint
    source_path: content/07-game-theory/.ipynb_checkpoints/expected-utility-checkpoint.ipynb
    type: textbook
    order: 8
  - file: 07-game-theory/.ipynb_checkpoints/index-checkpoint.md
    title: index-checkpoint
    source_path: content/07-game-theory/.ipynb_checkpoints/index-checkpoint.md
    type: textbook
    order: 9
  - file: 07-game-theory/python-classes.md
    title: python-classes
    source_path: content/07-game-theory/python-classes.ipynb
    type: textbook
    order: 10
- chapter: 8
  count: 2
  files:
  - file: 08-development/index.md
    title: index
    source_path: content/08-development/index.md
    type: textbook
    order: 1
  - file: 08-development/.ipynb_checkpoints/index-checkpoint.md
    title: index-checkpoint
    source_path: content/08-development/.ipynb_checkpoints/index-checkpoint.md
    type: textbook
    order: 2
- chapter: 9
  count: 13
  files:
  - file: 09-macro/index.md
    title: index
    source_path: content/09-macro/index.md
    type: textbook
    order: 1
  - file: 09-macro/CentralBanks.md
    title: CentralBanks
    source_path: content/09-macro/CentralBanks.ipynb
    type: textbook
    order: 2
  - file: 09-macro/.ipynb_checkpoints/CentralBanks-checkpoint.md
    title: CentralBanks-checkpoint
    source_path: content/09-macro/.ipynb_checkpoints/CentralBanks-checkpoint.ipynb
    type: textbook
    order: 3
  - file: 09-macro/fiscal_policy.md
    title: fiscal_policy
    source_path: content/09-macro/fiscal_policy.ipynb
    type: textbook
    order: 4
  - file: 09-macro/.ipynb_checkpoints/fiscal_policy-checkpoint.md
    title: fiscal_policy-checkpoint
    source_path: content/09-macro/.ipynb_checkpoints/fiscal_policy-checkpoint.ipynb
    type: textbook
    order: 5
  - file: 09-macro/.ipynb_checkpoints/index-checkpoint.md
    title: index-checkpoint
    source_path: content/09-macro/.ipynb_checkpoints/index-checkpoint.md
    type: textbook
    order: 6
  - file: 09-macro/Indicators.md
    title: Indicators
    source_path: content/09-macro/Indicators.ipynb
    type: textbook
    order: 7
  - file: 09-macro/.ipynb_checkpoints/Indicators-checkpoint.md
    title: Indicators-checkpoint
    source_path: content/09-macro/.ipynb_checkpoints/Indicators-checkpoint.ipynb
    type: textbook
    order: 8
  - file: 09-macro/is_curve.md
    title: is_curve
    source_path: content/09-macro/is_curve.ipynb
    type: textbook
    order: 9
  - file: 09-macro/.ipynb_checkpoints/is_curve-checkpoint.md
    title: is_curve-checkpoint
    source_path: content/09-macro/.ipynb_checkpoints/is_curve-checkpoint.ipynb
    type: textbook
    order: 10
  - file: 09-macro/phillips_curve.md
    title: phillips_curve
    source_path: content/09-macro/phillips_curve.ipynb
    type: textbook
    order: 11
  - file: 09-macro/.ipynb_checkpoints/phillips_curve-checkpoint.md
    title: phillips_curve-checkpoint
    source_path: content/09-macro/.ipynb_checkpoints/phillips_curve-checkpoint.ipynb
    type: textbook
    order: 12
  - file: 09-macro/.ipynb_checkpoints/visualizations-checkpoint.md
    title: visualizations-checkpoint
    source_path: content/09-macro/.ipynb_checkpoints/visualizations-checkpoint.ipynb
    type: textbook
    order: 13
- chapter: 10
  count: 4
  files:
  - file: 10-finance/index.md
    title: index
    source_path: content/10-finance/index.md
    type: textbook
    order: 1
  - file: 10-finance/options.md
    title: options
    source_path: content/10-finance/options.ipynb
    type: textbook
    order: 2
  - file: 10-finance/.ipynb_checkpoints/options-checkpoint.md
    title: options-checkpoint
    source_path: content/10-finance/.ipynb_checkpoints/options-checkpoint.ipynb
    type: textbook
    order: 3
  - file: 10-finance/value-interest.md
    title: value-interest
    source_path: content/10-finance/value-interest.ipynb
    type: textbook
    order: 4
- chapter: 11
  count: 9
  files:
  - file: 11-econometrics/index.md
    title: index
    source_path: content/11-econometrics/index.md
    type: textbook
    order: 1
  - file: 11-econometrics/multivariable.md
    title: multivariable
    source_path: content/11-econometrics/multivariable.ipynb
    type: textbook
    order: 2
  - file: 11-econometrics/.ipynb_checkpoints/multivariable-checkpoint.md
    title: multivariable-checkpoint
    source_path: content/11-econometrics/.ipynb_checkpoints/multivariable-checkpoint.ipynb
    type: textbook
    order: 3
  - file: 11-econometrics/reading-econ-papers.md
    title: reading-econ-papers
    source_path: content/11-econometrics/reading-econ-papers.ipynb
    type: textbook
    order: 4
  - file: 11-econometrics/.ipynb_checkpoints/reading-econ-papers-checkpoint.md
    title: reading-econ-papers-checkpoint
    source_path: content/11-econometrics/.ipynb_checkpoints/reading-econ-papers-checkpoint.ipynb
    type: textbook
    order: 5
  - file: 11-econometrics/single-variable.md
    title: single-variable
    source_path: content/11-econometrics/single-variable.ipynb
    type: textbook
    order: 6
  - file: 11-econometrics/.ipynb_checkpoints/single-variable-checkpoint.md
    title: single-variable-checkpoint
    source_path: content/11-econometrics/.ipynb_checkpoints/single-variable-checkpoint.ipynb
    type: textbook
    order: 7
  - file: 11-econometrics/statsmodels.md
    title: statsmodels
    source_path: content/11-econometrics/statsmodels.ipynb
    type: textbook
    order: 8
  - file: 11-econometrics/.ipynb_checkpoints/statsmodels-checkpoint.md
    title: statsmodels-checkpoint
    source_path: content/11-econometrics/.ipynb_checkpoints/statsmodels-checkpoint.ipynb
    type: textbook
    order: 9
- chapter: 12
  count: 7
  files:
  - file: 12-environmental/index.md
    title: index
    source_path: content/12-environmental/index.md
    type: textbook
    order: 1
  - file: 12-environmental/.ipynb_checkpoints/index-checkpoint.md
    title: index-checkpoint
    source_path: content/12-environmental/.ipynb_checkpoints/index-checkpoint.md
    type: textbook
    order: 2
  - file: 12-environmental/KuznetsHypothesis.md
    title: KuznetsHypothesis
    source_path: content/12-environmental/KuznetsHypothesis.ipynb
    type: textbook
    order: 3
  - file: 12-environmental/KuznetsHypothesis-Copy1.md
    title: KuznetsHypothesis-Copy1
    source_path: content/12-environmental/KuznetsHypothesis-Copy1.ipynb
    type: textbook
    order: 4
  - file: 12-environmental/.ipynb_checkpoints/KuznetsHypothesis-Copy1-checkpoint.md
    title: KuznetsHypothesis-Copy1-checkpoint
    source_path: content/12-environmental/.ipynb_checkpoints/KuznetsHypothesis-Copy1-checkpoint.ipynb
    type: textbook
    order: 5
  - file: 12-environmental/MAC.md
    title: MAC
    source_path: content/12-environmental/MAC.ipynb
    type: textbook
    order: 6
  - file: 12-environmental/.ipynb_checkpoints/textbook1-checkpoint.md
    title: textbook1-checkpoint
    source_path: content/12-environmental/.ipynb_checkpoints/textbook1-checkpoint.ipynb
    type: textbook
    order: 7




--- START .ipynb_checkpoints/intro-checkpoint.md ---

---
title: intro-checkpoint
type: textbook
source_path: content/.ipynb_checkpoints/intro-checkpoint.md
---

# Data 88E: Economic Models

Economics is in the world around us, and so is Data Science! It's in our every day lives. As we connect Data Science with Economics, we will be exploring real life datasets to illustrate how Economics concepts are shaped and how decisions lead to real-life impacts. This is a textbook developed for UC Berkeley's course Data 88E: Economic Models. [Data 88](https://data-88.github.io) is a generic course listing for Data 8 connector courses.


## Course Description

The idea for the class is to take students through a series of exercises to motivate and illustrate key concepts in Economics with examples in Python Jupyter notebooks. The class will cover concepts from Introductory Economics, MIcroeconomic Theory, Econometrics, Development Economics, Environmental Economics and Public Economics. The course will give data science students a pathway to apply python programming and data science concepts within the discipline of economics. The course will also give economics students a pathway to apply programming to reinforce fundamental concepts and to advance the level of study in upper division coursework and possible thesis work.

## Acknowledgements

**Course Instructor:** Eric Van Dusen

**Textbook Developer:** [Christopher Pyles](https://chrispyles.io)

**Content Developers:** [Alan Liang](http://alanliang.me/), Amal Bhatnagar, Andrei Caprau, [Christopher Pyles](https://chrispyles.io), Eric Van Dusen, Matthew Yep, Rohan Jha, Shashank Dalmia, Umar Maniku

## License

This textbook is licensed under a [BSD 3-Clause License](https://github.com/ds-connectors/econ-models-textbook/blob/master/LICENSE).


--- END .ipynb_checkpoints/intro-checkpoint.md ---



--- START 00-intro/.ipynb_checkpoints/index-checkpoint.md ---

---
title: index-checkpoint
type: textbook
source_path: content/00-intro/.ipynb_checkpoints/index-checkpoint.ipynb
chapter: 0
---

# Introduction

## The Financial Benefits of Your Major

```python
import ipywidgets as widgets
from datascience import *
import numpy as np
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
import pandas as pd
%matplotlib inline 
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
```

```python
import plotly.graph_objects as go
from plotly.offline import plot
from IPython.display import display, HTML
```

Welcome to Data 88E: *Economics Models*! This class will explore the intersection of Data Science and Economics. 
Specifically, we will utilize methods and techniques in data science to examine both basic and upper-division Economics concepts.
Throughout the course, we will consider a variety of economic problems both on the macro and micro level. 

In the first demo of the course, we hope to give you a sense of the types problems you can expect to explore this semester by considering a problem that may be of personal relevance to you: the post-graduate incomes of different majors at Cal.

We will be using various visualization techniques to analyze the median incomes of different majors at UC Berkeley, in addition to the median incomes of those same majors at other colleges.
If you forgot, the median income is the "middle" value: if you sorted all the individual incomes of a major in ascending order, the median would be the value that's exactly in the middle. The median is also called the 50th percentile -- at the median, exactly 50% of the individuals have an income lower than the median.

Do not be concerned if you don't understand the code below: this entire exercise is purely a demo to motivate many profound concepts in Economics. 
If you're interested, you may choose to come back to this demo at the end of the course and consider all the different techniques utilized in it - it'd be a great way of reflecting upon how much you've learnt!

## Data Collection

Before we can use data science to tackle any issue, we must–well–obtain data (kind of mind-boggling, I know).
Recall that we want to examine the median incomes of different majors at UC Berkeley as well as the median incomes of those same majors at other colleges.
The term 'other colleges' is a fairly general one, and in this case we shall consider the average median incomes of those majors at alll other colleges in the United States.

In order to obtain a dataset, you can either collect it yourself (via surveys, questionnaires, etc.) or you can use datasets that others have gathered for you.
In this demo, we are combining 3 different datasets:
- The median income for each major at Cal was obtained from [Cal's 2019 First Destination survey](https://career.berkeley.edu/survey/survey).
- The median income for each major overall was obtained from surveys conducted by the American Community Survey (ACS) from 2010 to 2012, a very popular data source for Economics Research! In the survey, ACS essentially calls college graduates and asked them their income as well as what they majored in at college. (As a side note, FiveThirtyEight later published this [article](https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/) using the results of the survey.) In this project, we will be using a modified version of the ACS survey - we will only be looking at the respondents who are 28 or younger. Can you think of why we would do this?
- The longitudinal data on long-run outcomes of UC Berkeley alumni was obtained from the [University of California webpage](https://www.universityofcalifornia.edu/infocenter/berkeley-outcomes). We will use this dataset later for a slightly different analysis.

Take a moment to consider the ways in which the 3 different datasets were created. 
Is it fair to draw direct comparisons between the datasets? What would be some potential issues and how could the differences in our datasets affect our analysis?


### Mean vs Median
Before proceeding further, it is important to consider why we are choosing to look at the median, and not the average, income.
In order to answer this question, let us think about what the *distribution* of incomes for a population would look like. 
Most likely, you would see a high amount of incomes around or slightly below the mean, with a few massive outlier incomes above the mean.
For example, consider a theatre major who becomes a star on Broadway - while they'd be doing absolutely fantastic in their career, they are not representative of the average theatre graduate from Berkeley and would likely pull the average income way up.
For this reason, using the median is more *robust*: it gives us a better idea of what the typical graduate for any major can generally expect to earn.

Now we'll load in all the data.
Take a look at the tables for each dataset.
Note that `P25th` referes to the 25th percentile of incomes (the income level at which exactly 25% of incomes are lower) and `P75th` refers to the 75th percentile of incomes (the income level at which exactly 75% of incomes are lower).
You may not know what all the different columns in the tables mean. That's okay! 
As data scientists, we often encounter a lot of irrelevant data that we will discard later.

```python
# Load in table of all majors' median incomes at Cal
cal_income = Table.read_table("cal_income.csv")
cal_income.show(10)
```

```python
# Load in table of all other universities' average major median incomes
other_income = Table.read_table("recent-grads.csv") 
other_income.show(10)
```

To make direct comparisons across majors, we combined all the tables above into a single one for us to use below.

```python
majors = Table.read_table("cal_vs_all.csv")
majors.show(10)
```

Our combined table above dropped the columns in above tables that we didn't need to conduct our exploration. 
It has a column `Median Income Difference`: this column is the Berkeley median income minus the overall median income for each major. 
It gives us a sense of the value of Cal over the average university: the difference is the additional income we recieve from obtaining a Cal degree.

Before moving forward, take a second to consider how well the above tables would match with each other.
For example, Electrical Engineering and Computer Science (EECS) is a popular major at Berkeley. However, the `majors` dataset didn't have a direct equivalent for it.
Instead, the `majors` dataset had Electrical Engineering, Electrical Engineering Technologies and Computer Engineering as separate majors. 
Since in theory students in EECS focus more on computer engineering, we chose to use the computer engineering data for drawing comparions in our final, combined table.
However, there's room for ambiguity here and that is another potential flaw in our exploration!

The below graph displays all the median salaries for all the majors in our dataset side by side. Feel free to look at the values for a few seconds - do you find anything interesting?

```python
def display_major_table(major):
    selected = majors.where("Major", are.contained_in(major))
    selected.show()
    
    ind = np.arange(selected.num_rows) 
    width = 0.35

    fig, ax = plt.subplots()
    rects1 = ax.bar(ind - width/2, selected.column("Cal Median"), width,
                    label='Cal Median', color = "navy")
    rects2 = ax.bar(ind + width/2, selected.column("Overall Median"), width,
                    label='Overall Median', color = "goldenrod")
    
    ax.set_ylabel('Income')
    ax.set_xticks(ind)
    ax.set_xticklabels(selected.column("Major"))
    ax.set_title("Median Incomes for Selected Majors at Cal (Blue) and Other Universities (Gold)", fontsize= 13)
    plt.xticks(rotation=90)


dropdown_majors = widgets.SelectMultiple(
    options=majors.column("Major"),
    description="Major",
    disabled=False
)
widgets.interact(display_major_table, major=dropdown_majors)
```

```python
mjs = majors.column("Major")[::-1]
cal = majors.column("Cal Median")[::-1]
ovr = majors.column("Overall Median")[::-1]

fig = go.Figure()
fig.add_trace(go.Bar(name="Cal Median", y=mjs, x=cal, orientation="h"))
fig.add_trace(go.Bar(name="Overall Median", y=mjs, x=ovr, orientation="h"))
fig.update_layout(
    title="Median Incomes for Selected Majors at Cal (Blue) and Other Universities (Red)",
    yaxis=dict(title=dict(text="Major")),
    height=1200,
)
plot(fig, filename="fig0.html", auto_open=False)

display(HTML("fig0.html"))
```

## The most lucrative major

Let's imagine that Belfort is a freshman at Berkeley. 
Belfort is a very unique individual - he is someone who would be equally good at all majors and enjoys all majors equally.
He also believes that money is the most important thing in the world.
These assumptions are vastly oversimplifcations and not the case for anyone in real life; but in Economics you'll find that we end up making significant simplifications to abstract away all the potential complications!

Since Belfort is equally good at and happy with all majors at Cal, the major he chooses is purely dependent on how much money he can earn after college with that major.
Therefore, he will choose the major with the highest median income.

Let us sort our table by the `Cal Median` column in descending order to see which major that would be.

```python
majors.sort('Cal Median',descending=True).show(10)
```

However, what if instead of just wanting the maximum amount of cash after college, Belfort was super proud of getting into Berkeley and instead wanted to maximise the amount of additional benefit he recieves from attending Berkeley over other universities?
Let's sort our table by the `Median Income Difference` column in descending order to see which major would give Belfort the highest additional income from going to Berkeley.  

Do any values in either of our sorted table surprise you? Why might that be?

```python
majors.sort("Median Income Difference",descending=True).show(10)
```

Let's now do the opposite and sort our table by the `Median Income Difference` column in ascending order.

```python
majors.sort("Median Income Difference").show(10)
```

As you can see in the table, the median income difference for both nuclear engineering and astrophysics is negative. 
This is rather peculiar, especially since Cal has great programs for both those majors.
Why would Cal graduates earn less income than the graduates of the average college for those 2 majors? 

*Hint*: consider what each of our datasets describes. Is it an apples-to-apples comparison?

The answer is because the dataset with the median incomes for all colleges actually described recent graduates (people who are 28 and younger, rather than just fresh college graduates.
It is likely that most people who graduate with Nuclear Engineering and Astrophysics degrees in other colleges tend to stay in the industry for their fields and keep accumulating experience. This would explain why a dataset that includes data from 28 year olds would have a higher median income than fresh Cal graduates in the same field.

### Generalization and Causal Inference

While Belfort would be equally good at and enjoy all majors equally, the implications for him may not generalize for anyone else. 

If you decide to major in EECS, it does not mean that you will have more income post-grad than that from majoring in anything else. For example, someone who is a fantastic artist but not very good at computer science would probably make far more money majoring in art than in EECS. In addition, just because EECS has the highest median income overall doesn't mean that any individual is guaranteed a high median income if they decide to major in EECS. Overall, our results are not completely *generalizable* to others.

The causal effect of majoring in EECS on incomes may also be overstated. Consider the fact that those who major in EECS tend to come from families whose parents are already in working in tech. As a result, these students will likely also find better jobs post-grad. This is what we call a confounding variable -- it is positively correlated with our treatment variable of majoring in EECS. If we do not observe this confounding factor, our analysis would likely overstate the effect of an EECS degree on post-grad incomes.

### A brief discourse on utility 

From seeing our results, you may be wondering: *why doesn't everyone just major in EECS then*? 

For Belfort, money is the only thing in the world that derives happiness. 
However, this is once again an oversimplication for the vast majority of people in the real world.
People derive utility from sources other than just their bank account; often, we choose relatively lower-paying jobs in order to derive more happiness in our lives.

**Utility** is a measure of 'satisfaction' or 'happiness' when we consume a good, while **disutility** is a measure of 'unsatisfaction' or 'harm' when we consume a *bad*. In economics, one assumption we will always make is that people will always seek to maximize their utility and minimize their disutility. 

However, we encounter *diminishing marginal utility* as we consume more and more of a good. For example, the utility of the first cookie you eat is likely much higher than the 20th cookie you eat. Similarly, we may encounter *increasing marginal disutility* as we consume more and more of a bad. For example, the disutility of the first hour doing something you dread is probably not as bad as the 10th hour of doing something you dread.

For many of us, money provides a source of utility, while working or the studying required to get there may provide a source of disutility. This presents a tradeoff: for some, the disutility from studying EECS or working as a programmer greatly outweights the utility from the money. Perhaps another job in a different field requires significantly lower disutility to achieve, without much impact on the utility of the income associated with it. This phenomenon would explain why not everyone decides to major in EECS!

## Incomes Over Time: Computer Science vs Economics 

So far our exploration of the data has considered a rather general timeframe of "post graduation", but perhaps it would be more insightful if we dove in and took a look at how incomes differed across various ages. We can do so using the table below that consists of data collected by the [UC ClioMetric History Project](http://uccliometric.org/). 
By compiling a database of digital student transcripts of all UC undergraduates and then linking California wages to individuals in the transcript database, 
they were able to produce comprehensive dashboards that visualize demographics, major choices, and long-run incomes of each UC campus’s alumni over the past 70 years. 


You can learn more about how they collected the data [here](https://www.universityofcalifornia.edu/infocenter/long-run-methodology), but for now we will just focus on a snapshot of the data. 
In particular, we are interested in exploring how percentile incomes compare between Cal alumni who majored in computer science versus those who majored in economics. Run the following cell to check it out.

```python
cs_vs_econ=Table.read_table("cs_econ_by_age.csv")
cs_vs_econ.show(10)
```

Here we have a table where each row corresponds to a certain age. 
It begins with age 23 (typically how old people are when getting their first job out of college), and goes up until age 62 (when people retire from the workforce). 

Let's graph our data using some line plots: line plots are especially useful when trying to visualize trends that change over time. 

Run the following cell to visualize the data. The yellow lines correspond to CS incomes, while the blue lines correspond to Econ incomes.

```python
df = cs_vs_econ.to_df()
df.plot("Age",color=["goldenrod","gold","yellow","khaki","navy","mediumblue","royalblue","cornflowerblue"])
plt.ylabel("Annual Income in Dollars")
plt.title("Comparing Incomes of Computer Science Majors vs Economics Majors");
fig = plt.gcf()
fig.set_size_inches(15, 11)
```

Let's take a look at how the incomes compare at each percentile. The most notable trend is probably the fact that for the most the part, CS majors typically make more money than Econ majors.
We especially observe this trend in the 25th and 50th percentiles, and this follows accordingly with our exploration earlier in the notebook that students who majored in L&S Computer Science on average had a higher median income than pretty much every other major.

The graph above helps explain why CS is one of the most popular majors here at Berkeley. 
Software engineering and related disciplines have a huge demand for fresh talent, so that individuals entering these fields are duly compensated. This in and of itself is a big appeal of students wanting to major in computer science. 

Right off the bat out of college, fresh CS graduates are looking at a six figure income: even the 25th percentile of CS majors are making over $103,000 by the time they are 25. At any given percentile level, CS graduates outearn their Economics counterparts immediately post-graduation.

### Intertemporal Effects

However, we don't just choose our majors based off of the immediate post-graduation income. Intuitively, we would want to maximize our total lifetime income we make. This idea is called *intertermporal utility maximization*: Economists often take into account one's utility across all possible time periods.

Considering lifetime income, which major would be better? Well, it depends: if you are at the bottom 25th or 50th percentile, CS is better. This suggests that the typical Economics major will likely never outearn the the typical CS major.

However, we observe that Economics majors at higher percentiles ultimately overtake CS majors in earnings. Intuitively, this should make sense. Perhaps top Economics majors are more likely to become successful executives later on in their careers while make more money than top CS majors, who continue to stay as software engineers or become engineering managers.

### Risk
We observe that Economics majors have a much larger spread in their lifetime earnings than CS majors: top Economics graduates make significantly more than bottom Economics graduates, compared to top CS and bottom CS graduates.

This brings to the light the notion of risk. Majoring in CS carries relatively lower variance: whether you are 
a top notch developer in the 75th percentile or a subpar developer in the 25th percentile, you will probably make a solid living. Since most CS majors typically end up pursuing similar careers in the realms of software development, there is less **variance** in the occupations. There is also less **risk**: most CS majors can expect to land a software engineering job and continue working as one twenty years into their career.

The same story is less true for Economics majors: a top-notch Economics major will significantly out-earn a subpar Economics major. This is partly due to the vastly differing occupation types Economics majors become: analysts, bankers, consultants, just to name a few (Economics is a very versatile degree, afterall). Twenty years down the line, the top Economics majors will have higher ceilings perhaps with a lot of opportunity for leadership or executive positions. There is a lot more **variance** in the types of jobs and thus salaries that Economics majors will attain. In addition, there is a lot more **risk**: not every Economics major will become a top executive, and most end up with more 'average' jobs that earn less than that of similar percentile CS majors.

This means that if you know you are at the top regardless of your major, you will tend to ultimately earn more income as an Economics major than a CS major. But a caveat: it's impossible to guarantee that you will be in the top, no matter how hard you work. Often, life has uncertainties and involves a great deal of luck to become successful.

The table and graph above only considered the percentile incomes of CS and Economics majors, but if you're interested in drawing more comparisons, check out [this page](https://www.universityofcalifornia.edu/infocenter/berkeley-outcomes). You can compare between all types of popular majors at UC Berkeley, and even toggle around with more features like gender, ethnicity, or even specific courses.

## An Afterword

In this demo, we've covered a series of fundamental Economics ideas such as income, utility, and risk. We've also gone over a series of more statistical concepts that are also at the heart of empirical Economics such as causal inference, generalization, and unobserved variables. We did this all using data science techniques and visualizations, while being cognizant of potential sources of error. We will be revisiting a lot of these themes in later parts of the course.

From this exercise you may have noticed that pursuing either Economics, Data Science, or both are great options in terms of post-graduation incomes. This means you've probably chosen wisely to have ended up in this class. 

Welcome to Data 88E!



--- END 00-intro/.ipynb_checkpoints/index-checkpoint.md ---



--- START 00-intro/index.md ---

---
title: index
type: textbook
source_path: content/00-intro/index.ipynb
chapter: 0
---

# Introduction

## The Financial Benefits of Your Major

```python
import ipywidgets as widgets
from datascience import *
import numpy as np
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display
import pandas as pd
%matplotlib inline 
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
```

```python
import plotly.graph_objects as go
from plotly.offline import plot
from IPython.display import display, HTML
```

Welcome to Data 88E: *Economics Models*! This class will explore the intersection of Data Science and Economics. 
Specifically, we will utilize methods and techniques in data science to examine both basic and upper-division Economics concepts.
Throughout the course, we will consider a variety of economic problems both on the macro and micro level. 

In the first demo of the course, we hope to give you a sense of the types problems you can expect to explore this semester by considering a problem that may be of personal relevance to you: the post-graduate incomes of different majors at Cal.

We will be using various visualization techniques to analyze the median incomes of different majors at UC Berkeley, in addition to the median incomes of those same majors at other colleges.
If you forgot, the median income is the "middle" value: if you sorted all the individual incomes of a major in ascending order, the median would be the value that's exactly in the middle. The median is also called the 50th percentile -- at the median, exactly 50% of the individuals have an income lower than the median.

Do not be concerned if you don't understand the code below: this entire exercise is purely a demo to motivate many profound concepts in Economics. 
If you're interested, you may choose to come back to this demo at the end of the course and consider all the different techniques utilized in it - it'd be a great way of reflecting upon how much you've learnt!

## Data Collection

Before we can use data science to tackle any issue, we must–well–obtain data (kind of mind-boggling, I know).
Recall that we want to examine the median incomes of different majors at UC Berkeley as well as the median incomes of those same majors at other colleges.
The term 'other colleges' is a fairly general one, and in this case we shall consider the average median incomes of those majors at alll other colleges in the United States.

In order to obtain a dataset, you can either collect it yourself (via surveys, questionnaires, etc.) or you can use datasets that others have gathered for you.
In this demo, we are combining 3 different datasets:
- The median income for each major at Cal was obtained from [Cal's 2024 First Destination survey](https://calviz.berkeley.edu/t/SAPublic/views/FirstDestinations2024-2/Employment).
- The median income for each major overall was obtained from surveys conducted by the American Community Survey (ACS) from 2010 to 2012, a very popular data source for Economics Research! In the survey, ACS essentially calls college graduates and asked them their income as well as what they majored in at college. (As a side note, FiveThirtyEight later published this article:[The Economic Guide to Picking a College Major](https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/) using the results of the survey.) In this project, we will be using a modified version of the ACS survey - we will only be looking at the respondents who are 28 or younger. Can you think of why we would do this?
- The longitudinal data on long-run outcomes of UC Berkeley alumni was obtained from the [University of California webpage](https://www.universityofcalifornia.edu/infocenter/berkeley-outcomes). We will use this dataset later for a slightly different analysis.

Take a moment to consider the ways in which the 3 different datasets were created. 
Is it fair to draw direct comparisons between the datasets? What would be some potential issues and how could the differences in our datasets affect our analysis?


### Mean vs Median
Before proceeding further, it is important to consider why we are choosing to look at the median, and not the average, income.
In order to answer this question, let us think about what the *distribution* of incomes for a population would look like. 
Most likely, you would see a high amount of incomes around or slightly below the mean, with a few massive outlier incomes above the mean.
For example, consider a theatre major who becomes a star on Broadway - while they'd be doing absolutely fantastic in their career, they are not representative of the average theatre graduate from Berkeley and would likely pull the average income way up.
For this reason, using the median is more *robust*: it gives us a better idea of what the typical graduate for any major can generally expect to earn.

Now we'll load in all the data.
Take a look at the tables for each dataset.
Note that `P25th` referes to the 25th percentile of incomes (the income level at which exactly 25% of incomes are lower) and `P75th` refers to the 75th percentile of incomes (the income level at which exactly 75% of incomes are lower).
You may not know what all the different columns in the tables mean. That's okay! 
As data scientists, we often encounter a lot of irrelevant data that we will discard later.

```python
# Load in table of all majors' median incomes at Cal
cal_income = Table.read_table("cal_income.csv")
cal_income.show(10)
```

```python
# Load in table of all other universities' average major median incomes
other_income = Table.read_table("recent-grads.csv") 
other_income.show(10)
```

To make direct comparisons across majors, we combined all the tables above into a single one for us to use below.

```python
majors = Table.read_table("cal_vs_all.csv")
majors.show(10)
```

Our combined table above dropped the columns in above tables that we didn't need to conduct our exploration. 
It has a column `Median Income Difference`: this column is the Berkeley median income minus the overall median income for each major. 
It gives us a sense of the value of Cal over the average university: the difference is the additional income we recieve from obtaining a Cal degree.

Before moving forward, take a second to consider how well the above tables would match with each other.
For example, Electrical Engineering and Computer Science (EECS) is a popular major at Berkeley. However, the `majors` dataset didn't have a direct equivalent for it.
Instead, the `majors` dataset had Electrical Engineering, Electrical Engineering Technologies and Computer Engineering as separate majors. 
Since in theory students in EECS focus more on computer engineering, we chose to use the computer engineering data for drawing comparions in our final, combined table.
However, there's room for ambiguity here and that is another potential flaw in our exploration!

The below graph displays all the median salaries for all the majors in our dataset side by side. Feel free to look at the values for a few seconds - do you find anything interesting?
[Following image is a bar chart of Median Incomes for Selected Majors]

```python
def display_major_table(major):
    selected = majors.where("Major", are.contained_in(major))
    selected.show()
    
    ind = np.arange(selected.num_rows) 
    width = 0.35

    fig, ax = plt.subplots()
    rects1 = ax.bar(ind - width/2, selected.column("Cal Median"), width,
                    label='Cal Median', color = "navy")
    rects2 = ax.bar(ind + width/2, selected.column("Overall Median"), width,
                    label='Overall Median', color = "goldenrod")
    
    ax.set_ylabel('Income')
    ax.set_xticks(ind)
    ax.set_xticklabels(selected.column("Major"))
    ax.set_title("Median Incomes for Selected Majors at Cal (Blue) and Other Universities (Gold)", fontsize= 13)
    plt.xticks(rotation=90)


dropdown_majors = widgets.SelectMultiple(
    options=majors.column("Major"),
    description="Major",
    disabled=False
)
widgets.interact(display_major_table, major=dropdown_majors)
```

```python
mjs = majors.column("Major")[::-1]
cal = majors.column("Cal Median")[::-1]
ovr = majors.column("Overall Median")[::-1]

fig = go.Figure()
fig.add_trace(go.Bar(name="Cal Median", y=mjs, x=cal, orientation="h"))
fig.add_trace(go.Bar(name="Overall Median", y=mjs, x=ovr, orientation="h"))
fig.update_layout(
    title="Median Incomes for Selected Majors at Cal (Blue) and Other Universities (Red)",
    yaxis=dict(title=dict(text="Major")),
    height=1200,
)
plot(fig, filename="fig0.html", auto_open=False)

display(HTML("fig0.html"))
```

## The most lucrative major

Let's imagine that Belfort is a freshman at Berkeley. 
Belfort is a very unique individual - he is someone who would be equally good at all majors and enjoys all majors equally.
He also believes that money is the most important thing in the world.
These assumptions are vastly oversimplifcations and not the case for anyone in real life; but in Economics you'll find that we end up making significant simplifications to abstract away all the potential complications!

Since Belfort is equally good at and happy with all majors at Cal, the major he chooses is purely dependent on how much money he can earn after college with that major.
Therefore, he will choose the major with the highest median income.

Let us sort our table by the `Cal Median` column in descending order to see which major that would be.

```python
majors.sort('Cal Median',descending=True).show(10)
```

However, what if instead of just wanting the maximum amount of cash after college, Belfort was super proud of getting into Berkeley and instead wanted to maximise the amount of additional benefit he recieves from attending Berkeley over other universities?
Let's sort our table by the `Median Income Difference` column in descending order to see which major would give Belfort the highest additional income from going to Berkeley.  

Do any values in either of our sorted table surprise you? Why might that be?

```python
majors.sort("Median Income Difference",descending=True).show(10)
```

Let's now do the opposite and sort our table by the `Median Income Difference` column in ascending order.

```python
majors.sort("Median Income Difference").show(10)
```

As you can see in the table, the median income difference for both nuclear engineering and astrophysics is negative. 
This is rather peculiar, especially since Cal has great programs for both those majors.
Why would Cal graduates earn less income than the graduates of the average college for those 2 majors? 

*Hint*: consider what each of our datasets describes. Is it an apples-to-apples comparison?

The answer is because the dataset with the median incomes for all colleges actually described recent graduates (people who are 28 and younger, rather than just fresh college graduates.
It is likely that most people who graduate with Nuclear Engineering and Astrophysics degrees in other colleges tend to stay in the industry for their fields and keep accumulating experience. This would explain why a dataset that includes data from 28 year olds would have a higher median income than fresh Cal graduates in the same field.

### Generalization and Causal Inference

While Belfort would be equally good at and enjoy all majors equally, the implications for him may not generalize for anyone else. 

If you decide to major in EECS, it does not mean that you will have more income post-grad than that from majoring in anything else. For example, someone who is a fantastic artist but not very good at computer science would probably make far more money majoring in art than in EECS. In addition, just because EECS has the highest median income overall doesn't mean that any individual is guaranteed a high median income if they decide to major in EECS. Overall, our results are not completely *generalizable* to others.

The causal effect of majoring in EECS on incomes may also be overstated. Consider the fact that those who major in EECS tend to come from families whose parents are already in working in tech. As a result, these students will likely also find better jobs post-grad. This is what we call a confounding variable -- it is positively correlated with our treatment variable of majoring in EECS. If we do not observe this confounding factor, our analysis would likely overstate the effect of an EECS degree on post-grad incomes.

### A brief discourse on utility 

From seeing our results, you may be wondering: *why doesn't everyone just major in EECS then*? 

For Belfort, money is the only thing in the world that derives happiness. 
However, this is once again an oversimplication for the vast majority of people in the real world.
People derive utility from sources other than just their bank account; often, we choose relatively lower-paying jobs in order to derive more happiness in our lives.

**Utility** is a measure of 'satisfaction' or 'happiness' when we consume a good, while **disutility** is a measure of 'unsatisfaction' or 'harm' when we consume a *bad*. In economics, one assumption we will always make is that people will always seek to maximize their utility and minimize their disutility. 

However, we encounter *diminishing marginal utility* as we consume more and more of a good. For example, the utility of the first cookie you eat is likely much higher than the 20th cookie you eat. Similarly, we may encounter *increasing marginal disutility* as we consume more and more of a bad. For example, the disutility of the first hour doing something you dread is probably not as bad as the 10th hour of doing something you dread.

For many of us, money provides a source of utility, while working or the studying required to get there may provide a source of disutility. This presents a tradeoff: for some, the disutility from studying EECS or working as a programmer greatly outweights the utility from the money. Perhaps another job in a different field requires significantly lower disutility to achieve, without much impact on the utility of the income associated with it. This phenomenon would explain why not everyone decides to major in EECS!

## Incomes Over Time: Computer Science vs Economics 

So far our exploration of the data has considered a rather general timeframe of "post graduation", but perhaps it would be more insightful if we dove in and took a look at how incomes differed across various ages. We can do so using the table below that consists of data collected by the [UC ClioMetric History Project](http://uccliometric.org/). 
By compiling a database of digital student transcripts of all UC undergraduates and then linking California wages to individuals in the transcript database, 
they were able to produce comprehensive dashboards that visualize demographics, major choices, and long-run incomes of each UC campus’s alumni over the past 70 years. 


You can learn more about how they collected the data in [Incomes Over Time ](https://www.universityofcalifornia.edu/infocenter/long-run-methodology), but for now we will just focus on a snapshot of the data. 
In particular, we are interested in exploring how percentile incomes compare between Cal alumni who majored in computer science versus those who majored in economics. Run the following cell to check it out.

```python
cs_vs_econ=Table.read_table("cs_econ_by_age.csv")
cs_vs_econ.show(10)
```

Here we have a table where each row corresponds to a certain age. 
It begins with age 23 (typically how old people are when getting their first job out of college), and goes up until age 62 (when people retire from the workforce). 

Let's graph our data using some line plots: line plots are especially useful when trying to visualize trends that change over time. 

Run the following cell to visualize the data. The yellow lines correspond to CS incomes, while the blue lines correspond to Econ incomes. 

[Following image is a graph of "Comparing Incomes of Computer Science Majors vs Economics Majors"]

```python
from labellines import labelLines
df = cs_vs_econ.to_df()
df.plot("Age",color=["#8A3B00","#E03B00","#DD31E6","#BB689C","navy","mediumblue","royalblue","#5A83A4"])
plt.ylabel("Annual Income in Dollars")
plt.title("Comparing Incomes of Computer Science Majors vs Economics Majors");

#add labels
# plt.text(62.5, 65000, 'Econ 25th Percentile')
# plt.text(62.5, 100000, 'CS 25th Percentile')
# plt.text(62.5, 120000, 'Econ Median')

xvals = [55, 55, 55, 55, 55, 55, 55, 55]
lines = plt.gca().get_lines()
labelLines(lines, align=True, xvals=xvals)

fig = plt.gcf()
fig.set_size_inches(15, 11)
```

Let's take a look at how the incomes compare at each percentile. The most notable trend is probably the fact that for the most the part, CS majors typically make more money than Econ majors.
We especially observe this trend in the 25th and 50th percentiles, and this follows accordingly with our exploration earlier in the notebook that students who majored in L&S Computer Science on average had a higher median income than pretty much every other major.

The graph above helps explain why CS is one of the most popular majors here at Berkeley. 
Software engineering and related disciplines have a huge demand for fresh talent, so that individuals entering these fields are duly compensated. This in and of itself is a big appeal of students wanting to major in computer science. 

Right off the bat out of college, fresh CS graduates are looking at a six figure income: even the 25th percentile of CS majors are making over $103,000 by the time they are 25. At any given percentile level, CS graduates outearn their Economics counterparts immediately post-graduation.

### Intertemporal Effects

However, we don't just choose our majors based off of the immediate post-graduation income. Intuitively, we would want to maximize our total lifetime income we make. This idea is called *intertermporal utility maximization*: Economists often take into account one's utility across all possible time periods.

Considering lifetime income, which major would be better? Well, it depends: if you are at the bottom 25th or 50th percentile, CS is better. This suggests that the typical Economics major will likely never outearn the the typical CS major.

However, we observe that Economics majors at higher percentiles ultimately overtake CS majors in earnings. Intuitively, this should make sense. Perhaps top Economics majors are more likely to become successful executives later on in their careers while make more money than top CS majors, who continue to stay as software engineers or become engineering managers.

### Risk
We observe that Economics majors have a much larger spread in their lifetime earnings than CS majors: top Economics graduates make significantly more than bottom Economics graduates, compared to top CS and bottom CS graduates.

This brings to the light the notion of risk. Majoring in CS carries relatively lower variance: whether you are 
a top notch developer in the 75th percentile or a subpar developer in the 25th percentile, you will probably make a solid living. Since most CS majors typically end up pursuing similar careers in the realms of software development, there is less **variance** in the occupations. There is also less **risk**: most CS majors can expect to land a software engineering job and continue working as one twenty years into their career.

The same story is less true for Economics majors: a top-notch Economics major will significantly out-earn a subpar Economics major. This is partly due to the vastly differing occupation types Economics majors become: analysts, bankers, consultants, just to name a few (Economics is a very versatile degree, afterall). Twenty years down the line, the top Economics majors will have higher ceilings perhaps with a lot of opportunity for leadership or executive positions. There is a lot more **variance** in the types of jobs and thus salaries that Economics majors will attain. In addition, there is a lot more **risk**: not every Economics major will become a top executive, and most end up with more 'average' jobs that earn less than that of similar percentile CS majors.

This means that if you know you are at the top regardless of your major, you will tend to ultimately earn more income as an Economics major than a CS major. But a caveat: it's impossible to guarantee that you will be in the top, no matter how hard you work. Often, life has uncertainties and involves a great deal of luck to become successful.

The table and graph above only considered the percentile incomes of CS and Economics majors, but if you're interested in drawing more comparisons, check out [Long Run outcomes for UC Berkeley Alumni](https://www.universityofcalifornia.edu/infocenter/berkeley-outcomes). You can compare between all types of popular majors at UC Berkeley, and even toggle around with more features like gender, ethnicity, or even specific courses.

## An Afterword

In this demo, we've covered a series of fundamental Economics ideas such as income, utility, and risk. We've also gone over a series of more statistical concepts that are also at the heart of empirical Economics such as causal inference, generalization, and unobserved variables. We did this all using data science techniques and visualizations, while being cognizant of potential sources of error. We will be revisiting a lot of these themes in later parts of the course.

From this exercise you may have noticed that pursuing either Economics, Data Science, or both are great options in terms of post-graduation incomes. This means you've probably chosen wisely to have ended up in this class. 

Welcome to Data 88E!



--- END 00-intro/index.md ---



--- START 01-demand/01-demand.md ---

---
title: 01-demand
type: textbook
source_path: content/01-demand/01-demand.ipynb
chapter: 1
---

```python
from datascience import *

import sympy
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as patches
# plt.style.use('seaborn-muted')
%matplotlib inline

from IPython.display import display
import numpy as np
import pandas as pd
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
import warnings
warnings.filterwarnings('ignore')
```

# Demand Curves

In this chapter, we will explore one of the most foundational yet important concepts in economics: demand curves. The demand curve shows the graphical relationship between the price of a good or service and the quantity demanded for it over a given period of time. 
In other words, it shows the quantity of goods or services consumers are willing to buy at each market price. 
The quantity of goods or services demanded or supplied can be modeled as a function of price, as in:  

$$\text{Quantity} = f(\text{Price})$$

Notably, the curve is downwards sloping because of the law of demand, which states that *as the price of a good or service increases, the quantity demanded for it decreases, assuming all other factors are held constant*. 

This should make intuitive sense: as prices increase, fewer people are willing to pay the higher price for the same good. On the other hand, as prices decrease, more people are willing to pay the lower price for the same good. Hence, the quantity demanded of a good or service has an inverse relationship with the price. 

For now, let's assume that the relationship is somewhat linear and can be described as 

$$\text{Quantity}_{d}=-a \cdot \text{Price}_{d} + b$$

We can interpret the equation above as follows: *as the price of a unit increases by 1, there is an $a$ unit decrease in the quantity demanded.* For example, $\text{Quantity}_{d}=-2 \cdot \text{Price}_{d} + 3$ would suggest that a price increase by 1 would decrease overall quantity demanded in the market by 2. 

Price can also be measured as function of quantity to denote demand. In this case, we use an inverse demand function, as it is the inverse function of the demand function above. Since price is a function of quantity, 

$$\text{Price} = f(\text{Quantity})$$

As we are solving for the inverse of the previous demand function, the inverse demand function for the example above is 

$$\text{Price}_{d}=-\frac{1}{2} \cdot \text{Quantity}_{d} + \dfrac{3}{2}$$

## Movement and the Demand Curve

### Shifts in the Demand Curve
The demand curve can shift in or out based on exogenous events that occur outside of the market. 
Some factors other than a change in price of the good/service could be changes in:

*  buyer's income
*  consumer preferences
*  expectation of future price/supply/demand
*  changes in the price of related goods

If any of these changes occur and causes the demand for the selected good/service to decrease, then the curve shifts to the left as less of the good or service will be demanded at every price. Similarly, if any of these changes causes the demand for the selected good/service to increase, the curve would shift to the right. This signifies that more of the good or service will be demanded at every price. For example, consumers' incomes decreased during the 2008 recession, thus decreasing overall buying power and shifting the demand curve leftwards; a left shift in the demand curve suggests that consumers would purchase fewer quantities of goods at every price.

```python
xs = np.linspace(-10, 20, 100)
ys = -3 * xs + 8
ys2 = -3 * xs + 3

arx = 5
ary = -8
ardx = -1
ardy = 0

fig = plt.figure(figsize=[7,7])
plt.plot(xs, ys, label="Original Demand Curve", linewidth=2)
plt.plot(xs, ys2, label="Shifted Demand Curve", linestyle="dashed", linewidth=2, color="#049348")
plt.arrow(arx, ary, ardx, ardy, head_width=0.8, head_length=0.5, length_includes_head=True)
plt.xticks([])
plt.yticks([])
plt.xlabel("Quantity", size=16)
plt.ylabel("Price", size=16)
plt.xlim([-0.5, 10.5])
plt.ylim([-25, 10])
plt.legend()
fig.savefig('fig-01-demand-orig-shifted-demand.png', bbox_inches='tight')
plt.close(fig)
plt.show()
```

```{figure} fig-01-demand-orig-shifted-demand.png
---
name: orig-shifted-demand.png
alt: Graph of shifting demand curve
---
Fig. Graph of a shifting demand curve
```

### Movements along the Demand Curve

Above, we looked at when exogenous events affect the demand curve. Another concept is movements along the demand curve, which would be considered endogenous events. In movements along the demand curve, changes in price affect the quantity demanded of the good or service. This assumes ceteris paribus, which means holding all other factors constant. This phenomenom is called the Movement of the Demand Curve. With a change in price, the quantity demanded for the good or service can shift the quantity demanded either upward or downward on the demand curve. For example, consider the shift on the graph below from quantity $q_1$ at price $p_1$ to quantity $q_2$ at price $p_2$.

[Following image is a graph of a shift along a demand curve]

```python
xs = np.linspace(-10, 20, 100)
ys = -3 * xs + 8

q_1, p_1 = 1, 5
q_2, p_2 = 8, -16

fig = plt.figure(figsize=[7,7])
plt.plot(xs, ys, color="#1F578E")
plt.scatter([q_1, q_2], [p_1, p_2], s=200, color="g", zorder=15)
plt.arrow(q_1, p_1, q_2 - q_1, p_2 - p_1, color="#CB7432", width=.1, head_length=1.5, head_width=0.5, length_includes_head=True, zorder=-1)

# (q_1, p_1)
plt.vlines(q_1, -1000, p_1, linestyle="dashed", color="#1F578E")
plt.hlines(p_1, -1000, q_1, linestyle="dashed", color="#1F578E")

# (q_2, p_2)
plt.vlines(q_2, -1000, p_2, linestyle="dashed", color="#1F578E")
plt.hlines(p_2, -1000, q_2, linestyle="dashed", color="#1F578E")

plt.xticks([q_1, q_2], [r"$q_1$", r"$q_2$"], size=14)
plt.yticks([p_1, p_2], [r"$p_1$", r"$p_2$"], size=14)
plt.xlabel("Quantity", size=16)
plt.ylabel("Price", size=16)
plt.xlim([-0.5, 10.5])
plt.ylim([-25, 10])
fig.savefig('fig-01-demand-q1-p1-q2-p2.png', bbox_inches='tight')
plt.close(fig)
```

```{figure} fig-01-demand-q1-p1-q2-p2.png
---
name: fig-01-demand-q1-p1-q2-p2.png
alt: Graph of demand curve illustrating change in quantity purchased(q1) at price one(p1) to quanitity purchased(q2) at price  two (p2)
---
Fig. Graph of a shifting demand curve as prices changes -- all other factors constant
```

## Income and Substitution Effect

Changes in the price of a good also leads to 2 effects, which can help explain the law of demand:

- **Income Effect**: Examines how the change in price of the good affects income, which then affects the quantity demanded of a good or service. If the price of a good increases, it would require the consumer to spend more of their income on the good. This dissuades consumers from purchasing more of the good, decreasing the quantity demanded. If the price of a good decreases, consumers would spend less money to receive the same good. This increases the quantity demanded for the good, because consumers would have more income remaining to purchase additional units, given the increase in the amount of purchasing power required to obtain the good.
- **Substitution Effect**: Examines how the change in price of the good affects its demand relative to other goods and services. If the price of a good increases, then consumers might look at similar goods that function in the same way or yield an equivalent amount of utility as the original good. Consumers are effectively shifting or substituting away from the relatively more expensive good to a cheaper alternative, thereby decreasing the quantity demanded for the original good. The converse is also true: if the price of a good decreases, then consumers that currently purchase other similar goods might start consuming this good instead, because it would cost less money for them to obtain.


For example, if the price of gas increases, the higher price for gas might encourage consumers to look at purchasing more efficient cars, such as electric or hybrid cars, thus decreasing the demand for gas. This is the subsitution effect. If the consumer stays with their original car, then they would have less disposable income after purchasing the now-more-expensive gas, so they might purchase less gas. This is the income effect.



--- END 01-demand/01-demand.md ---



--- START 01-demand/02-example.md ---

---
title: 02-example
type: textbook
source_path: content/01-demand/02-example.ipynb
chapter: 1
---

```python
# HIDDEN
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter("ignore")
from matplotlib import patches
from datascience import *
%matplotlib inline
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets
```

# An Empirical Demand Curve

Let's examine some historical data on non-organic avocado prices and sales volumes in San Francisco from 2015 to 2018. The original dataset is taken from Kaggle and can be found here:[Kaggle Dataset on Avocado Prices](https://www.kaggle.com/neuromusic/avocado-prices).

```python
avocados = Table.read_table("avocados.csv") # is it avocados or avocadoes?
avocados
```

## Visualizing the Relationship between Price and Quantity

To construct the demand curve, let's first see what the relationship between price and quantity is. We would expect to see a downward-sloping line between price and quantity; if a product's price increases, consumers will purchase less, and if a product's price decreases, then consumers will purchase more. 

To find this, we will create a scatterplot and draw a regression line (by setting `fit_line = True` in the `tbl.scatter` call) between the points. Regression lines are helpful because they consolidate all the datapoints into a single line, helping us better understand the relationship between the two variables. 

[Following image is a scatter plot of a demand curve for Avocados]

```python
avocados.scatter("Total Volume", "Average Price", fit_line = True, width = 7, height = 7)
plt.title("Demand Curve for Avocados", fontsize = 16);
```

The visualization shows a negative relationship between quantity and price, which is exactly what we expected! As we've discussed, as the price increases, fewer consumers will purchase avocados, so the quantity demanded will decrease. This corresponds to a leftward movement along the demand curve. Alternatively, as the price decreases, the quantity sold will increase because consumers want to maximize their purchasing power and buy more avocados; this is shown by a rightward movement along the curve.


As a quick refresher, scatterplots can show positive, negative, or neutral correlations among two variables: 
- If two variables have a positive correlation, then as one variable increases, the other increases too. 
- If two variables have a negative correlation, then as one variable increases, the other decreases. 
- If two variables have a neutral correlation, then if one varible increases, the other variable is unaffected. 

Note that scatterplots do not show or prove causation between two variables-- it is up to the data scientists to prove any causation.

## Fitting a Linear Demand Curve

We will now quantify our demand curve using NumPy's [`np.polyfit` function](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). `np.polyfit` returns an array of size 2, where the first element is the slope and the second is the $y$-intercept.

It takes 3 parameters:
- array of x-coordinates
- array of y-coordinates
- degree of polynomial 

Because we are looking for a **linear** function to serve as the demand curve, we will use 1 for the degree of polynomial. 

The general template for the demand curve is $y = mx + b$, where $m$ is the slope and $b$ is $y$-intercept.

### Demand with Price as a Function of Quantity

First, we will fit a demand curve expressed in terms of price as a function of quantity. This aligns with the axes of supply and demand curves, in which the quantity is on the x-axis and price is on the y-axis:

$$P(Q) = m\cdot Q + b$$

```python
slope, intercept = np.polyfit(avocados.column("Total Volume"), avocados.column("Average Price"), 1)
print("The slope is:", slope)
print("The intercept is:", intercept)
```

Thus, our demand curve is $P(Q) = -0.00000109Q+ 2.2495$; The slope is -0.00000109 and $y$-intercept is 2.2495. This means that as the quantity demanded increases by 1 unit (in this case, 1 avocado), we would expect to see price to decrease by 0.00000109 units.

We can plot this line on a graph. Notice that it is the same line as the one when we indicated `fit_line=True` above.

[Following image is a scatter plot of a demand curve for Avocados]

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
quantities = np.arange(400000, 1600000, 1000)
predicted_prices = slope * quantities + intercept
plt.plot(quantities, predicted_prices, color = 'red', label = "Demand curve")
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```

### Demand with Quantity as a Function of Price

Our interpretation of the demand curve and its slope above was probably not quite intuitive: changes in quantity demanded likely do not trigger changes in price, but instead it is the other way around. In addition, the slope was tiny: the marginal increase of one additional avocado sold had very little effect from the change in price. 

Thus, it is more intuitive to think the effect a one dollar change in price has on the quantity demanded, and to flip our axes:

$$D(P) = Q(P) = m\cdot P + b$$

In this course, we will write $Q(P)$ and $D(P)$ fairly interchangeably when referencing demand.

One key thing to remember: our axes are flipped for this demand curve! If you want to plot it, note that the left hand side (dependent variable) is actually the x-axis variable, while the independent variable is the y-axis variable.

Fitting our data using this function, we get:

```python
slope, intercept = np.polyfit(avocados.column("Average Price"), avocados.column("Total Volume"), 1)
print("The slope is:", slope)
print("The intercept is:", intercept)
```

Here, our demand curve is roughly $Q(P) = -476413P+ 1446952$; the slope is -476413 and $y$-intercept is 1446952. This means that as the price increases by 1 unit (in this case, \$1), we would expect to see quantity demanded to decrease by 476413 units (in this case, 476413 avocados). 

Note that this demand curve is not the same as the previous demand curve! It is not simply the inverse of the previous demand curve.

Plotting this line on a graph, we see a slightly different demand curve: can you see what is different between the two?

[Following image is a scatter plot and fit line of a demand curve for Avocados]

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
prices = np.arange(0.2, 2.3, 0.01)
predicted_quantities = slope * prices + intercept
plt.plot(predicted_quantities, prices, color = 'red', label = "Demand curve")
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```


--- END 01-demand/02-example.md ---



--- START 01-demand/03-log-log.md ---

---
title: 03-log-log
type: textbook
source_path: content/01-demand/03-log-log.ipynb
chapter: 1
---

# Log-log and Semi-log Demand Curves

```python
# HIDDEN
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter("ignore")
from matplotlib import patches
from datascience import *
%matplotlib inline
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets
```

So far, we have examined demand curves assuming that they were linear. Specifically, we've assumed that the relationship between quantity demanded and price was linear: for a \$1 change in price, we can expect a fixed change in units demanded at any price level.

Is this intuitively true? Probably not. For example, a \$1 decrease from \$100 is trivial compared to a \$1 decrease from an original price of \$2. As humans, we think about changes as proportions, and this fact should be reflected in the supply and demand curves. What this implies is that these curves should be exponential in nature: at higher prices, a larger change in price will yield the same change quantity as compared to that in lower prices. 

Perhaps a better model for demand, then, is that a 1\% change in price will lead to a fixed absolute change in units demanded, or a fixed percentage change in units demanded. To model this, we turn to log-log and semi-log demand curves, respectively.

## Using Logarithms for Proportional Changes

Before we jump into our new demand models, we must first go over measuring proportional changes.
To highlight this fact, let's actually consider how the variable GDP behaves. GDP tends to grow by a certain percent each year; no one is particularly interested in how *much* GDP changes from one year to the next, or from one country to another, but rather by *what percent* it changes.

If you were to plot GDP over time for a country, it might look something like this:

[Following image is a line of increasing upward sloping growth of GDP over time]

```python
GDPs = make_array(100)

for _ in np.arange(99):
    GDPs = np.append(GDPs, GDPs.item(-1) * 1.05)
    
plt.figure(figsize=(8,6))
plt.plot(np.arange(100), GDPs)
plt.xlabel('Years')
plt.ylabel('GDP')
plt.title('GDP Over Time');
```

The starting value for GDP is 100, and GDP grows by 5 percent each year. Look at how how much GDP grows in the later years!


While this phenomenon is impressive, it is misleading. At surface level, it seems to imply that something different happened at around year 50 or so that caused GDP to increase considerably in subsequent years. We know that this isn't true, and that this is just a consequence of exponential growth.

To counter this effect, **for variables that tend to vary from one observation to the next by proportions rather than absolute amounts, we take the natural log of these variables**. Let's do that for GDP:

[Following image is a line of constant upward sloping growth of GDP over time]

```python
ln_GDPs = np.log(GDPs)

plt.figure(figsize=(8,6))
plt.plot(np.arange(100), ln_GDPs)
plt.xlabel('Years')
plt.ylabel('log GDP')
plt.title('GDP Over Time');
```

We've now uncovered a linear relationship between years and GDP! You can interpret the slope of this line as the approximate factor that GDP increases by for an increase in one year. If we want the percentage change in GDP for an increase in one year, we can multiply by 100. (You will later see why the slope is not exactly 0.05). To verify:

```python
print('Slope between years 0 and 1: ', ln_GDPs[1] - ln_GDPs[0])
```

To generalize our results, taking the natural log of a variable allows us to interpret its change as a percentage change instead of an absolute change. Using our GDP example from above, the slope after taking the natural log of GDP is now roughly equal to:

$$\text{slope} = \frac{\text{% change in GDP}}{\text{Change in time (1 year)}} \approx \frac{\text{Change in log-GDP} \times 100}{\text{Change in time (1 year)}} $$

## Semi-log Demand Curves

Revisiting demand curves, suppose that a change in price by $1 leads to a change in quantity demanded by a factor of $m$. This means that our slope would be:

$$\text{slope} = \frac{\text{change in quantity by a factor of } m}{\text{ \$1 change in price}} \approx \frac{m \text{ change in log-quantity}}{\text{\$1 change in price}}$$

Using our intuition from above, we simply have to log transform our quantity-demanded variable to capture the above relationship. This is known as the semi-log demand curve, in which the price and log-quantity are linearly related:

$$\ln{D(P)} = m\cdot P + b$$

Let's gain some more intuition of this relationship. By exponentiating both sides, this is equivalent to:

$$\begin{align*}
D(P) &= e^{m\cdot P + b}\\
&= e^be^{m\cdot P } \\
\end{align*}$$

This rewriting provides us some intuition on the interpretation of $m$ and $b$, the slope and intercept terms in the semi-log relationship. $b$ (specifically $e^b$) corresponds to the 'baseline' quantity demanded when price is 0, since $e^{m \cdot P} = e^0 = 1$. 

$m$ corresponds roughly to how much a one dollar change in price will lead to a percentage change in quantity demanded. To see this, imagine that P goes up by one dollar such that we have:

$$
\begin{align*}
D(P+1) &= e^be^{m \cdot (P+1) }  \\
&= e^be^{m + m \cdot P }\\
&= e^be^me^{m \cdot P }\\
&= e^mD(P) \\
&\approx (1+m)D(P)
\end{align*}$$

The last line relies on the fact $e^{x} \approx (1+x)$ when $x$ is small. This is why in our GDP example, our slope was not exactly 0.05. Thus, our results leads to the caveat that our transformation is only approximate and only valid when our $m$ is small. Typically, we do not want $m$ to be larger than $0.2$, or else the approximation will start to diverge.

### Visualizing the semi-log relationship
From above, we can convert the linear semi-log relationship into non-log terms for price and quantity, ultimately getting $D(P) = e^be^{m\cdot P}$. Plotting this relationship, we get:

[Following image is a line of downward sloping semi-log Demand]

```python
m = -0.05
b = 5

x = np.arange(0,100)
y = (np.e ** (x * m)) * (np.e ** b)

plt.figure(figsize=(8,6))
plt.plot(x, y)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Semi-log Demand Curve');
```

## Log-log Demand Curves

Now suppose that a 1% change in price leads to an $m$% change in quantity demanded. This means that our slope would be:

$$\text{slope} = \frac{m * \text{pct change in quantity}}{1 \text{pct change in price}} \approx \frac{0.01m \text{ change in log-quantity}}{0.01 \text{ change in log-price}}$$

In this case, we have to log transform both our quantity-demanded variable as well as price variable to capture the above relationship. This is known as the log-log demand curve, in which the log-price and log-quantity are linearly related:

$$\ln{D(P)} = m\cdot\ln{P} + b$$

Let's gain some more intuition of this relationship. By exponentiating both sides, this is equivalent to:

$$\begin{align*}
D(P) &= e^{m\cdot\ln{P} + b}\\
&= e^be^{m\cdot\ln{P}} \\
&= e^b(e^{\ln{P}})^m \\
&= e^bP^m \\
\end{align*}$$

In this setup, $b$ does not have as clear a meaning. For $m$, we can suppose that $P$ goes up by one percent:

$$\begin{align*}
D(1.01P) &= e^b(1.01P)^m \\
&= e^b 1.01^m P^m\\
&= 1.01^m D(P) \\
&\approx (1 + 0.01m) D(P) 
\end{align*}$$

Where we utilize the approximation that $1.01^m \approx 1 + 0.01m$. Our caveat from the previous section about $m$ not being large continues to be in place here: typically, we do not want our $m$ to be larger than 30 (where $1.01^{30}\approx 1.35$), or else the approximation will fall apart.

### Visualizing the log-log relationship
From above, we can convert the linear log-log relationship into non-log terms for price and quantity, ultimately getting $D(P) = e^bP^m$. Plotting this relationship, we can see a demand curve that looks somewhat like an exponential decay curve:

[Following image is a line of downward sloping log-log Demand]

```python
m = -0.05
b = 5

x = np.arange(0,100)
y = (x ** m) * (np.e ** b)

plt.figure(figsize=(8,6))
plt.plot(x, y)
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Log-log Demand Curve');
```

A caveat about our model: since our model is ultimately linear between log-price and log-quantity, the slope is always the same. This means that at any price level, we assume a 1% change in price will yield the same percentage change in quantity. This is also known as fixed elasticities -- a topic we will dive in on the next page.

## Revisiting Avocados
We will revisit the avocados dataset from the previous page, which features historical data on non-organic avocado prices and sales volumes in San Francisco from 2015 to 2018.

```python
avocados = Table.read_table("avocados.csv") # is it avocados or avocadoes?
avocados
```

Taking a look at the natural relationship between price and volume sold, we see a possible exponentially decaying relationship between volume and price. We will try using both semi-log and log-log transformations on the data.

[Following image is a scatter plot of Volume vs Price ]

```python
avocados.scatter("Total Volume", "Average Price")
```

### Semi-log

```python
log_quantity = np.log(avocados.column("Total Volume"))

slope, intercept = np.polyfit(avocados.column("Average Price"), log_quantity, 1)
print("The slope is: ", slope)
print("The intercept is: ", intercept)
```

Interpreting the slope, we get that for every one dollar change in price of avocados, we would expect the change in quantity demanded to decrease by 57%. Take this result with a grain of salt – recall that our approximation typically is valid for small values of $m$, and here our $m=-0.57$. 

Now, let's plot our semi-log demand curve. First, we will plot it on an axes in which the quantity is log-transformed, exhibiting a linear demand curve:

[Following image is a straight line of downward sloping semi-log Demand]

```python
plt.scatter(log_quantity, avocados.column("Average Price"))
prices = np.arange(0.5, 2.3, 0.01)
predicted_quantities = slope * prices + intercept
plt.plot(predicted_quantities, prices, color = 'red', label = "Semi-log demand curve")
plt.xlabel("Log Quantity")
plt.ylabel("Price")
plt.legend();
```

Next, let's plot our semi-log demand curve in which neither axis is log-transformed:

[Following image is a curved line of downward sloping semi-log Demand]

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
prices = np.arange(0.5, 2.3, 0.01)
predicted_quantities = np.e ** (slope * prices + intercept)
plt.plot(predicted_quantities, prices, color = 'red', label = "Semi-log demand curve")
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```

### Log-log

```python
log_quantity = np.log(avocados.column("Total Volume"))
log_price = np.log(avocados.column("Average Price"))

slope, intercept = np.polyfit(log_price, log_quantity, 1)
print("The slope is: ", slope)
print("The intercept is: ", intercept)
```

Interpreting the slope, we get that for every 1% change in price of avocados, we would expect the change in quantity demanded to decrease by $-0.816\%$. Once again, keep in mind that our approximation typically is valid for small values of $m$, and here our $m=-0.816$.  

Plotting our log-log demand curve with both axes log-transformed, we observe a linear demand curve:

[Following image is a straight line of downward sloping log-log Demand]

```python
plt.scatter(log_quantity, log_price)
prices = np.arange(-0.2, 0.8, 0.01)
predicted_quantities = slope * prices + intercept
plt.plot(predicted_quantities, prices, color = 'red', label = "Log-log demand curve")

plt.xlabel("Log Quantity")
plt.ylabel("Log Price")
plt.legend();
```

Finally, let's plot our log-log demand curve in which neither axis is log-transformed:

[Following image is a curved line of downward sloping log-log Demand]

```python
plt.scatter(avocados.column("Total Volume"), avocados.column("Average Price"))
prices = np.arange(0.6, 2.5, 0.01)
predicted_quantities = (np.e ** intercept) * (prices ** slope)
plt.plot(predicted_quantities, prices, color = 'red', label = "Log-log demand curve")
plt.xlabel("Quantity")
plt.ylabel("Price")
plt.legend();
```

Which model is better: semi-log or log-log? There is no correct answer here, in fact justifying one approach over another is surprisingly profound. One way to approach this is to look at the graphs produced above and which red line goes through our data points "best" (but what does "best" mean? We'll save this for another day...). Another approach is to utilize our real-world knowledge to conclude which relationship is more accurate: do consumers react similarly to price changes that are in a proportion manner or in a absolute manner? This may also depend on the price, the promotion around it, the product itself, and many other factors. 

This example highlights how ambiguity is a big part of doing data science. We can approach ambiguity with statistical methods and with domain knowledge. Either way, as long as you can ultimately justify your approach, that is what is key in conducting robust data science.



--- END 01-demand/03-log-log.md ---



--- START 01-demand/04-elasticity.md ---

---
title: 04-elasticity
type: textbook
source_path: content/01-demand/04-elasticity.ipynb
chapter: 1
---

```python
from datascience import *
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
from utils import *
plt.style.use('seaborn-muted')
import sympy
```

# Elasticity

Suppose you own a business but you find yourself struggling to make a profit lately. How would you resolve this? You might consider raising your prices. But surely if you were to raise your prices too much you might lose some of your customers. To relate to our discussions above regarding surplus, maybe some of your customers are already at their highest willingness to pay, and raising prices would push them away from your business. Depending on the situation, you might lose so many customers that the increased revenue from raising your prices are completely offset by a decline in customers. This is a problem. To answer the question of how much to raise prices without losing too many customers and therefore losing revenue, we would like a way to measure this concept of customers' responsiveness to prices. We call this concept *elasticity*.

How would we measure elasticity? In the above context, we want to see how much quantity demanded of your business's goods or services changes in response to a change in price. It seems like a good place to start would be to define elasticity as the percent change in quantity over percent change in price. Let's use the demand curve, assuming linear non-log demand curves, depicted below as an example.

[Following image is a line of downward sloping Demand]

```python
def plot_demand1():
    p = sympy.Symbol("p")
    demand_equation = 10 - p
    prices = [x for x in range(0,11)]
    demand_Q = [demand_equation.subs(p,x) for x in prices]
    plt.figure(figsize = [9,6])
    plt.plot(prices,demand_Q)
    plt.xlabel("Demand Quantity")
    plt.ylabel("Price")
    plt.title("Demand Curve")

plot_demand1()
plt.show()
```

Now let's say our current price is 5, and we would like to increase the price to 6. We can see from the curve that this causes demand to drop from 5 to 4.

[Following image is a line of downward sloping Demand with movement from 5 to 6]

```python

```

```python
plot_demand1()
plt.plot(np.full(6,5), np.arange(0,6,1), 'r--')
plt.plot(np.full(7,4), np.arange(0,7,1), 'r--')
plt.plot(np.arange(0,6,1), np.full(6,5), 'r--')
plt.plot(np.arange(0,5,1), np.full(5,6), 'r--')
plt.plot(5,5,'ro')
plt.text(5.1,5.1,"(5,5)")
plt.plot(4,6,'ro')
plt.text(4.1,6.1,"(4,6)")
plt.arrow(4.9,5.1,-0.01,0.01,head_width=0.2)
plt.arrow(4.6,5.4,-0.01,0.01,head_width=0.2)
plt.arrow(4.3,5.7,-0.01,0.01,head_width=0.2)
plt.show()
```

Using our definition from above, we can calculate the elasticity.

$$
\begin{align}
\frac{\%\Delta Q}{\%\Delta P} &= \frac{\frac{4-5}{5}\times100\%}{\frac{6-5}{5}\times100\%} \\
&= \frac{-20\%}{20\%} \\
&= 1
\end{align}
$$

*Note that we take the absolute value for elasticity.*

This tells us that in this specific example, the percent change in price is met with an equal percent change in quantity. But what happens if we start at a price of 6 and go down to a price of 5? This movement is just the opposite of what we did above, so is the elasticity also 1? Let's see.

[Following image is a line of downward sloping Demand with movement from 6 to 5]

```python
plot_demand1()
plt.plot(np.full(6,5), np.arange(0,6,1), 'r--')
plt.plot(np.full(7,4), np.arange(0,7,1), 'r--')
plt.plot(np.arange(0,6,1), np.full(6,5), 'r--')
plt.plot(np.arange(0,5,1), np.full(5,6), 'r--')
plt.plot(5,5,'ro')
plt.text(5.1,5.1,"(5,5)")
plt.plot(4,6,'ro')
plt.text(4.1,6.1,"(4,6)")
plt.arrow(4.7,5.3,0.01,-0.01,head_width=0.2)
plt.arrow(4.4,5.6,0.01,-0.01,head_width=0.2)
plt.arrow(4.1,5.9,0.01,-0.01,head_width=0.2)
plt.show()
```

$$
\begin{align}
\frac{\%\Delta Q}{\%\Delta P} &= \frac{\frac{5-4}{4}\times100\%}{\frac{5-6}{6}\times100\%} \\
&= \frac{25\%}{-16.67\%} \\
&\approx 1.5
\end{align}
$$

We get a different number! This is of course a property of percentages. In both scenarios we moved price by 1, but we started at different prices, and increasing price by 1 starting from 5 is different percentage-wise from starting at 6 and decreasing by 1.

If we want to find a unique value for elasticity between the price points of 5 and 6, independent of price movement, we can use something called the **mid-point method**. It looks like this:

$$\frac{\frac{Q_2 - Q_1}{(Q_2 + Q_1)\times0.5}\times100\%}{\frac{P_2 - P_1}{(P_2 + P_1)\times0.5}\times100\%}$$

What this does is it essentially finds the elasticity of the point between the two given points.

It is also useful, however, to think of elasticity as a property of a single point. For example, we might want the price point of 5 in the example above to have a set elasticity of 1 irrespective of what other point we are going to or coming from. For this, we can use the **point-slope formula**.

$$\frac{\Delta Q}{\Delta P}\frac{P}{Q}$$

or

$$\frac{dQ}{dP}\Bigr|_{P,Q}\times\frac{P}{Q}$$

where the left fraction is the inverse slope at that particular point.

It turns out that in our example above, we can think of the very first formula for elasticity (percent change in quantity over percent change in price) as the point-slope elasticity of the starting point. To verify this, find the elasticity from a price of 5 to 6, from a price of 5 to 4, and at the price of 5 using the point-slope formula. You should get the same answer all three times. This does not hold in general for all curves, but using the base formula that was first introduced should be fine for our examples and other situations.

## Other Elasticities

We can measure elasticity in different contexts as well. Naturally, if we have an elasticity of demand, it follows that we should have an elasticity of supply. Just like consumers make adjustments to their quantity demanded based on changes in the market price, producers respond to market prices as well. Elasticity of supply can be calculated similarly to elasticity of demand.

Imagine that there are two fast food locations next to each other on campus competing for students' business. Let's call them Railway and Bear Express. Initially, their food is priced such that they get equal amounts of business from the students, and students on average don't prefer one over the other; they just want a good and affordable place to eat. Now, imagine that Bear Express raises the prices of their food, while Railway holds their prices steady. In this scenario, one can imagine that students would begin eating more frequently at Railway as opposed to Bear Express due to it being relatively cheaper out of the two options. Now, if Railway in turn increases their prices, the flow of students would shift back to Bear Express by a certain amount. These two restaurants are said to offer goods that are *substitutes* to each other. That is, a consumer will want one or the other, but usually not both. Furthermore, it would seem that just like the price of a good affects its quantity demanded, for substitutes the price of one good can affect the quantity demanded of the other. This is called *cross-price elasticity of demand* (as opposed to *own-price elasticity* that we just learned earlier). It can be defined as follows:

$$\frac{\%\Delta Q_{Good A}}{\%\Delta P_{Good B}}$$

Notice that here there is a notion of positive and negative elasticity. Substitutes will have positive cross-price elasticity, because we expect that an increase in the price of one good will lead to an increase in the quantity demanded of its substitute. But what about when cross-price elasticity is negative? In other words, are there goods where an increase in the price of one leads to a decrease in the quantity demanded of the other? Of course; these are called *complements*. Examples of complementary goods could be cars and gasoline, hot dogs and hot dog buns, or peanut butter and jelly.

There is one more type of elasticity that we will briefly discuss, and that is *income elasticity of demand*. For most goods, the more disposable income consumers have the more they will demand these goods. Almost all goods follow this pattern, and thus there are countless examples. These goods are called *normal goods*. More interestingly, there are some goods, called *inferior goods*, for which we see quantity demanded decrease as income increases. An example of this might be frozen, microwaveable food. As consumers grow wealthier and their palates grow more discerning, their consumption of frozen food might decrease in place of more cooked meals. To calculate this elasticity we would use the following:

$$\frac{\%\Delta Q}{\%\Delta Income}$$

## Elasticity and Revenue

Let's take a closer look at own-price elasticity of demand. Below we've defined a function using our first definition. Let's see what happens as we increase price. Note that is this percent change in quantity over % change in price. While this is negative quantity usually, we still denote it as a positive number.

```python
def elasticity(P1,P2,Q1,Q2):
    return ((Q2-Q1)/Q1)/((P2-P1)/P1)

p = sympy.Symbol("p")
demand_equation = 10-p
prices = [i for i in range(1,10)]

print("Price: Elasticity")
print()

for x in prices:
    print(str(x)+":", abs(elasticity(x,x+1,demand_equation.subs(p,x),demand_equation.subs(p,x+1))))
```

Notice how a price increase will increase revenue until a certain point, at which point revenue decreases again. Intuitively this makes sense. A firm can only increase its prices so much until it starts losing too many of its customers for the increase to be worthwhile. Let's try and find when this happens in general. First let's plot revenue as a function of price.

[Following image is a u-shaped curve of revenue at different prices]

```python
elast = [abs(elasticity(x,x+1,demand_equation.subs(p,x),demand_equation.subs(p,x+1))) for x in range(1,10)]
revenue = [demand_equation.subs(p,x)*x for x in range(1,10)]
prices = [x for x in range(1,10)]
plt.figure(figsize = [9,6])
plt.plot(prices,revenue)
plt.xlabel("Price")
plt.ylabel("Revenue")
plt.title("Revenue vs Price for Given Demand Curve")
plt.show()
```

We notice that revenue reaches a maximun at a price of 5. What elasticity does this price point have?

[Following image is an upward sloping line of elasticty at different prices ]

```python
plt.figure(figsize = [9,6])
plt.plot(prices,elast)
plt.plot(np.arange(1,6,1), np.full(5,1), 'r--')
plt.plot(np.full(2,5), np.arange(0,2,1), 'r--')
plt.plot(5,1,'ro')
plt.text(5,1.5,"(5,1)")
plt.xlabel("Price")
plt.ylabel("Elasticity")
plt.title("Elasticity vs Price for Given Demand Curve")
plt.show()
```



Revenue is maximized when elasticity is 1! To further reinforce this, let's plot revenue vs elasticity.

[Following image is a line revenue vs elasticity]

```python
plt.figure(figsize = [9,6])
plt.plot(elast,revenue)
plt.plot(np.full(17,1), np.arange(9,26,1), 'r--')
plt.plot(1,25,'ro')
plt.text(1.3,25,"(1,25)")
plt.xlabel("Elasticity")
plt.ylabel("Revenue")
plt.title("Revenue vs Elasticity for Given Demand Curve")
plt.show()
```

Before we continue to the final section let's make sure we have a solid intuitive grasp of the significance of our findings. Near low price points, a firm deciding how to raise prices will tend to face inelastic demand. You can imagine that if there were some market where let's say laptops cost only a few dollars, doubling the price of laptops would proabalby not decrease demand for laptops at all, since the price of laptops is still very low. Here, demand is inelastic, or in other words, it is less than 1, because a unit change in price is not met by the same change in quantity. Alternatively, if laptops were to cost as much as a house, and then the price for laptops doubled, the few people who were still in the market for laptops would probably be very discouraged from continuing to buy laptops. It wouldn't be unreasonable to assume that nearly all of the demand would be lost. Here, demand is elastic, or greater than 1, because a unit change in price is met by an even greater change in quantity. The point of the above exercise is to illustrate that there is some middle ground between these two extremes, where a firm with a good pulse on market demand should be pricing its products.

It's important to remember that while we have so far been discussing elasticity from a demand perspective, supply also has the property of elasticity.

## Relative Elasticity

Suppose that you are hiking in the mountains when a venomous snake bites you. Fortunately there is a hospital nearby. Unfortunately, it seems that their antivenom is very expensive. If the antivenom costs $1000, would you pay? What if the price doubled to $2000? What is your max willingness to pay for this antivenom? In the moment, you would likely be willing to pay anything and everything for the antivenom, as any money you refuse to pay won't be of much use to you if you're dead.

Contrast this with your demand for pizza. Imagine a pizza chain close to where you live doubles their prices from \\$3 a slice to \\$6 a slice. Most people would be disuaded from buying pizza by this price increase, assuming that there are many alternatives to pizza close to where you live. Since pizza is not a very essential good, and there are plenty of other places to eat close to where you live, most wouldn't see the point in getting pizza anymore.

We can easily see that any change in price is unlikely to change the demand for antivenom in our example, and thus we say the demand for antivenom is inelastic. But this is a different kind of elasticity. We are not measuring the marginal change in quantity demanded at a given price point or interval, we are saying that in general, on the demand curve for antivenom the elasticity at any price is more inelastic than some other typical demand curve. Similarly, in our example we can say that the demand for pizza is relatively elastic, as the pizza chain cannot change their prices too much without significantly altering the quantity demanded for their pizza. The keyword in this concept is *relative*. It doesn't really make much sense to say that the demand for antivenom is inelastic without there being at least one other kind of demand that is comparatively elastic.

More specifically, our example of antivenom likely exhibits *perfect inelasticity*, meaning that demand for antivenom is the same at absolutely any price; the hospital can charge any price it wants for antivenom. Contrast this with our nearby pizza chain. If we make some further assumptions and say that the market for food around your house or apartment is perfectly competitive, such that any marginal increase in price of one food chain will cause them to lose all of their customers to other food chains, then the demand for the specific food of that chain is *perfectly elastic*. The chain cannot increase its prices or else it will lose all demand. These two extreme scenarios are depicted below.


[Following image is a vertical line]
[Following image is a horizontal line]

```python
plt.figure(figsize = [9,6])
plt.plot(np.full(20,1), np.arange(0,20,1))
plt.xlabel("Quantity Demanded")
plt.ylabel("Price")
plt.title("Perfectly Inelastic Demand Curve")
plt.xticks([], [])
plt.yticks([], [])
plt.show()
```

```python
plt.figure(figsize = [9,6])
plt.plot(np.arange(0,20,1), np.full(20,1))
plt.xlabel("Quantity Demanded")
plt.ylabel("Price")
plt.title("Perfectly Elastic Demand Curve")
plt.xticks([], [])
plt.yticks([], [])
plt.show()
```


--- END 01-demand/04-elasticity.md ---



--- START 01-demand/index.md ---

---
title: index
type: textbook
source_path: content/01-demand/index.md
chapter: 1
---

# Demand and Elasticities

**Student Learning Outcomes:**

* Understand basic properties of the demand curve, including movements along and shifts in the curve
* Examine different types of demand curves and their implications: non-log, semi-log, and log-log
* Use `np.polyfit` to create a demand curve from provided data
* Understand different types of elasticities and calculate price elasticities for demand 


--- END 01-demand/index.md ---



--- START 02-supply/.ipynb_checkpoints/index-checkpoint.md ---

---
title: index-checkpoint
type: textbook
source_path: content/02-supply/.ipynb_checkpoints/index-checkpoint.md
chapter: 2
---

# Supply and Market Equilibrium

**Student Learning Outcomes:**
* Know how a supply curve is formed
* Understand when and how much a firm decided to produce
* Understand demand and price elasticities of goods
* Use SymPy to solve systems of supply and demand equations
* Calculate the price equilibrium



--- END 02-supply/.ipynb_checkpoints/index-checkpoint.md ---



--- START 02-supply/01-supply.md ---

---
title: 01-supply
type: textbook
source_path: content/02-supply/01-supply.ipynb
chapter: 2
---

```python
from datascience import *
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import pandas as pd
from utils import *
plt.style.use('seaborn-v0_8-muted')
from matplotlib import patches
from csaps import csaps
import warnings
warnings.filterwarnings("ignore")
```

# The Supply Curve

The supply of a commodity refers to the quantity for which producers or sellers are willing to produce and offer for sale, at a particular price in some given period of time. To answer questions like _"at a given price, what will be the supply of a good in the market?"_, we need to know the market supply curve. A supply curve is simply a curve (or graph) which shows the quantites of a good that can be produced and the prices they will be sold at.

It is good to discern between individual and market supply. **Individual supply** refers to the supply offered by a single firm or producer, while **market supply** refers to the supply offered by all the firms or producers in a market. It is the horizontal summation of the individual supply curves in the market.

The following table and graph will give an example of a market with two firms: A and B.

[Following image is a graph of horizontal supply addition]

```python
market_supply = Table().with_columns(
    "Price", make_array(2, 3, 4),
    "Quantity supplied by A", make_array(20, 30, 40),
    "Quantity supplied by B", make_array(30, 40, 50),
    "Market Supply", make_array(50, 70, 90)
)
market_supply
```

```python
plt.figure(figsize=[7,7])
plt.plot(market_supply.column(1), market_supply.column(0), marker='o')
plt.plot(market_supply.column(2), market_supply.column(0), marker='^', color="#049348")
plt.plot(market_supply.column(3), market_supply.column(0), marker='*', color="red")
plt.xlabel('Quantity')
plt.ylabel('Price')
plt.title('Market Supply')
plt.legend(make_array("Quantity supplied by A","Quantity supplied by B","Market Supply"), bbox_to_anchor=(1.04,1), loc="center left")

plt.show()
```

Market behavior relating to supply is based on the behavior of the individual firms that comprise it. Now, how does an individual firm make its decision about production? It does so based on the costs associated with production. If the price of a good is enough to recover the costs, the firm produces. Generally, costs increase with the quantity of production. So, to induce producers to increase the quantity supplied, the market prices need to be high enough to compensate for the increased costs.

## Costs

We will split costs into two categories: fixed costs and variable costs.

```{admonition} Definition
**Fixed costs** are costs associated with fixed factors (or inputs) of production. For example, land for a factory, capital equipment like machinery, etc. The quantity of these inputs cannot be changed quickly in the short term. A factory owner cannot purchase land quickly enough to ramp up production in a week. A key point to note is that fixed costs are irrespective of the quantity, i.e., they do not change with the quantity produced.

**Variable costs** are costs associated with variable factors (or inputs) of production. For example, labor, raw materials, etc. The quantity of these inputs can be changed quickly in the short term to adjust supply. A factory owner can hire more laborers or purchase more raw material to increase output. Variable costs change as the supply changes.
```

Another important cost calculation to consider is the marginal cost.

```{admonition} Definition
The **marginal cost** is the additional cost to produce one more unit of output. It can be calculated as the difference between the total cost and the current level of output and the total cost at the previous level of output.
```

Below is a table with the following costs incurred by the firm:

* **Output:** Units produced and supplied
* **Total Fixed Cost (TFC):** Cost incurred by firm on usage of all fixed factors.
* **Total Variable Cost (TVC):** Cost incurred by firm on usage of all variable factors.
* **Total Cost (TC):** Sum of the total fixed and variable costs.
* **Marginal Cost (MC)**
* **Average Fixed Cost (AFC):** Cost per unit of fixed factors. It can be calculated as the Total Fixed Cost divided by the corresponding output level. 
* **Average Variable Cost (AVC):** Cost per unit of variable factors. It can be calculated as the Total Variable Cost divided by the corresponding output level. 
* **Average Total Cost (ATC):** Total cost per unit. This is the sum of the Average Fixed Cost and the Average Variable Cost.

```python
individual_firm_costs = Table.read_table('supply_textbook.csv')
individual_firm_costs.show()
```

Let's create some visualizations to understand the relationships of the different cost curves.

[Following image is a graph of Total, Fixed and Variable Costs]

```python
plt.figure(figsize=[7,7])
plt.plot(individual_firm_costs.column("Output"), individual_firm_costs.column("Total Fixed Cost"), marker='o')
plt.plot(individual_firm_costs.column("Output"), individual_firm_costs.column("Total Variable Cost"), marker='^', color="#049348")
plt.plot(individual_firm_costs.column("Output"), individual_firm_costs.column("Total Cost"), marker='*', color="red")
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('TFC, TVC and TC')
plt.legend(make_array("Total Fixed Cost","Total Variable Cost","Total Cost"))

plt.show()
```

There are two important things to notice about the graph above. First, the total fixed cost is flat. This is because the fixed cost does not change regardless of quantity produced. Second, the vertical difference between the total variable cost and total cost is the TFC. This is because $\text{TC} = \text{TVC} + \text{TFC}$.

[Following image is a graph of Average Total, Fixed, and Variable Costs]

```python
plt.figure(figsize=[7,7])
plt.plot(individual_firm_costs.column("Output")[1:], individual_firm_costs.column("Average Fixed Cost")[1:], marker='o')
plt.plot(individual_firm_costs.column("Output")[1:], individual_firm_costs.column("Average Variable Cost")[1:], marker='^', color="#049348")
plt.plot(individual_firm_costs.column("Output")[1:], individual_firm_costs.column("Average Total Cost")[1:], marker='*', color="red")
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('AFC, AVC and ATC')
plt.legend(make_array("Average Fixed Cost","Average Variable Cost","Average Total Cost"))

plt.show()
```

From the graph above, note that:

- The average fixed cost is decreasing throughout. This is because at higher levels of production, the fixed cost is divided across more units. This implies that the difference between the ATC and AVC decreases as we increase production, since $\text{ATC} = \text{AVC} + \text{AFC}$.
- The AVC and ATC slope down initially and then slope up. This represents decreasing and then increasing marginal cost. Marginal cost initially decreases due to efficiencies in producing at scale, but then increases due to the law of variable proportions. 

Now let's introduce the marginal cost curve:

[Following image is a graph of Average Total and Variable Cost and Marginal Cost]

```python
output = individual_firm_costs.column("Output")[1:]
mc = individual_firm_costs.column("Marginal Cost")[1:]
avc = individual_firm_costs.column("Average Variable Cost")[1:]
atc = individual_firm_costs.column("Average Total Cost")[1:]

sp_mc = csaps(output, mc, smooth=0.85) #slightly different command
sp_avc = csaps(output, avc, smooth=0.85)
sp_atc = csaps(output, atc, smooth=0.85)

output_s = np.linspace(output.min(), output.max(), 150)
mc_s = sp_mc(output_s)
avc_s = sp_avc(output_s)
atc_s = sp_atc(output_s)

plt.figure(figsize=[7,7])
plt.plot(output, mc, marker = 'o', color = 'tab:blue')
plt.plot(output_s, mc_s, alpha=0.7, lw = 2, label='_nolegend_', color = 'tab:blue')
plt.plot(output, avc, marker = '^', color = '#2D8A3E')
plt.plot(output_s, avc_s, alpha=0.7, lw = 2, label='_nolegend_', color = '#2D8A3E')
plt.plot(output, atc, marker = '*', color = '#CB7432')
plt.plot(output_s, atc_s, alpha=0.7, lw = 2, label='_nolegend_', color = '#CB7432')
plt.hlines(y=min(avc), xmin = 6, xmax = 8, lw=3, color='b', zorder = 10)
plt.hlines(y=min(atc), xmin = 7.5, xmax = 9.5, lw=3, color='b', zorder = 10)
plt.xlabel('Quantity')
plt.ylabel('Cost')
plt.title('MC, AVC and ATC')
plt.legend(make_array("Marginal Cost","Average Variable Cost","Average Total Cost"))

plt.show()
```

Notice that the MC curve intersects the ATC and AVC curves at their minima. This is because when MC is below the AVC and ATC, it brings down the average since it costs less to produce an additional unit. But as MC begins to increase and surpasses the ATC and AVC cost curves, it will surpass the intersection, and pulls up the AVC and ATC curves. Therefore, it intersects at the minima.

## Production and Firm Behavior

A company decides to produce if the price is greater than or equal to its average variable cost. There are 3 different scenarios: 

- A firm does not produce at all
- It produces at a loss-minimizing quantity
- It produces at a profit
   
Profits are calculated as total revenue minus total costs, where total revenue is price times quantity. For any price that is less than AVC, the firm will not produce at all. This is because for any amount of production, they will lose money. In this case, they shut down and the loss is limited to its fixed costs. In this example, we can see this for prices 24 and below.

[Following image is a graph of Average Total, Fixed, and Marginal Costs and a price line]

```python
firm_behaviour(24, individual_firm_costs)
```

For any price that lies above the AVC curve but below the ATC curve, the firm will produce at a loss-minimising quantity. This is because for some levels of production, they will make revenue that is more than the total variable cost of production but is still less than the total cost, which includes the fixed cost. While they still lose money, they have offset some of the losses they would have incurred from the fixed cost. In our example, we see this for prices between 25 and 31. The red patch in the plot shows the loss.

[Following image is a graph of Average Total, Fixed, and Variable Costs with a loss]

```python
firm_behaviour(28, individual_firm_costs)
```

If the price is above the ATC curve, the firm produces at a profit. In this example, it is at prices 32 and above. The green patch shows the profit.

[Following image is a graph of Average Total, Fixed, and Variable Costs with a profit]

```python
firm_behaviour(36, individual_firm_costs)
```

So, we have seen that a firm produces if the price is above the AVC. The question now is: what is the level of production?

A profit-maximising firm will produce until price is less than or equal to marginal cost. In the above example, the firm produces 8 units. At the 8th unit, the marginal cost to produce that unit is 28, which is less than the price of 36. Thus the firm gets more revenue for the 8th unit than the cost to produce that unit. But the 9th unit costs an additional 38 to produce. The price of 36 is not enough to cover it. Thus it does not produce the 9th unit.

Therefore, based on the price, each firm looks at its costs and makes a decision to produce. At low prices, only the firms with the lowest production costs produce. As the price increases, firms with higher production costs find it feasible to produce and begin to supply. Thus, the market supply rises with higher prices. Firms with lower costs make extra profits.



--- END 02-supply/01-supply.md ---



--- START 02-supply/02-eep147-example.md ---

---
title: 02-eep147-example
type: textbook
source_path: content/02-supply/02-eep147-example.ipynb
chapter: 2
---

```python
from datascience import *
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import pandas as pd
from utils import *
plt.style.use('seaborn-muted')
from matplotlib import patches
import csaps
import warnings
warnings.filterwarnings("ignore")
```

# An Empirical Example from EEP 147

Let's take a look at an empirical example of production. The dataset for this section comes from EEP 147: Regulation of Energy and the Environment.

```python
ESG_table = Table.read_table('ESGPorfolios_forcsv.csv').select(
    "Group", "Group_num", "UNIT NAME", "Capacity_MW", "Total_Var_Cost_USDperMWH"
).sort("Total_Var_Cost_USDperMWH", descending = False).relabel(4, "Average Variable Cost")
ESG_table
```

This table shows some electricity generation plants in California and their costs. The `Capacity` is the output the firm is capable of producing. The `Average Variable Cost` shows the minimum variable cost per megawatt (MW) produced. At a price below AVC, the firm supplies nothing. At a price above the AVC, the firm can supply up to its capacity. Being a profit-maximising firm, it will try to supply its full capacity. 

First, lets look at just the Big Coal producers and understand this firm's particular behavior.

[Following image is a bar graph of 6 power plants ]

```python
selection = 'Big Coal'
Group = ESG_table.where("Group", are.equal_to(selection))
Group
```

```python
# Make the plot
plt.figure(figsize=(9,6))
plt.bar(new_x_group, height_group, width=width_group, edgecolor = "black")
# Add title and axis names
plt.title(selection)
plt.xlabel('Capacity_MW')
plt.ylabel('Variable Cost/Price')

plt.show()
```

We have created the Big Coal supply curve. It shows the price of electricity, and the quantity supplied at those prices, which depends on variable cost. For example, at any variable cost equal to or above 36.5, the producer `FOUR CORNERS` (the one with the lowest production costs) will supply, and so on. Notably, we observe that the supply curve is also upward sloping since we need higher prices to entice producers with higher variasble costs to produce. 

[Following image is a bar graph of 6 power plants with a price line]

```python
group_plot(30)
```

```python
group_plot(37)
```

```python
group_plot(50)
```

Now we will look at all the energy sources. They have been colored according to source for reference.

[Following image is a bar graph of all power plants in all portfilios ]

```python
ESG_plot(30)
```

```python
ESG_plot(50)
```

Look at the thin bars concentrated on the right end of the plot. These are plants with small capacities and high variable costs. Conversely, plants with larger capacities tend to have lower variable costs. Why might this be the case? Electricity production typically benefits from economies of scale: it is cheaper per unit when producing more units. Perhaps the high fixed cost required for electricity production, such as for equipment and land, is the reason behind this phenomenon.



--- END 02-supply/02-eep147-example.md ---



--- START 02-supply/03-sympy.md ---

---
title: 03-sympy
type: textbook
source_path: content/02-supply/03-sympy.ipynb
chapter: 2
---

# SymPy

```python
from datascience import *
import numpy as np
from sympy import *
import sympy
init_printing()
import matplotlib.pyplot as plt
%matplotlib inline
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
```

Python has many tools, such as the [SymPy library](https://docs.sympy.org/latest/tutorial/index.html) that we can use for expressing and evaluating formulas and functions in economics. 

Since SymPy helps with symbolic math, we start out by create a symbol using `Symbol`, which we assign to a variable name. Then, we can use the symbols to construct symbolic expressions.

```python
x = Symbol('x')
x
```

Now let's try using SymPy to create a symbolic expression for some hypothetical supply and demand curves. 

To define an upward sloping supply curve with price expressed as a function of quantity, we start off defining the symbol $Q$, which represents quantity. Then, we set up a negative relationship expressing $P_S$, which denotes the price of the supplied good (how much the producer earns), in terms of $Q$.

```python
Q = Symbol('Q')
P_S = 2 * Q - 3
P_S
```

Similarly, we will also use $Q$ to express a relationship with $P_D$, the price of the good purchased (how much the consumer pays), creating a downward sloping demand curve. 

Note that both functions are of the variable $Q$; this will be important in allowing us to solve for the equilibrium.

```python
P_D = 2 - Q
P_D
```

To solve for the equilibrium given the supply and demand curve, we know that the price paid by consumers must equal to the price earned by suppliers. Thus, $P_D = P_S$, allowing us to set the two equations equal to each other and solve for the equilibrium quantity and thus equilibrium price. To solve this by hand, we would set up the following equation to solve for $Q$:

$$
P_D = P_S\\
2-Q = 2Q-3
$$

Using SymPy, we call `solve`, which takes in 2 arguments that represent the 2 sides of an equation and solves for the underlying variable such that the equation holds. Here, we pass in $P_D$ and $P_S$, both represented in terms of $Q$, to solve for the value of $Q$ such that $P_D=P_S$. It's good to know that `solve` is a custom function built for this class, and will be provided in the notebooks for you.

```python
Q_star = solve(P_D, P_S)
Q_star
```

The value of $Q$ that equates $P_D$ and $P_S$ is known as the market equilibrium quantity, and we denote it as $Q^*$. Here, $Q^* = \frac{5}{3}$. 

With $Q^*$ determined, we can substitute this value as $Q$ to thus calculate $P_D$ or $P_S$. We substitute values using the `subs` function, which follows the syntax `expression.subs(symbol_we_want_to_substitute, value_to_substitute_with)`.

```python
P_D.subs(Q, Q_star)
```

We can also substitute $Q^*$ into $P_S$, and should get the same results.

```python
P_S.subs(Q, Q_star)
```

Thus, the equilibrium price and quantity are \$0.33 and $\frac{5}{3}$, respectively. 

Let's try another example. Suppose our demand function is $\text{Price}_{D}=-2 \cdot \text{Quantity} + 10$. Using SymPy, this would be

```python
demand = -2 * Q + 10
demand
```

In addition, let the supply function be $\text{Price}_{S}=3 \cdot \text{Quantity} + 1$. Using SymPy, this would be

```python
supply = 3 * Q + 1
supply
```

We will now try to find the market equilibrium. The market price equilibrium $P^*$ is the price at which the quantity supplied and quantity demanded of a good or service are equal to each other. Similarly, the market quantity equilibrium $Q^*$ is the quantity at which the price paid by consumers is equal to the price received by producers. 

Combined, the price equilibrium and quantity equilibrium form a point on the graph with quantity and price as its axes, called the equilibrium point. This point is the point at which the demand and supply curves intersect.

First, we solve for the quantity equilibrium.

```python
Q_star = solve(demand, supply)
Q_star
```

Next, we plug the quantity equilibrium into our demand or supply expression to get the price equilibrium:

```python
demand.subs(Q, 9/5)
```

Graphically, we can plot the supply and demand curves with quantity on the $x$ axis and price on the $y$ axis. The point at which they intersect is the equilibrium point.

[Following image is a graph with lines for supply and demand intersecting  ]

```python
def plot_equation(equation, price_start, price_end, label=None, color=None, linestyle=None):
    plot_prices = [price_start, price_end]
    plot_quantities = [equation.subs(list(equation.free_symbols)[0], c) for c in plot_prices]
    plt.plot(plot_prices, plot_quantities, label=label, color=color, linestyle=linestyle)
    
def plot_intercept(eq1, eq2):
    ex = sympy.solve(eq1-eq2)[0]
    why = eq1.subs(list(eq1.free_symbols)[0], ex)
    plt.scatter([ex], [why], color='black', label="Equilibrium")
    return (ex, why)
    
plot_equation(demand, 0, 5, label="Demand")
plot_equation(supply, 0, 5, label="Supply", color="#CB7432", linestyle="dashed")
equilibrium = plot_intercept(supply, demand)
plt.legend()
plt.ylim(0,20)
plt.xlabel("Quantity")
plt.ylabel("Price")
plot_intercept(supply, demand);
```

```python

```


--- END 02-supply/03-sympy.md ---



--- START 02-supply/04-market-equilibria.md ---

---
title: 04-market-equilibria
type: textbook
source_path: content/02-supply/04-market-equilibria.ipynb
chapter: 2
---

```python
from datascience import *

import sympy
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as patches
# plt.style.use('seaborn-muted')
mpl.rcParams['figure.dpi'] = 200
%matplotlib inline

from IPython.display import display
import numpy as np
import pandas as pd
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"

import warnings
warnings.filterwarnings('ignore')
```

# Market Equilibria

We will now explore the relationship between price and quantity of oranges produced between 1924 and 1938. Since the data {cite}`01demand-fruits` is from the 1920s and 1930s, it is important to remember that the prices are much lower than what they would be today because of inflation, competition, innovations, and other factors. For example, in 1924, a ton of oranges would have costed \$6.63; that same amount in 2019 is \$100.78.

```python
fruitprice = Table.read_table('fruitprice.csv')
fruitprice
```

## Finding the Equilibrium

An important concept in econmics is the market equilibrium. This is the point at which the demand and supply curves meet and represents the "optimal" level of production and price in that market.

```{admonition} Definition
The **market equilibrium** is the price and quantity at which the demand and supply curves intersect. The price and resulting transaction quantity at the equilibrium is what we would predict to observe in the market.
```

Let's walk through how to the market equilibrium using the market for oranges as an example.

### Data Preprocessing

Because we are only examining the relationship between prices and quantity for oranges, we can create a new table with the relevant columns: `Year`, `Orange Price`, and `Orange Unloads`. Here, `Orange Price` is measured in dollars, while `Orange Unloads` is measured in tons.

```python
oranges_raw = fruitprice.select("Year", "Orange Price", "Orange Unloads")
oranges_raw
```

Next, we will rename our columns. In this case, let's rename `Orange Unloads` to `Quantity` and `Orange Price` to `Price` for brevity and understandability.

```python
oranges = oranges_raw.relabel("Orange Unloads", "Quantity").relabel("Orange Price", "Price")
oranges
```

### Visualize the  Relationship

Let's first take a look to see what the relationship between price and quantity is. We would expect to see a downward-sloping relationship between price and quantity; if a product's price increases, consumers will purchase less, and if a product's price decreases, then consumers will purchase more. 

We will create a scatterplot between the points.

[Following image is a scatter plot for demand for oranges]

```python
oranges.scatter("Quantity", "Price", width=5, height=5)
plt.title("Demand Curve for Oranges", fontsize = 16);
```

The visualization shows a negative relationship between quantity and price, which is in line with our expectations: as the price increases, fewer consumers will purchase oranges, so the quantity demanded will decrease. This corresponds to a leftward movement along the demand curve. Alternatively, as the price decreases, the quantity sold will increase because consumers want to maximize their purchasing power and buy more oranges; this is shown by a rightward movement along the curve.

### Fit a Polynomial

We will now quantify our demand curve using NumPy's [`np.polyfit` function](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). Recall that `np.polyfit` returns an array of size 2, where the first element is the slope and the second is the $y$-intercept.

For this exercise, we will be expressing demand and supply as quantities in terms of price.

```python
np.polyfit(oranges.column("Price"), oranges.column("Quantity"), 1)
```

This shows that the demand curve is $D(P) = -3433 P+ 53626$. The slope is -3433 and $y$-intercept is 53626. That means that as price increases by 1 unit (in this case, \$1), quantity decreases by 3433 units (in this case, 3433 tons).

### Create the Demand Curve

We will now use SymPy to write out this demand curve. To do so, we start by creating a symbol `P` that we can use to create the equation.

```python
P = sympy.Symbol("P")
demand = -3432.846 * P + 53625.87
demand
```

### Create the Supply Curve

As you've learned, the supply curve is the relationship between the price of a good or service and the quantity of that good or service that the seller is willing to supply. It shows how much of a good suppliers are willing and able to supply at different prices. In this case, as the price of the oranges increases, the quantity of oranges that orange manufacturers are willing to supply increases. They capture the producer's side of market decisions and are upward-sloping.

Let's now assume that the supply curve is given by $S(P) = 4348P$. (Note that this supply curve is not based on data.)

```python
supply = 4348 * P
supply
```

This means that as the price of oranges increases by 1, the quantity supplied increases by 4348. At a price of 0, no oranges are supplied.

### Find the Price Equilibrium

With the supply and demand curves known, we can solve the for equilibrium. 
The equilibrium is the point where the supply curve and demand curve intersect, and denotes the price and quantity of the good transacted in the market.

The equilbrium consists of 2 components: the quantity equilbrium and price equilbrium. 
The price equilibrium is the price at which the supply curve and demand curve intersect: the price of the good that consumers desire to purchase at is equivalent to the price of the good that producers want to sell at. There is no shortage of surplus of the product at this price.


Let's find the price equilibrium. To do this, we will use the provided `solve` function. This is a custom function that leverages some SymPy magic and will be provided to you in assignments.

```python
P_star = solve(demand, supply)
P_star
```

This means that the price of oranges that consumers want to purchase at and producers want to provide is about \$6.89.

### Find the Quantity Equilibrium

Similarly, the quantity equilibrium is the quantity of the good that consumers desire to purchase is equivalent to the quantity of the good that producers supply; there is no shortage or surplus of the good at this quantity.

```python
demand.subs(P, P_star)
supply.subs(P, P_star)
```

This means that the number of tons of oranges that consumers want to purchase and producers want to provide in this market is about 29,967 tons of oranges.

### Visualize the Market Equilibrium 

Now that we have our demand and supply curves and price and quantity equilibria, we can visualize them on a graph to see what they look like. 

There are 2 pre-made functions we will use: `plot_equation` and `plot_intercept`.
- `plot_equation`: It takes in the equation we made previously (either demand or supply) and visualizes the equations between the different prices we give it
- `plot_intercept`: It takes in two different equations (demand and supply), finds the point at which the two intersect, and creates a scatter plot of the result

```python
def plot_equation(equation, price_start, price_end, label=None, color=None, linestyle=None):
    plot_prices = [price_start, price_end]
    plot_quantities = [equation.subs(list(equation.free_symbols)[0], c) for c in plot_prices]
    plt.plot(plot_quantities, plot_prices, label=label, color=color, linestyle=linestyle)
    
def plot_intercept(eq1, eq2):
    ex = sympy.solve(eq1-eq2)[0]
    why = eq1.subs(list(eq1.free_symbols)[0], ex)
    plt.scatter([why], [ex], zorder=10, color="black")
    return (ex, why)
```

We can leverage these functions and the equations we made earlier to create a graph that shows the market equilibrium.

[Following image is a scatter plot for supply and demand for oranges]

```python
mpl.rcParams['figure.dpi'] = 150
plot_equation(demand, 2, 10, label = "Demand")
plot_equation(supply, 2, 10, label = "Supply", color="#CB7432", linestyle="dashed")
plt.ylim(0,13)
plt.title("Orange Supply and Demand in 1920's and 1930's", fontsize = 15)
plt.xlabel("Quantity (Tons)", fontsize = 14)
plt.ylabel("Price ($)", fontsize = 14)
plot_intercept(supply, demand)
plt.legend(loc = "upper right", fontsize = 12)
plt.legend()
plt.show()
```

You can also practice on your own and download additional data sets [here](http://users.stat.ufl.edu/~winner/datasets.html), courtesy of the University of Flordia's Statistics Department.

## Movements Away from Equilibrium

What happens to market equilibrium when either supply or demand shifts due to an exogenous shock?

Let's assume that consumers now prefer Green Tea as their hot beverage of choice moreso than before. We have an outward shift of the demand curve - quantity demanded is greater at every price. The market is no longer in equilibrium.

[Following image is a hand drawn diagram of a shift in demand]

```{figure} fig1-demand.png
---
width: 500px
name: demand-shift
---
A shift in the demand curve
```

![title](fig1-demand.png)

At the same price level (the former equilibrium price), there is a shortage of Green Tea. The amount demanded by consumers exceeds that supplied by producers: $Q_D > Q_S$. This is a seller's market, as the excess quantity demanded gives producers leverage (or market power) over consumers. They are able to increase the price of Green Tea to clear the shortage. As prices increase, consumers who were willing and able to purchase tea at the previous equilibrium price would leave the market, reducing quantity demanded. $Q_S$ and $Q_D$ move up along their respective curves until the new equilibrium is achieved where $Q_S = Q_D$. 

This dual effect of increasing $Q_S$ and $Q_D$ is sometimes referred to as the "invisible hand". Sans government intervention, it clears out the shortage or surplus in the market, resulting in the eventual convergence to a new equilibrium level of quantity $Q^*$ and price $P^*$.



--- END 02-supply/04-market-equilibria.md ---



--- START 02-supply/index.md ---

---
title: index
type: textbook
source_path: content/02-supply/index.md
chapter: 2
---

# Supply and Market Equilibrium

**Student Learning Outcomes:**
* Know how a supply curve is formed
* Understand when and how much a firm decided to produce
* Understand demand and price elasticities of goods
* Use SymPy to solve systems of supply and demand equations
* Calculate the price equilibrium



--- END 02-supply/index.md ---



--- START 03-public/.ipynb_checkpoints/index-checkpoint.md ---

---
title: index-checkpoint
type: textbook
source_path: content/03-public/.ipynb_checkpoints/index-checkpoint.md
chapter: 3
---

# Public Economics

**Public economics** deals with the different methods government policy affect market equilibria, and studies these through the lens of economic efficiency and equity. This chapter begins with a graphical and mathematical overview of market equilibrium. It follows with a discussion of consumer and producer surplus - specifically how these concepts underscore the competing forces of supply and demand. Last, it covers the different ways in which governments intervene: taxes, subsidies and price controls. It discusses their effects on welfare and how intervention changes market participation


--- END 03-public/.ipynb_checkpoints/index-checkpoint.md ---



--- START 03-public/.ipynb_checkpoints/surplus-checkpoint.md ---

---
title: surplus-checkpoint
type: textbook
source_path: content/03-public/.ipynb_checkpoints/surplus-checkpoint.ipynb
chapter: 3
---

```python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import sympy
import numpy as np
import warnings
from datascience import *
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
import plotly.graph_objects as go
from plotly.offline import plot
from IPython.display import display, HTML
warnings.filterwarnings('ignore')
plt.style.use("seaborn-muted")
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y)) == 1 else "Not Single Solution"
%matplotlib inline
```

# Surplus

## Consumer Surplus
Although all consumers face the same market price, consumers are different in how much they individually value a good. We say that consumers have a maximum price that they are willing to pay for a good, and any price marginally higher than this price will dissuade the consumer from participating in the market. This max willingness to pay (WTP) price varies among entities based on their desire for the good, which in turn can be based on how much of the good they already have.

Consider the market for electricity. Among consumers we have entities such as households, commercial buildings, factories, and so on. A factory would likely have a very high WTP for electricity because the opportunity costs for factories to not operate are very high. Capital is expensive, employees still have to be paid, and it is inconvenient to have to stop and start up machinery frequently. Thus, for a factory it is preferable to always have a reliable supply of electricity to continue operations and this need is reflected in the WTP. Contrast this with households, who certainly value having electricity, but should electricity become prohibitively expensive, probably would decide to cut back on usage as the drawbacks of not having electricity aren't as severe compared to the factory above.

## Producer Surplus
Producers experience a similar characteristic. A producer has a minimum price at which it is willing to produce a good based on its costs. Any market price less than this price would dissuade a producer from supplying its good. Again, in the electricity example, we have several power plants that produce electricity, but each inherently does so at different costs. Imagine and contrast the operating costs of a solar farm with a coal plant, or a newer, more efficient natural gas plant versus an older one.

Putting all of these concepts together we arrive at the idea of economic welfare. Suppose electricity costs 10 cents per kWh. On the demand side, imagine a factory who's WTP is 30 cents/kWh. This factory enjoys a consumer surplus of 20 cents/kWh, in other words, it's paying 20 cents less per kWh than what it would be willing to pay. A household might have a WTP of 15 cents/kWh. Here the household's surplus is only 5 cents/kWh. We can also imagine a consumer whose WTP is less than the market price and thus doesn't participate in the market. Imagine for some reason that cryptocurrency prices have dropped to the point that they aren't worth the electricity it takes to mine them. In this case, we might have an idle or non-existent crypto-farm (a place with a lot of computing power) due to electricity being too expensive. On the producer side, maybe we have a solar plant which is operating at the market price, but a natural gas plant that is idling because the price of supplying electricity isn't sufficient to make up for operating costs.

Combining the surpluses of all individual consumers and producers yields the market consumer surplus and producer surplus. As the market price fluctuates, certain comsumers and producers enter and exit the market, and the total surplus varies. Note from the above examples that a consumer is not always an individual, it can be a firm buying from another firm. We now explore further.

## Example

We create a consumer class with a WTP characteristic, and a list of consumers with WTP from 10 to 1. The binary function `demand` ```1``` if the consumer participates in the market at a given price and ```0``` if not.

```python
class Consumer:
    def __init__(self, WTP):
        self.WTP = WTP
        
    def demand(self, price):
        if price <= self.WTP:
            return 1
        else:
            return 0
        
    def surplus(self, price):
        if price <= self.WTP:
            return self.WTP - price
        else:
            return 0
        
consumers = [Consumer(x) for x in range(10,0,-1)]

[x.demand(6) for x in consumers]
```

For a market price of 6, we have 5 consumers who participate and 5 who don't. Now let's make a table of the lists of participants for each market price between 1 and 10.

```python
per_consumer = np.array([[x.demand(i) for x in consumers] for i in range(10, 0, -1)]).T
Table().with_columns("Market Price", np.arange(10, 0, -1), 
                     "Consumer with WTP: 10", per_consumer[0],
                     "Consumer with WTP: 9", per_consumer[1],
                     "Consumer with WTP: 8", per_consumer[2],
                     "Consumer with WTP: 7", per_consumer[3],
                     "Consumer with WTP: 6", per_consumer[4],
                     "Consumer with WTP: 5", per_consumer[5],
                     "Consumer with WTP: 4", per_consumer[6],
                     "Consumer with WTP: 3", per_consumer[7],
                     "Consumer with WTP: 2", per_consumer[8],
                     "Consumer with WTP: 1", per_consumer[9])
```

You can draw a downward-sloping diagonal line sepearating ```1```s from ```0```s - a vague resemblance of a downward-sloping demand curve. The left-most consumer, with a WTP of 10, always participates for the market prices we have listed. The right-most consumer only participates at a market price of 1. Now lets try and find the number of consumers who participate for each price point, starting at 10.

```python
market = Table().with_column("Market Price", np.arange(10, 0, -1))
market = market.with_column("Number of Participants", 
                   market.apply(lambda price : sum([x.demand(price) for x in consumers]), "Market Price"))
market
```

Instead of printing a binary 1 or 0 indicating market participation, we've displayed each participant's actual surplus value. Similarly, let's find total surplus per price point.

```python
surplus = Table().with_column("Market Price", np.arange(10, 0, -1))
surplus = surplus.with_column("Total Surplus", 
                   surplus.apply(lambda price : sum([x.surplus(price) for x in consumers]), "Market Price"))
surplus
```

Clearly there must be an opposite "force" at play here, otherwise all prices would converge to 0 as consumers maximize their surplus (more on maximization later). Naturally, we must also consider the producers who sell their product to the consumers. We essentially repeat the exercise above, but now instead of a consumer class with individual willingness to pay, we have a producer class with some minimal market price at which production can occur.

```python
class Producer:
    def __init__(self, WTA):
        self.WTA = WTA
        
    def supply(self, price):
        if price >= self.WTA:
            return 1
        else:
            return 0
        
    def surplus(self, price):
        if price >= self.WTA:
            return price - self.WTA
        else:
            return 0
        
producers = [Producer(x) for x in range(1,11)]
```

```python
per_producer = np.array([[x.surplus(i) for x in producers] for i in np.arange(10, 0, -1)]).T

Table().with_columns("Market Price", np.arange(10, 0, -1), 
                     "Producer with WTP: 10", per_producer[0],
                     "Producer with WTP: 9", per_producer[1],
                     "Producer with WTP: 8", per_producer[2],
                     "Producer with WTP: 7", per_producer[3],
                     "Producer with WTP: 6", per_producer[4],
                     "Producer with WTP: 5", per_producer[5],
                     "Producer with WTP: 4", per_producer[6],
                     "Producer with WTP: 3", per_producer[7],
                     "Producer with WTP: 2", per_producer[8],
                     "Producer with WTP: 1", per_producer[9])
```

Looks familiar, but with an opposite slope! Here we've captured the idea of producer surplus. At a market price of 10, the leftmost producer is very happy with a surplus of 9, as in this case that producer is actually able to produce and sell at a price of 1 but is able to operate at a price of 10.

Before we continue, let's take a moment to think about the meaning and significance of our findings. Firms that can produce at lower market prices than their peers seem to be better off in the sense that they enjoy higher surplus. This minimum production price is based on the costs of operation the firm experiences, so naturally it seems that firms that can operate at lower costs do better. Certainly, if market prices decrease, more inefficent firms would be the first to shut down while these low operating cost firms continue to do business. This idea is very important in economics: Firms that can reduce their costs are rewarded with higher surplus. This is pretty much how society advances, at least in an economics context. Production methods continually to improve, and less efficient firms must either follow suit or shut down as prices decrease, to the benefit of consumers.

However, what would the equivalent be for the consumer side of things? We've discussed the idea of willingness to pay, and initially it might seem that in our perfectly-competitive market environment, only the consumers who most need a good or service will be the first to get it, as their WTP is the highest. We might think that resources are efficiently allocated in this way. Most of the time this is likely the case, but we've made an assumption while reaching this conclusion; an assumption that doesn't necessarily hold. We have assumed that a person with high willingness to pay also has at least an equally high *ability* to pay. In reality, this might not be the case. A hungry person might have high WTP for a serving of food, but if this person lacks the means to pay for this food, his willingness to pay won't do him much good. In this scenario, our earlier exercise reflects willingness to pay with a simultaneous ability to pay as well. While this week isn't about the ethics of certain types of markets and whether they achieve their goals, it's important to keep in mind that in these ideal exercises, an efficient economy with rational pricing should reflect consumers' willingness to pay, whereas in reality this might not actually be the case.

## Note on the Demand and Supply Curves

As pointed out above, the matrix we saw with rows of surpluses and columns of prices resembles the demand curve in the sense that we can see a diagonal line separating participants from non-participants. This is no coincidence. This idea is essentially what the demand and supply curves depict, except that due to there usually being many participants in a market, we simplify the concept to a continuous curve as opposed to a set of discrete values. This is helpful not only for visualization, but as we will soon see we can use these curves to find rates of change, which will prove to be useful as well.


Earlier we had a matrix of each individual's surplus at each price point, and the overall surplus at each price point. Notice how as the price decreased, surplus increased. Let's see this exact same concept illustrated on a familiar demand curve. Take a few moments to adjust the slider controlling the market price to see how consumer surplus behaves.

```python
filename="fig1.html"

p = sympy.Symbol("p")
consumers = [Consumer(x) for x in range(0,11)]
demand_equation = 10 - p
prices = [x for x in range(0,11)]
demand_Q = [float(demand_equation.subs(p,x)) for x in prices]

fig = go.Figure()

active_idx = None
active_val = 5
vals = []
for i, val in enumerate(np.arange(1, 11)):
    vals.append(val)
    if val == active_val:
        active_idx = 3 * i
        step_idx = i
        
    demand_subbed = float(demand_equation.subs(p, val))
    fig.add_trace(go.Scatter(
        x = demand_Q,
        y = prices,
        mode = "lines",
        name = f"Demand",
        visible = False
    ))
    fig.add_trace(go.Scatter(
        x = [demand_subbed, 0, 0, demand_subbed],
        y = [val, val, 10, val],
        name = "Consumer Surplus",
        fill = "toself",
        visible = False,
        marker_color = "green"
    ))
    fig.add_trace(go.Scatter(
        x = [demand_subbed],
        y = [val],
        mode = "markers",
        name = "Equilibrium",
        visible = False,
        marker = dict(color = "red", size = 10)
    ))
    
fig.data[active_idx].visible = True
fig.data[active_idx + 1].visible = True
fig.data[active_idx + 2].visible = True

steps = []
for i in range(len(fig.data)):
    if i % 3 != 0:
        continue
    val = vals[i // 3]
    step = dict(
        method = "update",
        args = [
            {"visible": [False] * len(fig.data) + [True]},
            {"annotations": [
                dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Consumer Surplus: {sum([person.surplus(val) for person in producers])}")
            ]}
        ],
        label = f"{val}"
    )
    step["args"][0]["visible"][i] = True
    step["args"][0]["visible"][i + 1] = True
    step["args"][0]["visible"][i + 2] = True
    steps.append(step)
    
sliders = [
    dict(
        active = step_idx,
        currentvalue = {"prefix": "Market Price: $"},
        pad = {"t": 75},
        steps = steps
    )
]

fig.update_layout(
    xaxis = dict(title = "Demand Quantity"),
    yaxis = dict(title = "Price"),
    title = "Demand Curve with Consumer Surplus Shaded",
    sliders = sliders,
    width = 800, 
    height = 600,
    showlegend = False,
    annotations = [dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Consumer Surplus: {sum([person.surplus(active_val) for person in consumers])}")]
)

plot(fig, filename=filename, auto_open=False, include_mathjax='cdn')
    
display(HTML(filename))
```

Producer surplus with the supply curve works exactly the same way but mirrored to reflect the fact that producers gain surplus from higher prices instead of lower.

```python
filename="fig2.html"

p = sympy.Symbol("p")
producers = [Producer(x) for x in range(1,11)]
supply_equation = p
prices = [x for x in range(0,11)]
supply_Q = [float(supply_equation.subs(p, x)) for x in prices]

fig = go.Figure()

active_idx = None
active_val = 5
vals = []
for i, val in enumerate(np.arange(1, 11)):
    vals.append(val)
    if val == active_val:
        active_idx = 3 * i
        step_idx = i
        
    supply_subbed = float(supply_equation.subs(p, val))
    fig.add_trace(go.Scatter(
        x = supply_Q,
        y = prices,
        mode = "lines",
        name = f"Supply",
        visible = False
    ))
    fig.add_trace(go.Scatter(
        x = [supply_subbed, 0, 0, supply_subbed],
        y = [val, val, 0, val],
        name = "Producer Surplus",
        fill = "toself",
        visible = False,
        marker_color = "green"
    ))
    fig.add_trace(go.Scatter(
        x = [supply_subbed],
        y = [val],
        mode = "markers",
        name = "Equilibrium",
        visible = False,
        marker = dict(color = "red", size = 10)
    ))
    
fig.data[active_idx].visible = True
fig.data[active_idx + 1].visible = True
fig.data[active_idx + 2].visible = True

steps = []
for i in range(len(fig.data)):
    if i % 3 != 0:
        continue
    val = vals[i // 3]
    step = dict(
        method = "update",
        args = [
            {"visible": [False] * len(fig.data) + [True]},
            {"annotations": [
                dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Producer Surplus: {sum([person.surplus(val) for person in producers])}")
            ]}
        ],
        label = f"{val}"
    )
    step["args"][0]["visible"][i] = True
    step["args"][0]["visible"][i + 1] = True
    step["args"][0]["visible"][i + 2] = True
    steps.append(step)
    
sliders = [
    dict(
        active = step_idx,
        currentvalue = {"prefix": "Market Price: $"},
        pad = {"t": 75},
        steps = steps
    )
]

fig.update_layout(
    xaxis = dict(title = "Supply Quantity"),
    yaxis = dict(title = "Price"),
    title = "Supply Curve with Producer Surplus Shaded",
    sliders = sliders,
    width = 800, 
    height = 600,
    showlegend = False,
    annotations = [dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Producer Surplus: {sum([person.surplus(active_val) for person in producers])}")]
)

plot(fig, filename=filename, auto_open=False, include_mathjax='cdn')
    
display(HTML(filename))
```

Here we used a demand curve of $10-P$ and a supply curve of $P$.



--- END 03-public/.ipynb_checkpoints/surplus-checkpoint.md ---



--- START 03-public/govt-intervention.md ---

---
title: govt-intervention
type: textbook
source_path: content/03-public/govt-intervention.ipynb
chapter: 3
---

# Other Forms of Government Intervention

## Price Controls

The last type of government intervention is far more forceful than taxes and subsidies. Instead of changing the per-unit cost of a good paid by consumers or producers, price controls directly fix the market price. It does not change the amount different consumers are willing and able to pay or how much producers are willing and able to produce. 

When price controls are introduced to a market, the equilibrium price and quantity no longer exist. Consumer and producers are forced to sell or buy goods at the prescribed price.

[Following image is a hand drawn diagram of a price floor]

```{figure} fig6-price-floor.png
---
width: 500px
name: price-floor
---
A price floor
```

![title](fig6-price-floor.png)

Let's take a price floor as the first example. This is when the government imposes a minimum price for which a good can be sold. For this to be effective, the price floor must be set above the equilibrium price, otherwise the market would just settle at equilibrium. This results in a surplus, as producers are willing to sell far more goods than consumers are willing to purchase. This results in a surplus as $Q_s > Q_d$. Normally, without government intervention, the invisible hand would push prices down until $Q_s = Q_d$. However, with the price floor, the market remains in a state of disequilibrium.

Minimum wages act as a price floor, keeping wages above a certain level. At this higher rate, more workers are willing to work than there is demand for, creating unemployment.

[Following image is a hand drawn diagram of a price ceiling ]

```{figure} fig7-price-ceil.png
---
width: 500px
name: price-ceil
---
A price ceiling
```

![title](fig7-price-ceil.png)

The other control is a price ceiling. This is when the government imposes a maximum price for which a good can be sold. This results in a shortage as consumers are to purchase more goods than producer are willing to sell: $Q_d > Q_s$.

Rent control in Berkeley acts as a price ceiling, keeping rents from rising above a certain level.

## Optional: World Trade vs. Autarky

Throughout the class so far, we have assumed that the economy is operating by itself, in isolation. Economists use the word "autarky" to represent the scenario when a country is economically self-sufficient. The key consequence of this is that the country's economy is unaffected by global events, allowing us to conclude that any changes to equilibrium price or quantity are purely a result of shifts in domestic demand or supply.

We will now assume that the country is no longer in autarky, and is thus subject to world prices, demand and supply.

We now label our demand and supply curves as domestic demand and supply, respectively. Their intersection represents where the market would be if we were in autarky. However, as we are now open to world trade, the price where the market operates is now determined by the world price, which may be totally different from the equilibrium price. 

Where the world price line intersects with domestic demand represents the total quantity demanded within the market. Where it intersects with the domestic supply curve is the total quantity supplied.

[Following image is a hand drawn diagram of a market with trade]

```{figure} fig8-autarky.png
---
width: 500px
name: autarky
---
Domestic supply and demand in the world market
```

![title](fig8-autarky.png)

Let's first imagine a scenario where world price is lower than domestic price. From our graph, we find that $Q_s < Q_d$. This is a shortage. Domestic producers are not willing to fully provide all goods demanded by consumers in the economy. The difference between what is produced domestically and the total quantity demanded will be fulfilled by international producers. Thus, $Q_d - Q_s =$ imports. As prices are lower than what some consumers were willing to pay, there is a significant consumer surplus. Unfortunately for producers, as world prices are lower than what they would have been able to charge, they lose producer surplus. There is a net gain in the change in consumer surplus is greater than the loss in producer surplus. This is represented by the triangle between the curves and above the world price line.

Now, let's imagine that world price is greater than domestic price. In this case, $Q_s > Q_d$. We now have a surplus where the quantity demanded by consumers is far less than what domestic producers are willing to provide at this higher price. Thus, domestic producers will willingly sell goods to domestic customers and sell the rest on the global market. $Q_s - Q_d =$ exports. As prices are higher than what some consumers were willing to pay, there is a significant decrease in consumer surplus. Producers however rejoice as world prices are higher than what they would have been able to charge. They gain producer surplus. There is a net gain in total surplus as the increase in producer surplus is greater than the loss in consumer surplus. This is represented by the triangle between the curves and above the world price line.



--- END 03-public/govt-intervention.md ---



--- START 03-public/index.md ---

---
title: index
type: textbook
source_path: content/03-public/index.md
chapter: 3
---

# Public Economics

**Public economics** deals with the different methods government policy affect market equilibria, and studies these through the lens of economic efficiency and equity. This chapter begins with a graphical and mathematical overview of market equilibrium. It follows with a discussion of consumer and producer surplus - specifically how these concepts underscore the competing forces of supply and demand. Last, it covers the different ways in which governments intervene: taxes, subsidies and price controls. It discusses their effects on welfare and how intervention changes market participation


--- END 03-public/index.md ---



--- START 03-public/surplus.md ---

---
title: surplus
type: textbook
source_path: content/03-public/surplus.ipynb
chapter: 3
---

```python
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import sympy
import numpy as np
import warnings
from datascience import *
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
import plotly.graph_objects as go
from plotly.offline import plot
from IPython.display import display, HTML
warnings.filterwarnings('ignore')
plt.style.use("seaborn-muted")
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y)) == 1 else "Not Single Solution"
%matplotlib inline
```

# Surplus

## Consumer Surplus
Although all consumers face the same market price, consumers are different in how much they individually value a good. We say that consumers have a maximum price that they are willing to pay for a good, and any price marginally higher than this price will dissuade the consumer from participating in the market. This max willingness to pay (WTP) price varies among entities based on their desire for the good, which in turn can be based on how much of the good they already have.

Consider the market for electricity. Among consumers we have entities such as households, commercial buildings, factories, and so on. A factory would likely have a very high WTP for electricity because the opportunity costs for factories to not operate are very high. Capital is expensive, employees still have to be paid, and it is inconvenient to have to stop and start up machinery frequently. Thus, for a factory it is preferable to always have a reliable supply of electricity to continue operations and this need is reflected in the WTP. Contrast this with households, who certainly value having electricity, but should electricity become prohibitively expensive, probably would decide to cut back on usage as the drawbacks of not having electricity aren't as severe compared to the factory above.

## Producer Surplus
Producers experience a similar characteristic. A producer has a minimum price at which it is willing to produce a good based on its costs. Any market price less than this price would dissuade a producer from supplying its good. Again, in the electricity example, we have several power plants that produce electricity, but each inherently does so at different costs. Imagine and contrast the operating costs of a solar farm with a coal plant, or a newer, more efficient natural gas plant versus an older one.

Putting all of these concepts together we arrive at the idea of economic welfare. Suppose electricity costs 10 cents per kWh. On the demand side, imagine a factory who's WTP is 30 cents/kWh. This factory enjoys a consumer surplus of 20 cents/kWh, in other words, it's paying 20 cents less per kWh than what it would be willing to pay. A household might have a WTP of 15 cents/kWh. Here the household's surplus is only 5 cents/kWh. We can also imagine a consumer whose WTP is less than the market price and thus doesn't participate in the market. Imagine for some reason that cryptocurrency prices have dropped to the point that they aren't worth the electricity it takes to mine them. In this case, we might have an idle or non-existent crypto-farm (a place with a lot of computing power) due to electricity being too expensive. On the producer side, maybe we have a solar plant which is operating at the market price, but a natural gas plant that is idling because the price of supplying electricity isn't sufficient to make up for operating costs.

Combining the surpluses of all individual consumers and producers yields the market consumer surplus and producer surplus. As the market price fluctuates, certain comsumers and producers enter and exit the market, and the total surplus varies. Note from the above examples that a consumer is not always an individual, it can be a firm buying from another firm. We now explore further.

## Example

We create a consumer class with a WTP characteristic, and a list of consumers with WTP from 10 to 1. The binary function `demand` ```1``` if the consumer participates in the market at a given price and ```0``` if not.

```python
class Consumer:
    def __init__(self, WTP):
        self.WTP = WTP
        
    def demand(self, price):
        if price <= self.WTP:
            return 1
        else:
            return 0
        
    def surplus(self, price):
        if price <= self.WTP:
            return self.WTP - price
        else:
            return 0
        
consumers = [Consumer(x) for x in range(10,0,-1)]

[x.demand(6) for x in consumers]
```

For a market price of 6, we have 5 consumers who participate and 5 who don't. Now let's make a table of the lists of participants for each market price between 1 and 10.

```python
per_consumer = np.array([[x.demand(i) for x in consumers] for i in range(10, 0, -1)]).T
Table().with_columns("Market Price", np.arange(10, 0, -1), 
                     "Consumer with WTP: 10", per_consumer[0],
                     "Consumer with WTP: 9", per_consumer[1],
                     "Consumer with WTP: 8", per_consumer[2],
                     "Consumer with WTP: 7", per_consumer[3],
                     "Consumer with WTP: 6", per_consumer[4],
                     "Consumer with WTP: 5", per_consumer[5],
                     "Consumer with WTP: 4", per_consumer[6],
                     "Consumer with WTP: 3", per_consumer[7],
                     "Consumer with WTP: 2", per_consumer[8],
                     "Consumer with WTP: 1", per_consumer[9])
```

You can draw a downward-sloping diagonal line sepearating ```1```s from ```0```s - a vague resemblance of a downward-sloping demand curve. The left-most consumer, with a WTP of 10, always participates for the market prices we have listed. The right-most consumer only participates at a market price of 1. Now lets try and find the number of consumers who participate for each price point, starting at 10.

```python
market = Table().with_column("Market Price", np.arange(10, 0, -1))
market = market.with_column("Number of Participants", 
                   market.apply(lambda price : sum([x.demand(price) for x in consumers]), "Market Price"))
market
```

Instead of printing a binary 1 or 0 indicating market participation, we've displayed each participant's actual surplus value. Similarly, let's find total surplus per price point.

```python
surplus = Table().with_column("Market Price", np.arange(10, 0, -1))
surplus = surplus.with_column("Total Surplus", 
                   surplus.apply(lambda price : sum([x.surplus(price) for x in consumers]), "Market Price"))
surplus
```

Clearly there must be an opposite "force" at play here, otherwise all prices would converge to 0 as consumers maximize their surplus (more on maximization later). Naturally, we must also consider the producers who sell their product to the consumers. We essentially repeat the exercise above, but now instead of a consumer class with individual willingness to pay, we have a producer class with some minimal market price at which production can occur.

```python
class Producer:
    def __init__(self, WTA):
        self.WTA = WTA
        
    def supply(self, price):
        if price >= self.WTA:
            return 1
        else:
            return 0
        
    def surplus(self, price):
        if price >= self.WTA:
            return price - self.WTA
        else:
            return 0
        
producers = [Producer(x) for x in range(1,11)]
```

```python
per_producer = np.array([[x.surplus(i) for x in producers] for i in np.arange(10, 0, -1)]).T

Table().with_columns("Market Price", np.arange(10, 0, -1), 
                     "Producer with WTP: 10", per_producer[0],
                     "Producer with WTP: 9", per_producer[1],
                     "Producer with WTP: 8", per_producer[2],
                     "Producer with WTP: 7", per_producer[3],
                     "Producer with WTP: 6", per_producer[4],
                     "Producer with WTP: 5", per_producer[5],
                     "Producer with WTP: 4", per_producer[6],
                     "Producer with WTP: 3", per_producer[7],
                     "Producer with WTP: 2", per_producer[8],
                     "Producer with WTP: 1", per_producer[9])
```

Looks familiar, but with an opposite slope! Here we've captured the idea of producer surplus. At a market price of 10, the leftmost producer is very happy with a surplus of 9, as in this case that producer is actually able to produce and sell at a price of 1 but is able to operate at a price of 10.

Before we continue, let's take a moment to think about the meaning and significance of our findings. Firms that can produce at lower market prices than their peers seem to be better off in the sense that they enjoy higher surplus. This minimum production price is based on the costs of operation the firm experiences, so naturally it seems that firms that can operate at lower costs do better. Certainly, if market prices decrease, more inefficent firms would be the first to shut down while these low operating cost firms continue to do business. This idea is very important in economics: Firms that can reduce their costs are rewarded with higher surplus. This is pretty much how society advances, at least in an economics context. Production methods continually to improve, and less efficient firms must either follow suit or shut down as prices decrease, to the benefit of consumers.

However, what would the equivalent be for the consumer side of things? We've discussed the idea of willingness to pay, and initially it might seem that in our perfectly-competitive market environment, only the consumers who most need a good or service will be the first to get it, as their WTP is the highest. We might think that resources are efficiently allocated in this way. Most of the time this is likely the case, but we've made an assumption while reaching this conclusion; an assumption that doesn't necessarily hold. We have assumed that a person with high willingness to pay also has at least an equally high *ability* to pay. In reality, this might not be the case. A hungry person might have high WTP for a serving of food, but if this person lacks the means to pay for this food, his willingness to pay won't do him much good. In this scenario, our earlier exercise reflects willingness to pay with a simultaneous ability to pay as well. While this week isn't about the ethics of certain types of markets and whether they achieve their goals, it's important to keep in mind that in these ideal exercises, an efficient economy with rational pricing should reflect consumers' willingness to pay, whereas in reality this might not actually be the case.

## Note on the Demand and Supply Curves

As pointed out above, the matrix we saw with rows of surpluses and columns of prices resembles the demand curve in the sense that we can see a diagonal line separating participants from non-participants. This is no coincidence. This idea is essentially what the demand and supply curves depict, except that due to there usually being many participants in a market, we simplify the concept to a continuous curve as opposed to a set of discrete values. This is helpful not only for visualization, but as we will soon see we can use these curves to find rates of change, which will prove to be useful as well.


Earlier we had a matrix of each individual's surplus at each price point, and the overall surplus at each price point. Notice how as the price decreased, surplus increased. Let's see this exact same concept illustrated on a familiar demand curve. Take a few moments to adjust the slider controlling the market price to see how consumer surplus behaves.

[Following image is an interactive graph of Consumer Surplus at different price points]

```python
filename="fig1.html"

p = sympy.Symbol("p")
consumers = [Consumer(x) for x in range(0,11)]
demand_equation = 10 - p
prices = [x for x in range(0,11)]
demand_Q = [float(demand_equation.subs(p,x)) for x in prices]

fig = go.Figure()

active_idx = None
active_val = 5
vals = []
for i, val in enumerate(np.arange(1, 11)):
    vals.append(val)
    if val == active_val:
        active_idx = 3 * i
        step_idx = i
        
    demand_subbed = float(demand_equation.subs(p, val))
    fig.add_trace(go.Scatter(
        x = demand_Q,
        y = prices,
        mode = "lines",
        name = f"Demand",
        visible = False
    ))
    fig.add_trace(go.Scatter(
        x = [demand_subbed, 0, 0, demand_subbed],
        y = [val, val, 10, val],
        name = "Consumer Surplus",
        fill = "toself",
        visible = False,
        marker_color = "green"
    ))
    fig.add_trace(go.Scatter(
        x = [demand_subbed],
        y = [val],
        mode = "markers",
        name = "Equilibrium",
        visible = False,
        marker = dict(color = "red", size = 10)
    ))
    
fig.data[active_idx].visible = True
fig.data[active_idx + 1].visible = True
fig.data[active_idx + 2].visible = True

steps = []
for i in range(len(fig.data)):
    if i % 3 != 0:
        continue
    val = vals[i // 3]
    step = dict(
        method = "update",
        args = [
            {"visible": [False] * len(fig.data) + [True]},
            {"annotations": [
                dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Consumer Surplus: {sum([person.surplus(val) for person in producers])}")
            ]}
        ],
        label = f"{val}"
    )
    step["args"][0]["visible"][i] = True
    step["args"][0]["visible"][i + 1] = True
    step["args"][0]["visible"][i + 2] = True
    steps.append(step)
    
sliders = [
    dict(
        active = step_idx,
        currentvalue = {"prefix": "Market Price: $"},
        pad = {"t": 75},
        steps = steps
    )
]

fig.update_layout(
    xaxis = dict(title = "Demand Quantity"),
    yaxis = dict(title = "Price"),
    title = "Demand Curve with Consumer Surplus Shaded",
    sliders = sliders,
    width = 800, 
    height = 600,
    showlegend = False,
    annotations = [dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Consumer Surplus: {sum([person.surplus(active_val) for person in consumers])}")]
)

plot(fig, filename=filename, auto_open=False, include_mathjax='cdn')
    
display(HTML(filename))
```

Producer surplus with the supply curve works exactly the same way but mirrored to reflect the fact that producers gain surplus from higher prices instead of lower.

[Following image is an interactive graph of Producer Surplus at different price points]

```python
filename="fig2.html"

p = sympy.Symbol("p")
producers = [Producer(x) for x in range(1,11)]
supply_equation = p
prices = [x for x in range(0,11)]
supply_Q = [float(supply_equation.subs(p, x)) for x in prices]

fig = go.Figure()

active_idx = None
active_val = 5
vals = []
for i, val in enumerate(np.arange(1, 11)):
    vals.append(val)
    if val == active_val:
        active_idx = 3 * i
        step_idx = i
        
    supply_subbed = float(supply_equation.subs(p, val))
    fig.add_trace(go.Scatter(
        x = supply_Q,
        y = prices,
        mode = "lines",
        name = f"Supply",
        visible = False
    ))
    fig.add_trace(go.Scatter(
        x = [supply_subbed, 0, 0, supply_subbed],
        y = [val, val, 0, val],
        name = "Producer Surplus",
        fill = "toself",
        visible = False,
        marker_color = "green"
    ))
    fig.add_trace(go.Scatter(
        x = [supply_subbed],
        y = [val],
        mode = "markers",
        name = "Equilibrium",
        visible = False,
        marker = dict(color = "red", size = 10)
    ))
    
fig.data[active_idx].visible = True
fig.data[active_idx + 1].visible = True
fig.data[active_idx + 2].visible = True

steps = []
for i in range(len(fig.data)):
    if i % 3 != 0:
        continue
    val = vals[i // 3]
    step = dict(
        method = "update",
        args = [
            {"visible": [False] * len(fig.data) + [True]},
            {"annotations": [
                dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Producer Surplus: {sum([person.surplus(val) for person in producers])}")
            ]}
        ],
        label = f"{val}"
    )
    step["args"][0]["visible"][i] = True
    step["args"][0]["visible"][i + 1] = True
    step["args"][0]["visible"][i + 2] = True
    steps.append(step)
    
sliders = [
    dict(
        active = step_idx,
        currentvalue = {"prefix": "Market Price: $"},
        pad = {"t": 75},
        steps = steps
    )
]

fig.update_layout(
    xaxis = dict(title = "Supply Quantity"),
    yaxis = dict(title = "Price"),
    title = "Supply Curve with Producer Surplus Shaded",
    sliders = sliders,
    width = 800, 
    height = 600,
    showlegend = False,
    annotations = [dict(xref="paper", yref="paper", x=-0.003, y=-0.2, showarrow=False, xanchor="left",
                     text=f"Producer Surplus: {sum([person.surplus(active_val) for person in producers])}")]
)

plot(fig, filename=filename, auto_open=False, include_mathjax='cdn')
    
display(HTML(filename))
```

Here we used a demand curve of $10-P$ and a supply curve of $P$.



--- END 03-public/surplus.md ---



--- START 03-public/taxes-subsidies.md ---

---
title: taxes-subsidies
type: textbook
source_path: content/03-public/taxes-subsidies.ipynb
chapter: 3
---

# Taxes and Subsidies

Now that we have discussed cases of market equilibrium with just demand and supply, also known as free market cases, we will examine what happens when the government intervenes. In all of these cases, the market is pushed from equilibrium to a state of disequilibrium. This causes the price to change and, as a result, the quantity transacted in the market.

Broadly, a tax is any type of financial charge imposed by the government, such as income tax, property tax, or excise tax. In this course, and for this section in particular, we will consider only taxes levied on consumption. These taxes are typically enforced on a state level in the US, and can take 2 forms:

- An *excise tax* levies a fixed dollar amount on a particular good or service. A flat \$1 tax per packet of cigarette sold is an example of an excise tax.
- An *ad valorem tax* levies a percentage amount on the purchase of a particular good or service. For example the sales tax is an ad valorem tax.

Notably, consumption taxes can be levied on either the producer or the consumer. The side that pays for the tax *upfront* (when a transaction occurs) is known as the party that bears the **statutory incidence** of the tax. However, as you'll soon learn, this does not mean that the party paying for the tax upfront bears the entire **economic incidence** of the tax.

A subsidy is the opposite of a tax; it involves either a monetary benefit given by the government or a reduction in taxes granted to individual businesses or whole industries.

## Why Tax or Subsidize?

The supply and demand models that we've examined so far do not necessarily reflect the entire picture; often, there are additional social costs or benefits associated with producing or consuming a good that is not paid for by a firm or considered by consumers. 

For example, take a factory producing dyed color T-shirts that pollute a nearby river. In a world without government intervention, the firm would not have to clean up the river even though they should include factor that into their costs. This is an example of a **negative externality**, in which the *private cost* faced in production by a firm or consumption by a consumer is lower than the actual *social cost*.

[Following image is a hand drawn diagram of marginal social vs private costs]

```{figure} fig1-negative-externality.png
---
width: 500px
name: negative-externality
---
A supply-side negative externality in which the marginal social cost is greater than the marginal private cost
```

![title](fig1-negative-externality.png)

Take another example: vaccines. By consuming a vaccine shot, a consumer benefits their communities to overall reduce transmission of a disease. However, chances are the consumer probably did not consider the social benefits when making a decision on whether to vaccinate. This is an example of a **positive externality**, in which the *private benefit* faced in production by a firm or consumption by a consumer is lower than the actual *social benefit* (alternatively, in some cases it may be more intuitive to think about it as the *private cost* is greater than the *social cost*. Similarly, the opposite holds true for negative externalities).

[Following image is a hand drawn diagram of marginal social vs private benefit]

```{figure} fig1-positive-externality.png
---
width: 500px
name: positive-externality
---
A demand-side positive externality in which the marginal social benefit is greater than the marginal private benefit
```

![title](fig1-positive-externality.png)

In this case, a **market failure** occurs, in which the true quantity demanded by society does not match what would occur in a free market without government intervention. This is where taxes and interventions come in: they can correct for externalities and thus resolve consequent market failures.

## Effects of Taxation

The primary method that governments use to intervene in markets to address negative externalities is taxation. 

[Following image is a hand drawn diagram of supply shifting with a tax]

```{figure} fig2-supply-tax.png
---
width: 500px
name: supply-tax
---
A shift in supply due to a tax levied on producers
```

![title](fig2-supply-tax.png)

If a tax is levied on producers, this decreases the quantity of goods they can supply at each price as the tax is effectively acting as an additional cost of production. This shifts the supply curve leftward. Compared to negative externality graph above, we can see that the tax essentially 'corrects' the supply curve based on the marginal private cost to instead mirror the supply curve based on the marginal social cost.

[Following image is a hand drawn diagram of demand shifting with a tax]

```{figure} fig3-demand-tax.png
---
width: 500px
name: demand-tax
---
A shift in demand due to a tax levied on consumers
```

![title](fig3-demand-tax.png)

If the tax is levied on consumers, this increases the price per unit they must pay, thereby reducing quantity demanded at every price. This shifts the demand curve leftward.

## Effects of Subsidies

To account for positive externalities, a popular form of government intervention is a subsidy. They intend to lower production or consumption costs, and thus increase the quantity supplied of goods and services at equilibrium.

[Following image is a hand drawn diagram of supply shifting with a subsidy]

```{figure} fig5-subsidy.png
---
width: 500px
name: subsidy
---
A shift in supply due to a new subsidy
```

![title](fig5-subsidy.png)

We represent this visually as a rightward shift in the supply curve. As costs are lower, producers are now willing to supply more goods and services at every price. The demand curve remains unchanged as a subsidy goes directly to producers. The resulting equilibrium has a lower price $P^*$ and higher quantity $Q^*$. It is assumed that the lower production costs would be passed onto consumers through lower market prices. $P^*$ is what consumers pay, but producers receive $P_P = P^* + \text{subsidy}$. This is depicted visually by the price along the new supply curve at quantity $Q^*$.

Consumer surplus increases as more individuals are able to purchase the good than before. Similarly, producer surplus has increased as the subsidy takes care of part (if not all) of their costs. Overall market surplus has increased.

The welfare gain is depicted in a similar way to that of a tax: a triangle with a vertex at the original market equilibrium and a base along $Q^*$. The cost of the subsidy to the government is $\text{per-unit subsidy} \cdot Q^*$.

## Examining the Effects of Taxes

For the rest of this section, we will examine the effects of taxes in more detail.

[Following image is a hand drawn diagram of changes in surplus due to a change in Demand due to a tax]

```{figure} fig4-dwl-tax-wedge.png
---
width: 500px
name: dwl-tax-wedge
---
Deadweight loss due to a tax levied on consumers
```

![title](fig4-dwl-tax-wedge.png)

The resulting equilibrium - both price and quantity - is the same in both cases. However, the prices paid by producers and consumers are different. Let us denote the equilibrium quantity to be $Q^*$. The price that producers pay $P_p$ occurs where $Q^*$ intersects with the supply curve. At the same time, the price that consumers pay $P_c$ occurs where $Q^*$ intersects  the demand curve.

You will notice that the vertical distance between $P_p$ and $P_c$ will be the value of the tax. That is to say, $P_c = P_p + \text{tax}$. We call the vertical distance between $P_p$ and $P_c$ at quantity $Q^*$ the tax wedge.

### Incidence
    
Determining who bears the greater burden, or economic incidence, of the tax depends on the relative producer and consumer price elasticities. A good that consumers are relatively more inelastic towards (such that producers are more elastic) would mean that the burden of paying the tax will fall on consumers moreso than producers. Intuitively, this is because consumers are less sensitive to price changes and thus are 'more willing' to pay more to adjust to the tax. The opposite is true if the consumers are relatively more elastic (i.e. the producers are relatively more inelastic). See the figure below for more details.

One can calculate the burden share, or the proportion of the tax paid by consumers or producers:

Consumer's burden share: 

$$\dfrac{\text{Increase in unit price after the tax paid by consumers} + \text{Increase in price paid per unit by consumers to producers}}{\text{Tax per unit}}$$

Producer's burden share: 

$$\dfrac{\text{Increase in unit price after the tax paid by producers} - \text{Increase in price paid per unit by consumers to producers}}{\text{Tax per unit}}$$

Graphically, the total tax burden is the rectangle formed by the tax wedge and the horizontal distance between 0 and $Q^*$: $Q^* \cdot \text{tax}$ This is also how you calculate the revenue from the tax earned by the government.

[Following image is a hand drawn diagram of supply shifts with different demand elasticities]

```{figure} fig6-elasticity-of-taxes.png
---
width: 900px
name: elasticity-of-taxes
---
Differences in economic incidences of a tax due to elastic and inelastic demand.
```

![title](fig6-elasticity-of-taxes.png)

### Deadweight Loss

Naturally, the introduction of the tax disrupts the economy and pushes it away from equilibrium. For consumers, the higher price they must pay essentially "prices" out some individuals - they are now unwilling to pay more for the good. This leads them to leave the market that they previously participated in. At the same time, for producers, the introduction of the tax increases production costs and cuts into their revenues. Some of the businesses that were willing to produce at moderately high costs now find themselves unable to make a profit with the introduction of the tax. They too leave the market. There are market actors who are no longer able to purchase or sell the good.

We call this loss of transactions: deadweight welfare loss. It is represented by the triangle with a vertex at the original market equilibrium and a base at the tax wedge. The area of the deadweight loss triangle, also known as Harberger's triangle, is the size of the welfare loss - the total value of transactions lost as a result of the tax.

Another way to think about deadweight loss is the change (decrease) in total surplus. Consumer and producer surplus decrease significantly, but this is slightly offset by the revenue earned by the government from the tax. 

We can calculate the size of Harberger's triangle using the following formula: $\dfrac{1}{2} \cdot \dfrac{\epsilon_s \cdot \epsilon_d}{\epsilon_s - \epsilon_d} \cdot \dfrac{Q^*}{P_p} (\text{tax})^2$ where $\epsilon_s$ is the price elasticity of supply and  $\epsilon_d$ is the price elasticity of demand.

### Salience

We noted in our discussion about taxes that the equilibrium quantity and price is the same regardless of whether the tax is levied on producers or consumers. This is the traditional theory's assumption: that individuals, whether they be producers or consumers, are fully aware of the taxes they pay. They decide how much to produce or consume with this in mind.

We call the visibility at which taxes are displayed their salience. As an example, the final price of a food item in a restaurant is not inclusive of sales tax. Traditional economic theory would say that this difference between advertized or poster price and the actual price paid by a consumer has no bearing on the quantity they demand. That is to say taxes are fully salient. However, recent research has suggested that this is not the case. 

A number of recent studies, including by Chetty, Looney and Kroft in 2009, found that posting prices that included sales tax actually reduces demand for those goods. Individuals are not fully informed or rational, implying that tax salience does matter.

## Calculating Taxes Algebraically

### Expressing Quantity as a Function of Price

So far, we have expressed our demand and supply curves using prices as a function of quantity, e.g. $D(Q) = 100 - Q$. This format aligns with the axes of our plots, since quantity is on the x-axis and price is on the y-axis. However, it perhaps makes more sense to switch this around, expressing quantity demanded or supplied as a function of price. Intuitively, the price of a good or service causes the quantity supplied or demanded to alter; at high prices, producers would be willing to supply a great deal of units while few consumers would enter the market, while the opposite is true at low prices.

To switch in between the different formats, we simply have to solve for the independent variable and express it in terms of our dependent variable. For 
example, if demand is expressed as $D(Q) = 100 - Q$, then:

$$
\begin{align*}
P &= 100 - Q \\
P - 100 &= -Q \\
Q &= 100 - P = D(P)
\end{align*}
$$

### Solving for the new quantity and price equilibria

In previous weeks where there was no tax in the market, we could equate demand and supply to solve for the market price/quantity:

$$D(P) = S(P)$$

In reality, the demand function is based on the price consumers pay (which we'll denote $P_c$), and the supply function is based on the price producers receive (which we'll denote $P_p$). Hence, the actual demand and supply functions are $D(P_c)$ and $S(P_p)$, so we should be equating:

$$D(P_c) = S(P_p)$$

In the no-tax scenario, the price received by producers is the same as the price paid by consumers. Hence, we are able to get away by expressing them both as $P$ above, where $P=P_p=P_c$. 

However, in the case of tax, $P_p$ is no longer equal to $P_c$. Specifically, $P_p+\text{tax}=P_c$. As a result, to solve for equilibrium with taxes, we can use substitution to express $P_c$ as $P_p+\text{tax}$, or $P_p$ as $P_c−\text{tax}$. Hence we actually aim to equate:

$$D(P_c)=S(P_p)\implies D(P_p+\text{tax})=S(P_p)\quad \text{or} \quad D(P_c)=S(P_c−\text{tax})$$

Because there are now 3 unknowns ($P_c,P_p,Q$) and 3 equations ($P_p+\text{tax}=P_c$, supply equation, and demand equation), we conduct this substitution to reduce it to 2 equations and 2 unknowns. Essentially, you'll find that the tax simply was just a shift in the intercepts, which matches our graphical intuition from the diagrams above!

Once we are able to solve for $P_p$ or $P_c$, we can add/subtract the tax to get the other. We can also plug $P_c$ into $D(P_c)$ to get the equilibrium quantity, which should be the same as plugging in $P_p$ into $S(P_p)$. 

Lastly, to calculate the consumer burden, we seek to measure how much more the consumers are now paying due to the tax. Hence, this value is $P_c−P$, where $P$ is the original price when there is no tax. Similarly, to calculate the producer burden, we seek to measure how much less the producers are now receiving due to the tax; this value is $P−P_p$.

## An Example

**Part 1:** Suppose the demand for rutabagas is $D(P_c) = 2000 − 100P_c$. The supply of rutabagas is: $S(P_p) = −100 + 200P_p$. What is the equilibrium price without the tax?

```{admonition} Solution
:class: dropdown

Since there is no tax, $P_p = P_c = P$. Thus:

$$
\begin{align*}
2000 - 100P &= -100 + 200 P \\
2100 - 100P &= 200 P \\
2100 &= 300 P \\
P &= 7 \\
\end{align*}
$$

```

**Part 2:**
What is the equilibrium price with a per unit \$2 sales tax?

```{admonition} Solution
:class: dropdown

With a \$2 sales tax, we know that $P_c = P_p + 2$. Thus:

$$
\begin{align*}
2000 - 100P_c &= -100 + 200 P_p\\
2000 - 100(P_p+2) &= -100 + 200P_p \\
2000 - 100P_p - 200 &= -100 + 200P_p \\
1900 - 100P_p &= + 200P_p \\
1900 &= 300P_p \\
P_p &= 6.33 \\ 
P_c &= 6.33 + 2 = 8.33
\end{align*}
$$
```

**Part 3:**
What are the tax burdens on the consumer and producer?

```{admonition} Solution
:class: dropdown

Consumer burden: $\text{New price paid} - \text{Old price paid} = 8.33 - 7 = 1.33$

Producer burden: $\text{Old price received} - \text{New price received} = 7 - 6.33 = 0.67$

This means that the consumer bears $\frac{2}{3}$ of the total burden of the tax.
```

**Part 4:**
What is change in quantity transacted due to the tax?

```{admonition} Solution
:class: dropdown

Originally, $Q = 2000 - 100P_c = 2000 - 100\times 7 = 1300$. 

Now, $Q = 2000 - 100P_c = 2000 - 100\times 8.33 = 1167$.
The difference in quantity transacted is thus $1300 - 1167 = 133$ units.

Note that we can also plug in $P_p$ into the supply equation and get the same results!

```



--- END 03-public/taxes-subsidies.md ---



--- START 04-production/index.md ---

---
title: index
type: textbook
source_path: content/04-production/index.md
chapter: 4
---

# Production

**Student Learning Outcomes:**

* Understand how Cobb-Douglas Production Functions model the means by which nations produce output
* Derive and visualize how a per-unit change in Capital, Labor or Total Factor Productivity affect output
* Introduce the concept of returns to scale
* Develop a framework of comparing how countries produce output over time


--- END 04-production/index.md ---



--- START 04-production/production.md ---

---
title: production
type: textbook
source_path: content/04-production/production.ipynb
chapter: 4
---

```python
from textbook_utils import *
%matplotlib inline
```

# Production and Cobb-Douglas Functions

## Production in the Economy

At the core of macroeconomics is the study of how a nation's various resources are used as inputs in the production of goods and services. The aggregate value of what a nation produces is known as its Gross Domestic Product, which is calculated in many different ways. The focus of this lecture is on production and the functions that aim to model how much output a country can produce, when given a certain set of inputs. 

These set of inputs are known as factors of production:
- $K$: Capital - a monetary value of the stock or value of productive assets.
- $L$: Labor - the number of worker hours.
- $A$: Total Factor Productivity - a measure of the effectiveness with which the two factors of production are used.

This model of production in an economy provides a simple yet effective way of modeling output. It would be way too complicated to account for every possible input to production, especially as we are operating at the country level. However, the key simplication is that we can classify all of these different inputs as either capital or labor: anything physical or tangible is capital and any work done by humans is labor. Taking the monetary value of either of these, while a rough approximation, still yields great insight into the different ways countries produce goods and services. Even if two countries have very similar GDPs, one maybe more capital intensive than the other. Having this knowledge would greatly inform policy and would help governments direct funding towards areas of concern.

We will see this in action during Project 2 where we will examine real life data from different countries and compare/contrast their usage of labor, capital and total factor productivity.

This simplication has allowed economists to derive the following key notion:
A nation's output is a function of the amount of the factors of production that are utilized in its economy; that is to say output is a function of labor and capital.

Thus, the economy's production function is: 

$$Y = A \cdot f(K, L)$$

$f(K, L)$ refers to any specific mathematical model of output. One such example is the Cobb-Douglas production function that we will be examining in the next section.

### Total Factor Productivity

In modern economies, one way to think about total factor producivity (TFP) is technology or research and development. A country with a high TFP (or technology) can produce far more goods and services than another with a lower TFP but the exact same amount of capital and labor. Think about it: a country with 5 factories utilizing robotic arms to assemble cars will be able to produce more than another nation that also has 5 factories but utilizes workers working in 8-hour shifts. The former country would have a higher TFP than the latter. Thus, it can be said that technology increases the efficiency with which the factors of production are used.

There are three key differences between TFP and the other two factors of production:
1. TFP "scales" production by some factor A. The other two are raised to an exponent that is less than 1, reducing its value relative to the input. Thus, TFP is very powerful as it creates a proportional increase in output.
2. Technology is "non-rivalrous", meaning that more than one person can use it at any given time. For example, robotics technology is not limited to one person, but a specific robotic arm can only be used by a single person at a time.
3. Technology is "non-excludable", meaning that one person cannot block another from using that factor. Even with the patent system, after expiry, technologies that were once protected now becomes free to use or adapt.

Note that TFP has no intrinsic value by itself, but becomes informative when it is compared across nations. For example, a TFP of 1.4 means nothing. However, if one country has a TFP of 1.8 while the other is 1.4, then we can say that the first country is more effective at utilizing its resources to produce output.

## The Cobb-Douglas Production Function

The Cobb-Douglas Production Function is

$$\begin{aligned}
f(K, L) &= K^\alpha L^\beta \\
 Y &= A \cdot f(K, L) \\
 &= A K^\alpha L^\beta
\end{aligned}$$

where $\alpha$ and $\beta$ are exponents.

A common simplification is that $\beta = 1 - \alpha$. We will later explore the implications of this statement. For now, let us rewrite the above function:

$$
Y = A K^\alpha L^{1 - \alpha}
$$

Note that this is a function of two variables, $K$ and $L$. If we were to plot this function utilizing both variables, we would need a 3D plot with $K$, $L$ and $Y$ each having their own axis. For now, let us gain greater insight of what this function will look like by holding one variable constant and plot the other versus output.

### Capital

For the first case, let us visualize the Cobb-Douglas Production Function with output as a function of capital, holding the amount of labor constant at $\bar L$.

[Following image is a concave upward sloping curve with output increasing with Capital Stock]

```python
plt.figure(figsize=[11,7])
cobb_douglas_plotter_K(1, 0.5, 0.4, cobb_douglas(1, np.arange(0, 1.01, .01), 0.5, 0.4))
```

Notice some of the properties of the function above:

1. It is increasing. This is called increasing returns to capital wherein any increase in capital will lead to an increase in output, assuming that labor is held constant.
2. It is concave (increasing at a decreasing rate). This is called diminishing marginal returns to capital wherein any additional unit of capital will lead to smaller and smaller increases in capital. For a better idea of this, let us take the partial derivative of the Cobb-Douglas function with respect to capital.

$$\begin{aligned}
Y &= A K^\alpha L^{1 - \alpha} \\
\dfrac{\partial Y}{\partial K} &= \alpha A \left ( \dfrac{L}{K} \right )^{1 - \alpha} 
\end{aligned}$$

$\dfrac{\partial Y}{\partial K}$ is called the **marginal product of capital (MPK)**. Let us plot this function, once again holding labor constant at $\bar L$.

[Following image is a convex downward sloping curve with MPK decreasing with Capital Stock]

```python
A = 1
alpha = 0.4
L_bar = 0.5

K_s = np.linspace(0.001, 1, 100)
V_2 = MPK(A, K_s, L_bar, alpha)

plt.figure(figsize=[11,7])
plt.plot(K_s, V_2)
plt.title(fr"MPK with $\bar L$ = {L_bar}, $A$ = {A} and $\alpha$ = {alpha}", size=16)
plt.xlabel("Capital Stock", size=16)
plt.ylabel("MPK or Rental Rate of Capital", size=16);
```

Note that $\text{MPK} \cdot P = R$ is the rental rate of capital less the cost of purchasing or renting an additional unit of capital.

The MPK is monotonically decreasing, converging towards an asymptote at $\text{MPK} = 0$. This means that the rate of increase of output due to an increase in capital will become 0, meaning that the amount of output added per unit of additional capital will become constant. What would be the intuition behind this?

Say a company making pizzas has 1 oven and 10 employees. There is a hard limit on how many pizzas can be baked in a given period of time. However, if the company purchases a second oven, suddenly the employees can bake more pizzas at the same time, thereby increasing the number that can be baked in the same amount of time. In this case, the MPK would be very high as output has greatly increased just by addding slightly to the company's capital stock. 

Let us move to the case when the company has 100 ovens and 10 employees. Adding another oven would do little to increase output as the 10 employees can only do so much - the extra capacity would not be helpful. In this case, the MPK would be very low as output has not increased by much (if at all) even when the company's capital stock increased.

### Labor

We will now move to using the Cobb-Douglas function for output as a function of labor, holding the amount of capital constant at $\bar K$.

[Following image is a concave upward sloping curve with output increasing with Labor]

```python
plt.figure(figsize=[11,7])
cobb_douglas_plotter_L(1, 0.5, 0.4)
```

The properties of the Labor function are similar to that of the capital function. Let us take the partial derivative of the Cobb-Douglas function with respect to labor.

$$\begin{aligned}
Y &= A K^\alpha L^{1 - \alpha} \\
\dfrac{\partial Y}{\partial L} &= A (1 - \alpha) \left ( \dfrac{K}{L} \right )^{\alpha}
\end{aligned}$$

$\dfrac{\partial Y}{\partial L}$ is called the **marginal product of labor (MPL)**. Let us plot this function, once again holding capital constant at $\bar K$.

[Following image is a convex downward sloping curve with MPL decreasing with Labor]

```python
A = 1
K_bar = 0.5
L_s = np.linspace(0.001, 1, 100)
alpha = 0.4
V_4 = MPL(A, K_bar, L_s, alpha)

plt.figure(figsize=[11,7])
plt.plot(L_s, V_4)
plt.title(fr"MPL with $\bar K$ = {K_bar}, $A$ = {A} and $\alpha$ = {alpha}", size=16)
plt.xlabel("Labor", size=16)
plt.ylabel("MPL or Wage", size=16);
```

Note that $\text{MPL} \cdot P = W$, the real wage rate less the cost of hiring an additional unit of labor.

Similar to the MPK, the MPL is monotonically decreasing, converging towards an asymptote at $\text{MPL} = 0$. This means that the rate of increase of output due to an increase in labor will become 0.

Say the same company making pizzas has 5 ovens and 5 employees. One oven per employee seems like overkill but provides significant extra capacity in terms of capital that would give great flexibility for the company when producing pizzas. However, if the company hires 1 more worker, each oven can be utilized more effectively, as another employee can go to prepping pizzas before baking. This greatly increasies the number of pizzas baked in a given unit of time. The MPL would be very high as output increases significantly with the addition of one more worker. On the above graph, we would be on the steep part of the function.

If the company has 5 ovens but 20 employees, hiring an additional worker would do little to increase output. The kitchen would probably be too crowded and there are only so many servers needed. The MPL would be very low as output has not increased by much even when the company's amount of labor has increased. We would be near the flat part of the graph, as the MPL approaches 0.

### Implication for Cross-Country Comparisons

Work by Professors C.W. Cobb and P.H. Douglas found that production or output was a weighted average of the log of capital and labor. The equation for Cobb-Douglas production functions were the result of their research, especially when a log transformation was applied to the equation:

$$\begin{aligned}
Y &= A K^\alpha L^{1 - \alpha} \\
\ln Y &= \ln A + \alpha \ln K + (1 - \alpha) \ln L
\end{aligned}$$

Note that this is exactly the weighted average that Cobb & Douglas found in their empirical findings: capital and labor are weighted by $\alpha$ and $1 - \alpha$ respectively. However, this is still showing production as a function of two variables, $K$ and $L$. Rearranging the equation yields something interesting:

$$\begin{aligned}
\ln Y &= \ln A + \alpha \ln K + \ln L - \alpha \ln L \\
\ln Y- \ln L &= \ln A + \alpha \left ( \ln K - \ln L \right ) \\
\ln \frac{Y}{L} &= \ln A + \alpha \ln \frac{K}{L}
\end{aligned}$$

The Cobb-Douglas function is now an equation in 1 variable: $\ln \frac{K}{L}$. This provides a pathway for comparing values of $A$ and $\alpha$ across countries, and by extension how capital and labor are deployed in different ways between nations. We will explore this idea further in the next section.



--- END 04-production/production.md ---



--- START 04-production/shifts.md ---

---
title: shifts
type: textbook
source_path: content/04-production/shifts.ipynb
chapter: 4
---

```python
from textbook_utils import *
```

# Analyzing Shifts in $A$ and $\alpha$

## Shifts in $A$ and their Effect on Output

First, let us plot a 3D surface of the Cobb-Douglas production function. Output, $Y$ , will go on the vertical (or $z$ ) axis. Capital and labor will go on the $y$ and $x$ axes, resp. The plot below plots the Cobb-Douglas function with $A=2$, also showing $A=1$ for reference.

[Following image is a 3D plot of Output increasing with Capital and Labor]

```python
#change_A(2, filename="fig1.html")
display(HTML("fig1.html"))
```

Supply or total factor productivity shocks could cause $A$ to change. These occur if there is a change in total output for a given level of capital and labor. Examples of these include financial crises, technology shocks, natural environment/distasters and energy prices.

[Following image is an interactive 3D plot of Output increasing with Capital and Labor as A changes]

```python
filename = "fig6.html"

# L_s = np.arange(0, 10.11, 0.1)
# K_s = np.arange(0, 10.11, 0.1)
# alpha = 0.5
# xx, yy = np.meshgrid(K_s, L_s)
# # plot_cobb_douglas(curr_V, orig_cobb_douglas(), fr"A = {A}", r"A = 1", filename=filename)

# layout = go.Layout(
#     title = "Cobb-Douglas Production Function", autosize=False, width=500, height=500, margin = dict(l = 65, r = 50, b = 65, t = 90),
#     scene = dict(xaxis = dict(title = 'K'), yaxis = dict(title = 'L'), zaxis = dict(title = 'Y'))
# )

# fig = go.Figure(layout = layout)

# active = None
# As = []
# for i, A in enumerate(np.arange(0.5, 10.1, 0.5)):
#     As.append(A)
#     if A == 1:
#         active = i
#     V = cobb_douglas(A, xx, yy, alpha)
#     fig.add_trace(go.Surface(
#         z = V, contours = go.surface.Contours(
#             z = go.surface.contours.Z(
#                 show = False, project = dict(z = True)
#             )
#         ), colorscale = "Electric", showscale = False, name=f"A = {A}", visible=False
#     ))
    
# fig.data[active].visible = True

# steps = []
# for i in range(len(fig.data)):
#     step = dict(
#         method="update", args=[
#             {"visible": [False] * len(fig.data) + [True]},
#         ], label=f"A = {As[i]}"
#     )
#     step["args"][0]["visible"][i] = True
#     steps.append(step)
    
# sliders = [dict(
#     active=active,
#     currentvalue={"prefix": ""},
#     pad={"t": 50},
#     steps=steps
# )]

# fig.update_layout(sliders=sliders, width=500, height=600)

# orig_V = orig_cobb_douglas()
# fig.add_trace(go.Surface(
#     z = orig_V, contours = go.surface.Contours(
#         z = go.surface.contours.Z(
#             show = False, project = dict(z = True)
#         )
#     ), colorscale = "Viridis", showscale = False, name="A = 1"
# ))

# plot(fig, filename=filename, auto_open=False, include_mathjax='cdn')

display(HTML(filename))
```

Favorable shocks rotate the production function upward through an increase in A. Thus, each unit of input from capital and labor now simulataneously produce more output. What does this mean for the rental rate of capital and the real wage? Recall the functions for both of them:

$$\begin{aligned}
\text{MPL} &= A (1 - \alpha) \left ( \frac{K}{L} \right )^{\alpha} \\
\text{MPK} &= \alpha A \left ( \frac{L}{K} \right )^{1 - \alpha} 
\end{aligned}$$

Both MPK and MPL will increase by a factor of $A$. Thus, it would be more expensive to hire an additional unit of labor or rent an additional unit of capital. As they are both more productive than previously, they are both more valuable to a business and thus will cost more. Negative shocks do the opposite. They rotate the production function downward through a decrease in $A$. Each unit of input is now less productive, meaning that both the rental rate of capital and the real wage are lower.

## Shifts in $\alpha$ and their Effect on Output

We will now plot what happens to the Cobb-Douglas function as we vary $\alpha$, while holding all other variables constant. The plot below shows $\alpha = 0.8$ (the purple-yellow surface) with $\alpha=0.5$ for reference (the blue-yellow surface). Try and hypothesize what this will do to our production function.

[Following image is a 3D plot of Output increasing with Capital and Labor, with shifts as alpha changes]

```python
#change_alpha(0.8, filename="fig2.html")
display(HTML("fig2.html"))
```

The next plot has $\alpha = 0.2$ (the purple-yellow surface) with $\alpha=0.5$ for reference (the blue-yellow surface). What is the difference between the plot above and the one below?

[Following image is a 3D plot of Output increasing with Capital and Labor, with shifts as alpha changes]

```python
#change_alpha(0.3, filename="fig3.html")
display(HTML("fig3.html"))
```

Below is an interactive plot with a slider to change $\alpha$. Try out different values and see how the shape of the production function changes.

[Following image is an interactive 3D plot of Output increasing with Capital and Labor as alpha changes]

```python
filename = "fig7.html"

# #L_s = np.arange(0, 10.11, 0.1)
# K_s = np.arange(0, 10.11, 0.1)
# A = 1
# xx, yy = np.meshgrid(K_s, L_s)

# layout = go.Layout(
#     title = "Cobb-Douglas Production Function", autosize=False, width=500, height=500, margin = dict(l = 65, r = 50, b = 65, t = 90),
#     scene = dict(xaxis = dict(title = 'K'), yaxis = dict(title = 'L'), zaxis = dict(title = 'Y'))
# )

# fig = go.Figure(layout = layout)

# active = None
# alphas = []
# for i, alpha in enumerate(np.arange(0.1, 1.01, 0.1)):
#     alphas.append(alpha)
#     if alpha == 0.5:
#         active = i
#     V = cobb_douglas(A, xx, yy, alpha)
#     fig.add_trace(go.Surface(
#         z = V, contours = go.surface.Contours(
#             z = go.surface.contours.Z(
#                 show = False, project = dict(z = True)
#             )
#         ), colorscale = "Electric", showscale = False, name=f"alpha = {alpha}", visible=False
#     ))
    
# fig.data[active].visible = True

# steps = []
# for i in range(len(fig.data)):
#     step = dict(
#         method="update", args=[
#             {"visible": [False] * len(fig.data) + [True]},
#         ], label=f"alpha = {alphas[i]}"
#     )
#     step["args"][0]["visible"][i] = True
#     steps.append(step)
    
# sliders = [dict(
#     active=active,
#     currentvalue={"prefix": ""},
#     pad={"t": 50},
#     steps=steps
# )]

# fig.update_layout(sliders=sliders, width=500, height=600)

# orig_V = orig_cobb_douglas()
# fig.add_trace(go.Surface(
#     z = orig_V, contours = go.surface.Contours(
#         z = go.surface.contours.Z(
#             show = False, project = dict(z = True)
#         )
#     ), colorscale = "Viridis", showscale = False, name="alpha = 0.5"
# ))

# plot(fig, filename=filename, auto_open=False, include_mathjax='cdn')

display(HTML(filename))
```

$\alpha$ and $\beta$ are called the output elasticities of capital and labor, respectively. They measure the responsiveness of output to a change in the levels of either labor or capital, holding all else constant. This means that if $\alpha$ or $\beta$ were high, then any small increase in their respective input would lead to a relatively large increase in output. As an example, if $\alpha$ was 0.4, then a 1% increase in capital would lead to a 0.4% increase in output.

Note, we assume that there are constant returns to scale. Thus, an increase in $\alpha$ necessarily means $\beta$ decreases. This reveals something important when comparing countries: the higher the $\alpha$, the more capital-intensive the country's production is. This means that $\alpha$ and $\beta$ give economists and policymakers insight as to how resources are allocated across nations.

## Returns to Scale

The significance of the exponents adding up to 1 ($\alpha + \beta = 1$) is that this implies constant returns to scale. If all inputs are scaled by a common non-zero factor, the output will be scaled by that same factor. Below is a generalization of this:

$$\begin{aligned}
Y &= A (c \cdot K)^\alpha (c \cdot L)^{1 - \alpha} \\
&= A c^\alpha K ^ \alpha c^{1 - \alpha}L^{1 - \alpha} \\
&= A c^{\alpha + 1 - \alpha}K^\alpha L^{1 - \alpha} \\
&= c \cdot A K^\alpha L^{1 - \alpha}
\end{aligned}$$

Thus, any increase in either of the inputs will lead to a 1-1 increase in output. This is a significant assumption to make, as it essentially incentivizes companies to continue to "scale" their production inputs. They are not losing out on how much return is produced - they are getting output that matches exactly what they put into production.

The alternative case is when $\alpha + \beta < 1$. This is called decreasing returns to scale, and occurs when a company scales their production inputs by a factor of c, but gets a scaling in output that is less than c.

The last case is the most profitable one, when $\alpha + \beta > 1$. This is called increasing returns to scale, and occurs when a company increases their production inputs by c, but gets an increase in output that is greater than c.

Let us visually examine how values of $\alpha$ and $\beta$ affect output.

```python
#change_alpha_beta(2, filename="fig4.html")
display(HTML("fig4.html"))
```

The above graph shows increasing returns to scale with $\alpha + \beta = 2$ (the purble-yellow surface) with constant returns to scale for comparison (the blue-yellow surface). Notice how the orange function no longer increases at a decreasing rate, but seems to mimic exponential growth. This is once again because of the definition of increasing returns to scale. As companies continue to increase their inputs by a factor of $c$, they see their output increase by more than that factor. Thus, as inputs $(K, L)$ increase, output will increase at an even faster rate than that - in this case by the square.

```python
#change_alpha_beta(0.5, filename="fig5.html")
display(HTML("fig5.html"))
```

The above graph exhibits decreasing returns to scale as $\alpha + \beta = 0.5$ (the purple-yellow surface) with constant returns to scale for comparison (the blue-yellow surface). This time, the orange production function flattens out far faster than the regular constant returns to scale function. You can prove this to yourself using the slider below, which adjusts the value of $\alpha + \beta$.

```python
filename = "fig8.html"

# L_s = np.arange(0, 10.11, 0.1)
# K_s = np.arange(0, 10.11, 0.1)
# A = 1
# xx, yy = np.meshgrid(K_s, L_s)

# layout = go.Layout(
#     title = "Cobb-Douglas Production Function", autosize=False, width=500, height=500, margin = dict(l = 65, r = 50, b = 65, t = 90),
#     scene = dict(xaxis = dict(title = 'K'), yaxis = dict(title = 'L'), zaxis = dict(title = 'Y'))
# )

# fig = go.Figure(layout = layout)

# active = None
# default = 1
# ab_sums = []
# for i, ab_sum in enumerate(np.arange(0.1, 3.01, 0.1)):
#     ab_sums.append(ab_sum)
#     if ab_sum == default:
#         active = i
#     alpha, beta = ab_sum / 2, ab_sum / 2
#     V = cobb_douglas(A, xx, yy, alpha, beta)
#     fig.add_trace(go.Surface(
#         z = V, contours = go.surface.Contours(
#             z = go.surface.contours.Z(
#                 show = False, project = dict(z = True)
#             )
#         ), colorscale = "Electric", showscale = False, name=f"alpha + beta = {ab_sum}", visible=False
#     ))
    
# fig.data[active].visible = True

# steps = []
# for i in range(len(fig.data)):
#     step = dict(
#         method="update", args=[
#             {"visible": [False] * len(fig.data) + [True]},
#         ], label=f"alpha + beta = {ab_sums[i]}"
#     )
#     step["args"][0]["visible"][i] = True
#     steps.append(step)
    
# sliders = [dict(
#     active=active,
#     currentvalue={"prefix": ""},
#     pad={"t": 50},
#     steps=steps
# )]

# fig.update_layout(sliders=sliders, width=500, height=600)

# orig_V = orig_cobb_douglas()
# fig.add_trace(go.Surface(
#     z = orig_V, contours = go.surface.Contours(
#         z = go.surface.contours.Z(
#             show = False, project = dict(z = True)
#         )
#     ), colorscale = "Viridis", showscale = False, name=f"alpha + beta = {default}"
# ))

# plot(fig, filename=filename, auto_open=False, include_mathjax='cdn')

display(HTML(filename))
```



--- END 04-production/shifts.md ---



--- START 05-utility/budget-constraints.md ---

---
title: budget-constraints
type: textbook
source_path: content/05-utility/budget-constraints.ipynb
chapter: 5
---

```python
import pandas as pd
import numpy as np
import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')
```

# Budget Constraints and Utility Maximization

In this section, we will assume that $\alpha = 0.5$ (i.e. the utility function is: $u(x_1, x_2) = x_1^{0.5}x_2^{0.5}$).

Now we introduce the concept of money into our model. Consumers face a budget constraint when choosing to maximize their utility. Given an income $M$ and prices $p_1$ for good $x_1$ and $p_2$ for good $x_2$, the consumer can at most spend up to $M$ for both goods:

$$M \geq p_1x_1 + p_2x_2$$

Since goods will always bring non-negative marginal utility, consumers will try to consume as many goods as they can. Hence, we can rewrite the budget constraint as an equality instead (since if they have more income leftover, they will use it to buy more goods).

$$M = p_1x_1 + p_2x_2$$

This means that any bundle of goods $(x_1,x_2)$ that consumers choose to consume will adhere to the equality above. What does this mean on our graph? Let's examine the indifference curve plots, assuming that $M = 32$, and $p_1 =2$ and $p_2 = 4$. 

[Following image is s graph of a series of nested convex Indifference Curves with a budget constraint ]

```python
M = 32
p_1 = 2
p_2 = 4

# Plot default indifference curves
utilities = np.arange(1, 9)
x1_indiff_val = np.linspace(0,50,1000)
x2_indiff_vals = []
for u in utilities:
    x2_indiff_vals.append(((u/(x1_indiff_val ** (1/2))) ** (2)))
traces = []
colors = ['blue', 'red','green','purple'] + ['blue', 'red','green','purple']
for u,c,x2 in zip(utilities,colors,x2_indiff_vals):
    traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2,
    name = 'utility = ' + str(u),
    line = dict(color = c,width = 1)))
    
# for i in range(len(traces) - 4):
#     del traces[-1] # This is a hacky method to not continually append to TRACES upon an update from the slider.
x2_bc_val = (M - (p_1*x1_indiff_val))/p_2
traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2_bc_val,
    name = 'Budget Constraint',
    line = dict(color = 'black',width = 1,dash="dot")))
data = traces
layout = dict(title = 'Budget Constraint and Indifference Curves for the Cobb-Douglas Utility Function (alpha = 0.5)',
              xaxis = dict(title = 'X1', range = [0,18]),
              yaxis = dict(title = 'X2', range = [0,10]),)
fig = dict(data=data, layout=layout)

plot(fig, filename="fig3.html", auto_open=False)
display(HTML("fig3.html"))
```

The budget constraint is like a possibilities curve: moving up or down the constraint means gaining more of one good while sacrificing the other.

Let's take a look at what this budget constraint means. Because of the budget constraint, any bundle of goods $(x_1,x_2)$ that consumers ultimately decide to consume will lie on the budget constraint line. Adhering to this constraint where $M=32, p_1 = 2, p_2 = 4$, we can see that consumers will be able to achieve 2 units of utility, and can also achieve 4 units of utility. But what is the maximum amount of utility that consumers can achieve? 

Notice an interesting property about indifference curves: **the utility level of the indifference curves gets larger as we move up and to the right.** Hence, the maximizing amount of utility in this budget constraint is the rightmost indifference curve that still touches the budget constraint line. In fact, it'll only 'touch' (and not intersect) the budget constraint and be tangential to it. 

[Following image is s graph of a series of nested convex Indifference Curves with a budget constraint ]

```python
M = 32
p_1 = 2
p_2 = 4

# Plot default indifference curves
utilities = np.arange(1, 9)
x1_indiff_val = np.linspace(0,50,1000)
x2_indiff_vals = []
for u in utilities:
    x2_indiff_vals.append(((u/(x1_indiff_val ** (1/2))) ** (2)))
traces = []
colors = ['blue', 'red','green','purple'] + ['blue', 'red','green','purple']
for u,c,x2 in zip(utilities,colors,x2_indiff_vals):
    traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2,
    name = 'utility = ' + str(u),
    line = dict(color = c,width = 1)))

# PLOT BC
x2_bc_val = (M - (p_1*x1_indiff_val))/p_2
traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2_bc_val,
    name = 'Budget Constraint',
    line = dict(color = 'black',width = 1,dash="dot")))


# PLOT MAX UTIL INDIFF CURVE
max_utility = ((1/2*M/p_1) ** (1/2)) * ((1/2*M/p_2) ** (1/2))
x2_max_util = (max_utility/(x1_indiff_val ** (1/2))) ** 2
x2_max_util = (max_utility/(x1_indiff_val ** (1/2))) ** 2
traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2_max_util,
    name = 'Maximized Utility = ' + str(round(max_utility, 2)),
    line = dict(color = 'black',width = 2)))
data = traces

layout = dict(title = 'Budget Constraint and Indifference Curves for the Cobb-Douglas Utility Function (alpha = 0.5)',
              xaxis = dict(title = 'X1', range = [0,20]),
              yaxis = dict(title = 'X2', range = [0,15]),)
fig = dict(data=data, layout=layout)

plot(fig, filename="fig4.html", auto_open=False)
display(HTML("fig4.html"))
```

Notice that as the price of one good increases, the indifference curve that represents the maximum attainable utility shifts towards the left (i.e. the max utility decreases). Intuitively, this makes sense. As the price of one good increases, consumers have to make adjustments to their consumption bundles and buy less of one, or both, goods. Hence, their maximum utility will decrease.

Let's visualize the budget constraint in 3D where $M=30, p_1=3, p_2=3$. Here, any point along the curve in which the 2 planes intersect represents an amount of utility in which the budget constraint holds true (i.e. where we've spent all our income). The utility maximizing quantity is a point on this intersecting curve at which the utility level is the highest.

[Following image is an interactive 3D plot of Utility increasing with X1 and X2, wih a plane representing the budget constraint]

```python
def cobb_douglas(x1, x2):
    return (x1 ** (1/2)) * (x2 ** (1/2))
x1 = np.linspace(0,10,10)
x2 = np.linspace(0,10,10)
X1,X2 = np.meshgrid(x1,x2)
z = cobb_douglas(X1,X2)

def budget_constraint(x1, x2):
    return 10000*(3*x1 + 3*x2 - 30) # We multiply this by 10000 to get a very steep plane, which should be similar to the actual BC, a vertical plane.

z2 = budget_constraint(X1, X2)

data = [go.Surface(
    z=z, contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=True,highlightcolor="#42f462",
                                                              project=dict(z=True))), name="Cobb-Douglas Utility Function"),
       go.Surface(
   z=z2, contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=False,
                              highlightcolor="#42f462",project=dict(z=True))),showscale=False, colorscale="balance", name="Budget Constraint")]
layout = go.Layout(
    title='Cobb-Douglas Utility Function with Budget Constraint', autosize=False,width=500, height=500, margin=dict(l=65,r=50,b=65,t=90),
    scene = dict(xaxis = dict(title='X1', range = [0,10]), yaxis = dict(title='X2'),
    zaxis = dict(title = 'Utility', nticks=4, range = [0,10],)))
fig = go.Figure(data=data, layout=layout)

plot(fig, filename="fig5.html", auto_open=False)
display(HTML("fig5.html"))
```



--- END 05-utility/budget-constraints.md ---



--- START 05-utility/index.md ---

---
title: index
type: textbook
source_path: content/05-utility/index.md
chapter: 5
---

# Utility

**Student Learning Outcomes:**

* Understand the basic principles of utility 
* Become familiar with utility function such as the Cobb-Douglas utility function
* Translate utility functions with 2 inputs to indifference curves
* Understand budget constraints and how they affect utility maximization


--- END 05-utility/index.md ---



--- START 05-utility/utility.md ---

---
title: utility
type: textbook
source_path: content/05-utility/utility.ipynb
chapter: 5
---

```python
import pandas as pd
import numpy as np
import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
from IPython.display import display, HTML
import warnings
warnings.filterwarnings('ignore')
```

# Utility Functions and Indifference Curves

## What is Utility?

When we consume a good, we assume that the good will have some impact on our total utility. Utility is a fundamental measure that helps economists model how consumers make decisions. An assumed rule in economics is that consumers will always act rationally, which translates to the assumption that consumers will always attempt to maximize their own utility. 

It is important to note that utility doesn't have specified units and even the face value of utility doesn't have any meaning. *What does an apple providing 5 utility units even mean?* What is valuable, however, is that utility can be compared; if an apple provides 5 utility units and an orange provides 3 utility units, then we prefer apples to oranges.

As a very simple example, say Anne has 6 dollars and she can choose to buy any combination of goods A and B. If good A costs 2 dollars and provides 5 utility units per unit of A consumed, while good B costs 3 dollars and provides 6 utility units per unit of B consumed, then Anne will buy 3 units of good A, since that maximizes her utility. 

In economics, however, our models are a little more complex than that. Typically, utility is the product of the consumption of many goods; typically having a lot of one good but not another does not provide much utility. In addition, consumption of one good faces diminishing marginal returns, i.e. holding all things equal, the consumption of one additional unit of a good will provide less utility than the utility received from the previous unit. Intuitively, imagine Bob is very hungry and decides to eat slices of pizza. The first slice of pizza will bring Bob the most utility, but the 8th slice will be much less satisfying to eat.

## Utility Functions
A consumer's utility is determined by the amount of consumption from all the goods they consume. Typically, utility functions are multivariate: they take in multiple inputs (which represent the different amounts of consumption for each good, which we call a consumption bundle), and output one value, the utility. Today, we'll only look at the case where consumers can only choose between 2 goods $x_1$ and $x_2$. Hence, a utility function can be represented by: $u(x_1,x_2)$. 

With that in mind, let's start graphing some utility functions!

### Cobb-Douglas Utility Function

Consider the following utility function across $x_1$ and $x_2$:

$$u(x_1, x_2)=x_1^{\alpha}x_2^{1-\alpha}\quad\text{where } 0<\alpha<1$$

This is known as the **Cobb-Douglas utility function**. To visualize this function, we'll need a 3D plot. 

[Following image is an interactive 3D plot of Utility increasing with X1 and X2]

```python
alpha = 0.5
def cobb_douglas(x1, x2):
    return (x1 ** alpha) * (x2 ** (1-alpha))
x1 = np.linspace(0,10,10)
x2 = np.linspace(0,10,10)
X1,X2 = np.meshgrid(x1,x2)
z = cobb_douglas(X1,X2)
data = [go.Surface(z=z, contours=go.surface.Contours(z=go.surface.contours.Z(show=True,usecolormap=True,highlightcolor="#42f462",project=dict(z=True))))]
layout = go.Layout(title='Cobb-Douglas Utility Function (alpha = 0.5)',autosize=False,width=500,height=500,margin=dict(l=65,r=50,b=65,t=90),
scene = dict(xaxis = dict(title='X1'),yaxis = dict(title=r'X2'),zaxis = dict(title='Utility'),))
fig = go.Figure(data=data, layout=layout)

plot(fig, filename="fig1.html", auto_open=False)
display(HTML("fig1.html"))
```

## Examining the Utility Function

There are 2 rules that utility functions generally follow: 

- Non-negative marginal utility: the consumption of a good will not decrease the utility. Economists generally assume that 'more is better.' If the consumption of a good decreased utility, then we would consume less of a good. 
- Diminishing marginal returns: all else equal, as consumption increases the marginal utility derived from each additional unit declines.

### Non-negative Marginal Utility
Say we are currently consuming 2 units of $x_1$ and $x_2$ each with $\alpha = \frac{1}{2}$, providing $u(2,2)=2^{0.5}2^{0.5}=2$ utility units. One additional unit of $x_1$ will provide me a higher point of utility: we can verify this result both graphically and numerically: $u(3,2)=3^{0.5}2^{0.5}\approx2.45$. Indeed, consuming one more unit of a good should increase our utility!

### Marginal Utility and the Law of Diminishing Returns
Now let's check for the second result: diminishing marginal returns. From above, we know that holding the consumption of $x_2$ constant at 2, going from 2 to 3 units of $x_1$ increases our utility by $2.45-2=0.45$. Going from 3 to 4 units of $x_1$ brings our utility to $u(4,2)=4^{0.5}2^{0.5}\approx 2.83$, an increase of $2.83-2.45=0.38$ utility units.

Using calculus, we can more formally define the marginal utility of a good. Since marginal utility is the change in utility that one additional unit of consumption provides (holding all others constant), the marginal utility with respect to $x_1$ is its partial derivative: $\frac{\partial u}{\partial x_1}$. In our case:

$$
\begin{aligned}
\textrm{Marginal Utility of } x_1: &\quad\frac{\partial u}{\partial x_1} = \frac{1}{2}x_1^{-0.5}x_2^{0.5} \\
\textrm{Marginal Utility of } x_2: &\quad\frac{\partial u}{\partial x_2} = \frac{1}{2}x_1^{0.5}x_2^{-0.5}
\end{aligned}
$$

Or, more generally,

$$\begin{aligned}
\textrm{Marginal Utility of } x_1: &\quad\frac{\partial u}{\partial x_1} = \alpha x_1^{\alpha-1}x_2^{1-\alpha} \\
\textrm{Marginal Utility of } x_2: &\quad\frac{\partial u}{\partial x_2} = (1-\alpha) x_1^{\alpha}x_2^{-\alpha}
\end{aligned}$$


With marginal utility defined, note that both conditions can be explained using the marginal utility function $\frac{\partial u}{\partial x}$: 

- Non-negative marginal utility: $\frac{\partial u}{\partial x} \geq 0$
- Diminishing marginal returns: $\frac{\partial^2 u}{\partial x^2} < 0$

## Indifference Curves

Although the utility function above in 3D is cool, you'll typically find utility graphs to be in 2D with $x_1$ and $x_2$ as the axis (eliminating the utility axis). 

To represent utility levels, we plot a set of indifference curves on the 2D graph. An indifference curve satisfies the property in which **any point on the curve has the exact same amount of utility**, so that consumers are _indifferent_ to any point on the curve. In our 3D plot, any point on the indifference curve has the exact same height, which represents the value of utility. If you're familar with contour plots, you can also think of indifference curves as following the same idea. 

[Following image is s graph of a series of nested convex Indifference Curves for different amounts of X1 and X2 ]

```python
alpha = 0.5
utilities = np.arange(1, 9)
x1_indiff_val = np.linspace(0,50,1000)
x2_indiff_vals = []
for u in utilities:
    x2_indiff_vals.append(((u/(x1_indiff_val ** alpha)) ** (1/(1-alpha))))
traces = []
colors = ['blue', 'red','green','purple'] + ['blue', 'red','green','purple']
for u, c, x2 in zip(utilities, colors, x2_indiff_vals):
    traces.append(go.Scatter(
    x = x1_indiff_val,
    y = x2,
    name = 'utility = ' + str(u),
    line = dict(color = c,width = 1)))

data = traces

# Edit the layout
layout = dict(title = 'Indifference Curves for the Cobb-Douglas Utility Function (alpha = ' + str(alpha) + ')',
              xaxis = dict(title = 'X1', range = [0,10]),
              yaxis = dict(title = 'X2', range = [0,10]),)

fig = dict(data=data, layout=layout)

plot(fig, filename="fig2.html", auto_open=False)
display(HTML("fig2.html"))
```



--- END 05-utility/utility.md ---



--- START 06-inequality/.ipynb_checkpoints/historical-inequality-checkpoint.md ---

---
title: historical-inequality-checkpoint
type: textbook
source_path: content/06-inequality/.ipynb_checkpoints/historical-inequality-checkpoint.ipynb
chapter: 6
---

```python
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from datascience import *
%matplotlib inline
plt.style.use('seaborn-muted')
mpl.rcParams['figure.figsize'] = (10.0, 10.0)
```

# Income Inequality Historically

<!-- Written by Amal Bhatnagar -->

In the last chart on the previous page, you may have noticed that income inequality was rising in the United States in the last few decades. We will examine this in more detail, and also observe global trends in inequality.

## The United States

Let's look at historical trends of income inequality in the US over the last 100 years. The data has been collected from [The World Inequality Database](https://wid.world/), which is co-directed by Berkeley Economics professors Emanuel Saez and Gabriel Zucman. Specifically, we will observe income distributions for the bottom 50 percent, top 10 percent, and top 1 percent.

```python
us_hist = Table.read_table("US_inequality.csv")
us_hist.show(5)
```

```python
us_hist.take(np.arange(100,105))
```

Let's begin with some data cleaning: it seems like our 3 brackets are 'vertically stacked' on top of each other. Instead, we would like a table with 5 columns: `Year`, `bottom 50% income share`, `top 10% income share`, and `top 1% income share`.

```python
bottom_50_us = us_hist.where("Percentile", "p0p50").drop("Percentile").relabeled("Income Share", "Bottom 50% Share")
top_10_us = us_hist.where("Percentile", "p90p100").drop("Percentile").relabeled("Income Share", "Top 10% Share")
top_1_us = us_hist.where("Percentile", "p99p100").drop("Percentile").relabeled("Income Share", "Top 1% Share")
us_hist_joined = bottom_50_us.join("Year", top_10_us).join("Year", top_1_us)
us_hist_joined
```

Oh no, there are some `nan` values! NaN (not a number) values are very common in real world datasets: often, we may not have some observations simply because no data was collected, or perhaps the data collected was faulty. Sometimes, we can try to impute or replace NaN values in order to avoid having gaps in our data, but for now let's ignore NaNs and when plotting to see what's going on:

```python
# mpl.rcParams['figure.dpi'] = 120
us_hist_joined.plot("Year", width=11, height=7)
plt.title("Income Share over Time", fontsize = 16)
plt.ylabel("Proportion", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

# Income Inequality for the Rest of the World

Now let's examine the trends of income inequality in other parts of the world.

```python
world_hist = Table.read_table("World_Inequality.csv")
bottom_50_world = world_hist.where("Percentile", "p0p50").drop("Percentile")
top_10_world = world_hist.where("Percentile", "p90p100").drop("Percentile")
top_1_world = world_hist.where("Percentile", "p99p100").drop("Percentile")
top_10_world
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

Just like the US, it seems global inequality has been rising around the world, especially in China, India, Russia, and across Europe. However, in absolute terms, the level of income inequality in Europe is much lower than that in the United States. 

Also look at Russia: income inequality spiked up around 1991. This was likely caused by the fall of the USSR: the failing Soviet state left the ownership of state assets uncontested, which allowed former USSR officials to acquire state property through informal deals. This led to the rise of many Russian oligarchs - those who rapidly accumulated wealth during the era of Russian privatization directly follwing the dissolution of the Soviet Union.

```python
top_10_world.select("Year", "USA", "Europe").plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

##  The Elephant Graph

```{figure} elephant_curve.jpg
---
width: 500px
name: elephant-curve
---
The elephant curve {cite}`10inequality-elephantCurve`
```

The elephant curve is a graph that shows the real income growth per adult across each income group’s percentile around the world. 

There are 3 key features of the elephant curve: a hump for the world’s poorest, valley for the middle class, and trunk for the upper class. The thump is made of the world’s poorest countries, most likely those from developing countries. The valley comprises the working class from the developed world and upper class from developing countries. The trunk is made of people from the upper class from developed countries. The hump and valley indicate growth among emerging countries, and the top global 1%’s growth is higher than any other income group, thus explaining the positively sloped shape of the trunk. 


A study done by the Brookings Institution] found that “poorer countries, and the lower income groups within those countries, have grown most rapidly in the past 20 years” {cite}`10inequality-brookings`. This supports the World Bank’s claim that inequality between countries and within countries is decreasing. The Brookings Institution used only household surveys, however, which usually excludes the top and bottom percentile of the population, due to non-response bias. Still, the study is useful in corroborating the trends and growth in global income inequality.

# Factors that Affect Income Inequality

Economists have isolated multiple factors that influence a country's income inequality
- top marginal tax rates
- unemployment rates
- population growth

We will look at each of these scenarios independently and see its overall trends

## Top marginal tax rates

Let's also take a look at the top marginal tax rates in the United States throughout this time. Overall, the United States (and most of the rest of the world) has a progressive tax system, which means that the more income you earn, the higher percentage you will be taxed at. A good way to reduce income inequality is through progressive taxation; having the richer paying a higher portion of their income will help increase equality. Currently, the top marginal tax rate is 37%, as we can see in the table below.

```{figure} MTR.png
---
width: 500px
name: irs
---
Marginal tax rates. Image from the IRS
```

The top marginal tax rate only applies to the portion of your income above a certain income level. For example, if you earned 19501 dollars in 2019, then you will pay 1940 dollars plus 12% of $19501-19401$, i.e. 12 dollars. For another example, if you earned 80000 dollars, then you will pay $9086 + 0.22(80000-78950) = 9317$ dollars in tax, effectively a $\frac{9317}{80000} = 11.6\%$ tax rate.

In general, the idea is you will pay a lower tax rate for your first $x$ dollars, but a higher rate for dollars earned over $x$.

Now let's look at the historical trends in marginal top tax rates, which is the % taxed at the highest tax bracket.

```python
toptax = Table.read_table("toptaxrate.csv")
toptax
```

```python
# mpl.rcParams['figure.dpi'] = 120
toptax.plot(0,1, width=11, height=7)
plt.title("Top Marginal Tax Rate over Time", fontsize = 18)
plt.show()
```

```python
# mpl.rcParams['figure.dpi'] = 120
us_hist_joined.plot("Year", width=11, height=7)
plt.title("Income Share over Time", fontsize = 18)
plt.ylabel("Proportion", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

This graph depicts income inequality decreasing between 1910 and 1970 and increasing from 1970 to present. 

In 1913, Congress implemented the current income tax to promote equality. Originally meant to help compensate for revenue lost from reducing high tariffs, the new policy essentially made the top 1% start contributing to taxes. Additionally, the top marginal tax rate increased from 7% in 1913 to 73% in 1918, thus helping reduce income inequality. Right before the Great Depression, income inequality peaked, where the richest 1% possessed 19.6% of all income. During the Great Depression, top marginal tax rates increased, peaking at 94% in 1944. The top marginal tax rate decreased but remained high over subsequent decades, where it was 70% in 1965 and 50% in 1982. These high top marginal tax rates are correlated with low income inequality. During the Great Depression, the richest 1% had about 15% of total income. For the 10 years after the Great Depression, the top 1% had below 10% of total income and 8% for the 30 years afterwards. This period was known as the Great Compression, as income differentials between the top 1% and the rest of the country decreased.

In the 1970s, the economy took a turn for the worse with high unemployment and inflation (stagflation) and low growth. In order to stimulate economic growth, the government reduced top marginal tax rates (70% to 38.5% in 1980s), deregulated corporate institutions, and attacked labor union memberships (membership decreased by half within 30 years). Although these policies improved economic growth, it resulted in higher income inequality.

The graph below better shows that the share of income earned by the bottom 50% percentile steadily decreased, while the share earned by the top 1% increased steadily. This means that the top 1% has more wealth than the entire bottom 50% of the population. Suppose a class of 100 people has \$100 in aggregate. In a world with perfect equality, each person would have \$1. With this current level of income inequality, one person would have more wealth than 50 people combined. 

The media continues to report on the nation's significant income disparity. [The Washington Post wrote a story](https://www.washingtonpost.com/business/2019/09/26/income-inequality-america-highest-its-been-since-census-started-tracking-it-data-show/) and found that “The number of families earning \$15,000 or less has fallen since 2007, according to the latest census data, while the number of households bringing in \$250,000 a year or more has grown more than 15 percent.” 

Can we conclude that high marginal tax rates lead to low income inequality but slow economic growth?

## Unemployment rates

Economists believe that unemployment is one of the leading factors that leads to income inequality. When looking at what influences the Gini coefficient, a paper from [Princeton](https://rpds.princeton.edu/sites/rpds/files/media/menendez_unemployment_ar.pdf) found that the unemployment rate had the largest effect on the income inequality rates

Below, we look at the unemployment rates for the past 20 years across many different countries. These are the same countries and regions that we will further study below.

```python
unemployment = Table.read_table("Unemployment.csv")
unemployment
```

As we can see from the graph, the unemployment rates for China, India and the rest of the world have stayed somewhat steady. On the other hand, Brazil, the US, Russia and Europe are encountering drastically different unemployment situations than before.

```python
# mpl.rcParams['figure.dpi'] = 120
unemployment.plot("Year", width=11, height=7)
plt.ylabel("Unemployment Rate (%)", fontsize = 14)
plt.xlabel("Year", fontsize =14)
plt.title("Unemployment Rate over Time", fontsize = 18)
plt.show()
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", size=14)
plt.xlabel("Year", fontsize=14)
plt.title("International Income Inequality over Time", fontsize=18)
plt.show()
```

The graphs above show a positive correlation between unemployment and income inequality. As unemployment increases, income inequality also increases. In 2011, Hanan Morsy, an Egyptian economist who serves as the Director of Macroeconomic Policy, Forecasting and Research at the African Development Bank, actually researched this topic {cite}`10inequality-morsy`. Her group examined the member nations of the Organization  for  Economic  Cooperation  and  Development  (OECD) between 1980 and 2005. She found that specific groups that were vulnerable to the economic shocks that led to an increase in income inequality:
- young workers
- low-skilled workers
- workers who had been out of work for a long time

Her solution was to increase job creation opportunities for temporary, recently fired, and recently hired workers, provide job assistance and training to prevent long-term unemployment, and improve incentives for working by aligning incentives with productivity of labor. Morsy's research found that the most vulnerable groups to economic shocks were young, low-skilled and temporary workers. Creating opportunities for these different demographics would help them be more protected from potential shocks and thus decrease income inequality.

## Population growth

As the number of people in a country's population increase, it becomes more difficult for a country to distribute its public goods to everyone. This leads to many social consequences in which resources are not fairly distributed to all members of the population, cause inaccessibility for different parts of the population

The table below shows how the population growth has changed for the same countries we saw above. We are only looking at data for the past 10 years.

```python
pop_growth = Table.read_table("Population Growth.csv")
pop_growth
```

```python
pop_growth.plot("Year", width=11, height=7)
plt.title("Population Growth over Time", fontsize = 18)
plt.ylabel("Population Growth (%)", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("International Income Inequality over Time", fontsize=18)
plt.show()
```

The graphs above show that most countries high population growth between 1-2% during the 1980's. The effects of this can be seen in the rising income inequality during the 90's.  

Recent research by University of Toronto's Marijn Bolhuis and Univeristy of Oxford's Alexandra de Pleijt shows that there is a strong correlation between a country's population growth (measured by birth rates) and its income inequality {cite}`10inequality-popGrowth`. Their most recent study in 2016 analyzed income inequality and birth rates data between 1870 and 2000 across 67 countries. They concluded that if a country had 50% higher income inequality, then that country's birth rate would be about twice as high as another country with the same level of economic development. Bolhuis says that these higher birth rates mean that economic growth has to be equal to or greater than the birth rate to offset the implications of higher birth rates. 

This is part of a larger debate about the relationship between birth rates and income inequality. Economist Thomas Piketty finds that low birth rates, rather than high birth rates, are causing today's income inequality. With lower birth rates, fewer children per couple are being borne, so these children get more of their parents' inheritance.



--- END 06-inequality/.ipynb_checkpoints/historical-inequality-checkpoint.md ---



--- START 06-inequality/.ipynb_checkpoints/inequality-checkpoint.md ---

---
title: inequality-checkpoint
type: textbook
source_path: content/06-inequality/.ipynb_checkpoints/inequality-checkpoint.ipynb
chapter: 6
---

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datascience import *
%matplotlib inline
plt.style.use('seaborn-muted')
```

# Measurements of Inequality

## The Lorenz Curve
The Lorenz Curve visually presents income inequality by plotting household income percentile on the $x$-axis, and the cumulative income share that the bottom $x$ percentile own on the $y$-axis. The households are sorted by income, so that the first household at the 0th percentile has the least amount of income, while the household at the 100th percentile has the greatest income.

For any point $(x,y)$ on the Lorenz curve, “the bottom x percent own y% of the income”. For example, if the $x$-axis reads 0.30 and $y$-axis reads 0.10, then it means that the bottom 30% of the population received 10% of the total population's income. This yields 2 implications for the Lorenz Curve:
- The points $(0,0)$ and $(1,1)$ are always on the curve. $(0,0)$ represents the 0% of the population owning 0% of the income and $(1,1)$ represents 100% of the population owning 100% of the income.
- The slope is always increasing. This is because households are sorted by income as percentiles: for a slight increase in $x$, households become richer and hence provide a larger share of total income.

### Line of Perfect Equality
In a world of perfect equality, everyone would have the exact same income. In this case, the Lorenz curve would just be a 45-degree straight line that runs through $(0,0)$ and $(1,1)$, i.e. $y=x$. Mathematically, this is because the derivative is constant: for a slight increase in $x$, the total share of income increases at a constant rate. Another way to think about this is that the bottom 10% of the population will own 10% of the total income, the bottom 50% of the population will own 50% of the total income, and so on. This line is known as the *line of perfect equality*, and we typically display this line when plotting our Lorenz curves as a reference.

### A Toy Example
Let's suppose country 1 has the following income distribution: 
- The bottom 10% owns a cumulative 2% of total income 
- The bottom 20% owns 5% of total income
- The bottom 30% owns 9% of total income
- The bottom 40% owns 15% of total income 
- The bottom 50% owns 23% of total income
- The bottom 60% owns 33% of total income
- The bottom 70% with 45% of total income
- The bottom 80% with 59% of total income 
- The bottom 90% with 75% of total income
- The bottom 100% with 100% of total income

We will create an array of income shares and call it `Country1`.

```python
Country1 = make_array(0, 0.02, 0.05, 0.09, 0.15, 0.23, 0.33, 0.45, 0.59, 0.75, 1.0)
```

To better see this information, we will create a table containing population percentage and cumulative income share.

```python
income_distribution = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country1 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution
```

How will the Lorenz Curve for this income distribution look?

```python
income_distribution.scatter(0, 1, width=11, height=7)
plt.plot([0,100], [0,100], color='k');
```

### Comparing Lorenz Curves
Now let's compare 2 countries' Lorenz curves. Suppose country 2 has the following income distribution:
- The bottom 10% owns a cumulative 3% of total income 
- The bottom 20% owns 7% of total income
- The bottom 30% owns 13% of total income
- The bottom 40% owns 19% of total income 
- The bottom 50% owns 27% of total income
- The bottom 60% owns 37% of total income
- The bottom 70% with 50% of total income
- The bottom 80% with 65% of total income 
- The bottom 90% with 81% of total income
- The bottom 100% with 100% of total income

```python
Country2 = make_array(0, 0.03, 0.07, 0.13, 0.19, 0.27, 0.37, 0.5, 0.65, 0.81, 1.0)
income_distribution2 = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country2 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution2
```

Comparing the 2 countries' income distributions side by side:

```python
income_distribution.join(
    ["Population Percentage (%)", "Perfect Equality Income Share (%)"], 
    income_distribution2, ["Population Percentage (%)", "Perfect Equality Income Share (%)"]
).relabel(
    "Cumulative Income Share (%)", "Country 1 Cumulative Income Share (%)"
).relabel(
    "Cumulative Income Share (%)_2", "Country 2 Cumulative Income Share (%)"
)
```

Plotting both countries' Lorenz curves, can you tell which country has a higher level of income inequality?

```python
plt.figure(figsize=[7,7])
plt.plot(income_distribution.column(0), income_distribution.column(1), "-o", c = 'b')
plt.plot(income_distribution.column(0), income_distribution2.column(1), "-o", c = 'r')
plt.legend(["Country 1", "Country 2"])
plt.plot([0,100], [0,100], color='k');
```

In this case, we can see that country 2's Lorenz curve is closer to the line of equality than that of country 1, which intuitively would suggest that country 2 is more equal. If we were to look at the numbers, we see that the bottom percentiles own a higher % of total national income in country 2 than in country 1, while top percentiles own less in country 2 than in country 1. This would suggest that country 2 is more equal in income than country 1, so that country 1 has a higher level of income inequality.

But now let's consider a different case; suppose country 3 has the following distribution:

```python
Country3 = make_array(0, 0.03, 0.07, 0.12, 0.18, 0.25, 0.33, 0.42, 0.54, 0.73, 1.0)
income_distribution3 = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country3 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution.join(
    ["Population Percentage (%)", "Perfect Equality Income Share (%)"], 
    income_distribution3, ["Population Percentage (%)", "Perfect Equality Income Share (%)"]
).relabel(
    "Cumulative Income Share (%)", "Country 1 Cumulative Income Share (%)"
).relabel(
    "Cumulative Income Share (%)_2", "Country 3 Cumulative Income Share (%)"
)
```

```python
plt.figure(figsize=[7,7])
plt.plot(income_distribution.column(0), income_distribution.column(1), "-o", c = 'b')
plt.plot(income_distribution.column(0), income_distribution3.column(1), "-o", c = 'r')
plt.legend(["Country 1", "Country 3"])
plt.plot([0,100], [0,100], color='k');
```

Now, ambiguity arises; while bottom income percentiles earn a larger share of national income in country 3, top income percentiles also have a larger share. We can visualize this phenomenon by the 'crossing' of Lorenz curves on the plot. As a result, we do cannot easily tell which country has a higher level of income inequality.

As you may see, the Lorenz curve is not able to produce a 'quantitative' measure of income inequality, making the scenario above hard for us to compare the 2 countries. For this, we turn to the Gini coefficient.

## The Gini Coefficient

We can use the Gini coefficeint to quantify the level of income inequality.

```{figure} Gini.png
---
width: 500px
name: gini-coefficient
---
The Gini coefficient
```

![](Gini.png)

```{admonition} Definition

The **Gini coefficient** is the ratio of the area between the line of equality and the Lorenz curve to the total area under the line of equality. Referring to $A$ and $B$ from {numref}`gini-coefficient`:

$$\text{Gini} = \frac{\text{Area between line of equality and Lorenz curve}}{\text{Area under line of equality}} = \frac{A}{A+B}$$

If we express the Lorenz curve as $L(x)$, we can use calculus to derive an equation for the Gini coefficient:

$$\text{Gini} = \frac{\frac{1}{2} - \int_0^1 L(x)\text{d}x}{\frac{1}{2}} = 1 - 2\int_0^1 L(x)\text{d}x$$
```

Intuitively, the closer the Lorenz curve is to the line of equality, the lower income inequality exists. Hence, the smaller the area of A, the lower the inequality. **This means that the smaller the Gini coefficient, the lower the income inequality.** Also note that the Gini coefficient will always be between 0 and 1. Mathematically, since $A$ and $B$ are both positive, $0<\frac{A}{A+B}<1$.

```python
# This function estimates the Gini coefficient. You don't have to understand how this code works below.
def gini(distribution):
    sorted_distribution = sorted(distribution)
    height = 0
    area = 0
    for i in sorted_distribution:
        height += i
        area += height - i / 2
    fair_area = height * len(distribution) / 2.
    return (fair_area - area) / fair_area
```

When we use our population as the parameter to the `gini` function, we get:

```python
gini_coefficient_country1 = gini(Country1)
gini_coefficient_country1
```

```python
gini_coefficient_country2 = gini(Country2)
gini_coefficient_country2
```

```python
gini_coefficient_country3 = gini(Country3)
gini_coefficient_country3
```

These results confirm our intuition from the analysis we did previously via Lorenz curves. Previously, we concluded that country 1 had a higher level of income inequality than country 2, and this is supported by country 1's higher gini coefficient. On the other hand, we had trouble comparing levels of inequality between country 1 and country 3. Here, the gini coefficient would indicate that country 1 has a higher level of income inequality than country 3.

## Other Forms of Measurement

The Gini coefficient is a fairly comprehensive and robust measure of inequality with just a single value: as we've seen above, we look at the entire population's income distribution to determine the Gini coefficient. As a result, calculating the Gini is often a challenging task. In reality, we will never observe the true population Lorenz curve without conducting a census to know how much exactly each household earns. As a result, economists will often attempt to interpolate the Lorenz curve and consequent Gini coefficient from the data they have available. The Gini coefficient is also not as easy to understand or explain; the value by itself does not have significant meaning.

Instead, another common measure of income inequality is the share of income earned by the "top $x$%" or the "bottom $y$%". These measures are more often used in the media or by politicians in discussing the extent of inequality, since they are much easier to understand. Consider the chart below, which plots the share of income earned by the top 1%, bottom 90%, and the 90-99%:

```{figure} alt_measure.png
---
---
Alternative forms of measurement
```

![](alt_measure.png)



--- END 06-inequality/.ipynb_checkpoints/inequality-checkpoint.md ---



--- START 06-inequality/historical-inequality.md ---

---
title: historical-inequality
type: textbook
source_path: content/06-inequality/historical-inequality.ipynb
chapter: 6
---

```python
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from datascience import *
%matplotlib inline
# plt.style.use('seaborn-muted')
mpl.rcParams['figure.figsize'] = (10.0, 10.0)
```

# Income Inequality Historically

<!-- Written by Amal Bhatnagar -->

In the last chart on the previous page, you may have noticed that income inequality was rising in the United States in the last few decades. We will examine this in more detail, and also observe global trends in inequality.

## The United States

Let's look at historical trends of income inequality in the US over the last 100 years. The data has been collected from [The World Inequality Database](https://wid.world/), which is co-directed by Berkeley Economics professors Emanuel Saez and Gabriel Zucman. Specifically, we will observe income distributions for the bottom 50 percent, top 10 percent, and top 1 percent.

```python
us_hist = Table.read_table("US_inequality.csv")
us_hist.show(5)
```

```python
us_hist.take(np.arange(100,105))
```

Let's begin with some data cleaning: it seems like our 3 brackets are 'vertically stacked' on top of each other. Instead, we would like a table with 5 columns: `Year`, `bottom 50% income share`, `top 10% income share`, and `top 1% income share`.

```python
bottom_50_us = us_hist.where("Percentile", "p0p50").drop("Percentile").relabeled("Income Share", "Bottom 50% Share")
top_10_us = us_hist.where("Percentile", "p90p100").drop("Percentile").relabeled("Income Share", "Top 10% Share")
top_1_us = us_hist.where("Percentile", "p99p100").drop("Percentile").relabeled("Income Share", "Top 1% Share")
us_hist_joined = bottom_50_us.join("Year", top_10_us).join("Year", top_1_us)
us_hist_joined
```

Oh no, there are some `nan` values! NaN (not a number) values are very common in real world datasets: often, we may not have some observations simply because no data was collected, or perhaps the data collected was faulty. Sometimes, we can try to impute or replace NaN values in order to avoid having gaps in our data, but for now let's ignore NaNs and when plotting to see what's going on:

[Following image has line graphs for different income shares over time]

```python
# mpl.rcParams['figure.dpi'] = 120
# us_hist_joined.plot("Year", width=11, height=7)
plt.plot(us_hist_joined["Year"], us_hist_joined["Bottom 50% Share"], color='blue', label='Bottom 50% Share')
plt.plot(us_hist_joined["Year"], us_hist_joined["Top 10% Share"], color='green', label='Top 10% Share')
plt.plot(us_hist_joined["Year"], us_hist_joined["Top 1% Share"], color='red', label='Top 1% Share')
plt.title("Income Share over Time", fontsize = 16)
plt.ylabel("Proportion", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.legend()
plt.show()
```

# Income Inequality for the Rest of the World

Now let's examine the trends of income inequality in other parts of the world.

[Following image has line graphs for different income shares over time across countries]

```python
world_hist = Table.read_table("World_Inequality.csv")
bottom_50_world = world_hist.where("Percentile", "p0p50").drop("Percentile")
top_10_world = world_hist.where("Percentile", "p90p100").drop("Percentile")
top_1_world = world_hist.where("Percentile", "p99p100").drop("Percentile")
top_10_world
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

Just like the US, it seems global inequality has been rising around the world, especially in China, India, Russia, and across Europe. However, in absolute terms, the level of income inequality in Europe is much lower than that in the United States. 

Also look at Russia: income inequality spiked up around 1991. This was likely caused by the fall of the USSR: the failing Soviet state left the ownership of state assets uncontested, which allowed former USSR officials to acquire state property through informal deals. This led to the rise of many Russian oligarchs - those who rapidly accumulated wealth during the era of Russian privatization directly follwing the dissolution of the Soviet Union.

[Following image has line graphs for different income shares over time in USA & Europe]

```python
top_10_world.select("Year", "USA", "Europe").plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("Income Inequality over Time", fontsize=18);
```

##  The Elephant Graph

[Following image has a line graph connecting points for the income growth vs income percentile]

```{figure} elephant_curve.jpg
---
width: 500px
name: elephant-curve
---
The elephant curve {cite}`10inequality-elephantCurve`
```

The elephant curve is a graph that shows the real income growth per adult across each income group’s percentile around the world. 

There are 3 key features of the elephant curve: a hump for the world’s poorest, valley for the middle class, and trunk for the upper class. The thump is made of the world’s poorest countries, most likely those from developing countries. The valley comprises the working class from the developed world and upper class from developing countries. The trunk is made of people from the upper class from developed countries. The hump and valley indicate growth among emerging countries, and the top global 1%’s growth is higher than any other income group, thus explaining the positively sloped shape of the trunk. 


A study done by the Brookings Institution] found that “poorer countries, and the lower income groups within those countries, have grown most rapidly in the past 20 years” {cite}`10inequality-brookings`. This supports the World Bank’s claim that inequality between countries and within countries is decreasing. The Brookings Institution used only household surveys, however, which usually excludes the top and bottom percentile of the population, due to non-response bias. Still, the study is useful in corroborating the trends and growth in global income inequality.

# Factors that Affect Income Inequality

Economists have isolated multiple factors that influence a country's income inequality
- top marginal tax rates
- unemployment rates
- population growth

We will look at each of these scenarios independently and see its overall trends

## Top marginal tax rates

Let's also take a look at the top marginal tax rates in the United States throughout this time. Overall, the United States (and most of the rest of the world) has a progressive tax system, which means that the more income you earn, the higher percentage you will be taxed at. A good way to reduce income inequality is through progressive taxation; having the richer paying a higher portion of their income will help increase equality. Currently, the top marginal tax rate is 37%, as we can see in the table below.

```{figure} MTR.png
---
width: 500px
name: irs
---
Marginal tax rates. Image from the IRS
```

The top marginal tax rate only applies to the portion of your income above a certain income level. For example, if you earned 19501 dollars in 2019, then you will pay 1940 dollars plus 12% of $19501-19401$, i.e. 12 dollars. For another example, if you earned 80000 dollars, then you will pay $9086 + 0.22(80000-78950) = 9317$ dollars in tax, effectively a $\frac{9317}{80000} = 11.6\%$ tax rate.

In general, the idea is you will pay a lower tax rate for your first $x$ dollars, but a higher rate for dollars earned over $x$.

Now let's look at the historical trends in marginal top tax rates, which is the % taxed at the highest tax bracket.

[Following image has line graphs for the top tax rate over time]

```python
toptax = Table.read_table("toptaxrate.csv")
toptax
```

```python
# mpl.rcParams['figure.dpi'] = 120
toptax.plot(0,1, width=11, height=7)
plt.title("Top Marginal Tax Rate over Time", fontsize = 18)
plt.show()
```

```python
# mpl.rcParams['figure.dpi'] = 120
us_hist_joined.plot("Year", width=11, height=7)
plt.title("Income Share over Time", fontsize = 18)
plt.ylabel("Proportion", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

This graph depicts income inequality decreasing between 1910 and 1970 and increasing from 1970 to present. 

In 1913, Congress implemented the current income tax to promote equality. Originally meant to help compensate for revenue lost from reducing high tariffs, the new policy essentially made the top 1% start contributing to taxes. Additionally, the top marginal tax rate increased from 7% in 1913 to 73% in 1918, thus helping reduce income inequality. Right before the Great Depression, income inequality peaked, where the richest 1% possessed 19.6% of all income. During the Great Depression, top marginal tax rates increased, peaking at 94% in 1944. The top marginal tax rate decreased but remained high over subsequent decades, where it was 70% in 1965 and 50% in 1982. These high top marginal tax rates are correlated with low income inequality. During the Great Depression, the richest 1% had about 15% of total income. For the 10 years after the Great Depression, the top 1% had below 10% of total income and 8% for the 30 years afterwards. This period was known as the Great Compression, as income differentials between the top 1% and the rest of the country decreased.

In the 1970s, the economy took a turn for the worse with high unemployment and inflation (stagflation) and low growth. In order to stimulate economic growth, the government reduced top marginal tax rates (70% to 38.5% in 1980s), deregulated corporate institutions, and attacked labor union memberships (membership decreased by half within 30 years). Although these policies improved economic growth, it resulted in higher income inequality.

The graph below better shows that the share of income earned by the bottom 50% percentile steadily decreased, while the share earned by the top 1% increased steadily. This means that the top 1% has more wealth than the entire bottom 50% of the population. Suppose a class of 100 people has \$100 in aggregate. In a world with perfect equality, each person would have \$1. With this current level of income inequality, one person would have more wealth than 50 people combined. 

The media continues to report on the nation's significant income disparity. [The Washington Post wrote a story](https://www.washingtonpost.com/business/2019/09/26/income-inequality-america-highest-its-been-since-census-started-tracking-it-data-show/) and found that “The number of families earning \$15,000 or less has fallen since 2007, according to the latest census data, while the number of households bringing in \$250,000 a year or more has grown more than 15 percent.” 

Can we conclude that high marginal tax rates lead to low income inequality but slow economic growth?

## Unemployment rates

Economists believe that unemployment is one of the leading factors that leads to income inequality. When looking at what influences the Gini coefficient, a paper from [Princeton](https://www.researchgate.net/profile/Alicia-Menendez-4/publication/23544180_The_Effect_of_Unemployment_on_Labor_Earnings_Inequality_Argentina_in_the_Nineties/links/09e4150645c0d0e83e000000/The-Effect-of-Unemployment-on-Labor-Earnings-Inequality-Argentina-in-the-Nineties.pdf) found that the unemployment rate had the largest effect on the income inequality rates

Below, we look at the unemployment rates for the past 20 years across many different countries. These are the same countries and regions that we will further study below.

```python
unemployment = Table.read_table("Unemployment.csv")
unemployment
```

As we can see from the graph, the unemployment rates for China, India and the rest of the world have stayed somewhat steady. On the other hand, Brazil, the US, Russia and Europe are encountering drastically different unemployment situations than before.

[Following image has line graphs for unemployment rates over time across countries]

```python
# mpl.rcParams['figure.dpi'] = 120
unemployment.plot("Year", width=11, height=7)
plt.ylabel("Unemployment Rate (%)", fontsize = 14)
plt.xlabel("Year", fontsize =14)
plt.title("Unemployment Rate over Time", fontsize = 18)
plt.show()
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", size=14)
plt.xlabel("Year", fontsize=14)
plt.title("International Income Inequality over Time", fontsize=18)
plt.show()
```

The graphs above show a positive correlation between unemployment and income inequality. As unemployment increases, income inequality also increases. In 2011, Hanan Morsy, an Egyptian economist who serves as the Director of Macroeconomic Policy, Forecasting and Research at the African Development Bank, actually researched this topic {cite}`10inequality-morsy`. Her group examined the member nations of the Organization  for  Economic  Cooperation  and  Development  (OECD) between 1980 and 2005. She found that specific groups that were vulnerable to the economic shocks that led to an increase in income inequality:
- young workers
- low-skilled workers
- workers who had been out of work for a long time

Her solution was to increase job creation opportunities for temporary, recently fired, and recently hired workers, provide job assistance and training to prevent long-term unemployment, and improve incentives for working by aligning incentives with productivity of labor. Morsy's research found that the most vulnerable groups to economic shocks were young, low-skilled and temporary workers. Creating opportunities for these different demographics would help them be more protected from potential shocks and thus decrease income inequality.

## Population growth

As the number of people in a country's population increase, it becomes more difficult for a country to distribute its public goods to everyone. This leads to many social consequences in which resources are not fairly distributed to all members of the population, cause inaccessibility for different parts of the population

The table below shows how the population growth has changed for the same countries we saw above. We are only looking at data for the past 10 years.

[Following image has line graphs for population growth rates over time]

```python
pop_growth = Table.read_table("Population Growth.csv")
pop_growth
```

```python
pop_growth.plot("Year", width=11, height=7)
plt.title("Population Growth over Time", fontsize = 18)
plt.ylabel("Population Growth (%)", fontsize = 14)
plt.xlabel("Year", fontsize = 14)
plt.show()
```

```python
top_10_world.plot("Year", width=11, height=7)
plt.ylabel("Gini Coefficient", fontsize=14)
plt.xlabel("Year", fontsize=14)
plt.title("International Income Inequality over Time", fontsize=18)
plt.show()
```

The graphs above show that most countries high population growth between 1-2% during the 1980's. The effects of this can be seen in the rising income inequality during the 90's.  

Recent research by University of Toronto's Marijn Bolhuis and Univeristy of Oxford's Alexandra de Pleijt shows that there is a strong correlation between a country's population growth (measured by birth rates) and its income inequality {cite}`10inequality-popGrowth`. Their most recent study in 2016 analyzed income inequality and birth rates data between 1870 and 2000 across 67 countries. They concluded that if a country had 50% higher income inequality, then that country's birth rate would be about twice as high as another country with the same level of economic development. Bolhuis says that these higher birth rates mean that economic growth has to be equal to or greater than the birth rate to offset the implications of higher birth rates. 

This is part of a larger debate about the relationship between birth rates and income inequality. Economist Thomas Piketty finds that low birth rates, rather than high birth rates, are causing today's income inequality. With lower birth rates, fewer children per couple are being borne, so these children get more of their parents' inheritance.



--- END 06-inequality/historical-inequality.md ---



--- START 06-inequality/index.md ---

---
title: index
type: textbook
source_path: content/06-inequality/index.md
chapter: 6
---

# Inequality

Income inequality is a relevant issue that often comes up in the news. In this chapter, we'll go over measurements of inequality, trends in income inequality in the US, and compare income inequality measures across countries. 

**Student Learning Outcomes:**

* Understand the Lorenz curve to depict income distributions within a country
* Calculate the Gini coefficient as a measure of inequality


--- END 06-inequality/index.md ---



--- START 06-inequality/inequality.md ---

---
title: inequality
type: textbook
source_path: content/06-inequality/inequality.ipynb
chapter: 6
---

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datascience import *
%matplotlib inline
plt.style.use('seaborn-v0_8-muted')
```

# Measurements of Inequality

## The Lorenz Curve
The Lorenz Curve visually presents income inequality by plotting household income percentile on the $x$-axis, and the cumulative income share that the bottom $x$ percentile own on the $y$-axis. The households are sorted by income, so that the first household at the 0th percentile has the least amount of income, while the household at the 100th percentile has the greatest income.

For any point $(x,y)$ on the Lorenz curve, “the bottom x percent own y% of the income”. For example, if the $x$-axis reads 0.30 and $y$-axis reads 0.10, then it means that the bottom 30% of the population received 10% of the total population's income. This yields 2 implications for the Lorenz Curve:
- The points $(0,0)$ and $(1,1)$ are always on the curve. $(0,0)$ represents the 0% of the population owning 0% of the income and $(1,1)$ represents 100% of the population owning 100% of the income.
- The slope is always increasing. This is because households are sorted by income as percentiles: for a slight increase in $x$, households become richer and hence provide a larger share of total income.

### Line of Perfect Equality
In a world of perfect equality, everyone would have the exact same income. In this case, the Lorenz curve would just be a 45-degree straight line that runs through $(0,0)$ and $(1,1)$, i.e. $y=x$. Mathematically, this is because the derivative is constant: for a slight increase in $x$, the total share of income increases at a constant rate. Another way to think about this is that the bottom 10% of the population will own 10% of the total income, the bottom 50% of the population will own 50% of the total income, and so on. This line is known as the *line of perfect equality*, and we typically display this line when plotting our Lorenz curves as a reference.

### A Toy Example
Let's suppose country 1 has the following income distribution: 
- The bottom 10% owns a cumulative 2% of total income 
- The bottom 20% owns 5% of total income
- The bottom 30% owns 9% of total income
- The bottom 40% owns 15% of total income 
- The bottom 50% owns 23% of total income
- The bottom 60% owns 33% of total income
- The bottom 70% with 45% of total income
- The bottom 80% with 59% of total income 
- The bottom 90% with 75% of total income
- The bottom 100% with 100% of total income

We will create an array of income shares and call it `Country1`.

```python
Country1 = make_array(0, 0.02, 0.05, 0.09, 0.15, 0.23, 0.33, 0.45, 0.59, 0.75, 1.0)
```

To better see this information, we will create a table containing population percentage and cumulative income share.

```python
income_distribution = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country1 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution
```

How will the Lorenz Curve for this income distribution look? 

[Following image is an line graph and a scatter plot of population share vs income share]

```python
income_distribution.scatter(0, 1, width=11, height=7)
plt.plot([0,100], [0,100], color='k');
```

### Comparing Lorenz Curves
Now let's compare 2 countries' Lorenz curves. Suppose country 2 has the following income distribution:
- The bottom 10% owns a cumulative 3% of total income 
- The bottom 20% owns 7% of total income
- The bottom 30% owns 13% of total income
- The bottom 40% owns 19% of total income 
- The bottom 50% owns 27% of total income
- The bottom 60% owns 37% of total income
- The bottom 70% with 50% of total income
- The bottom 80% with 65% of total income 
- The bottom 90% with 81% of total income
- The bottom 100% with 100% of total income

```python
Country2 = make_array(0, 0.03, 0.07, 0.13, 0.19, 0.27, 0.37, 0.5, 0.65, 0.81, 1.0)
income_distribution2 = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country2 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution2
```

Comparing the 2 countries' income distributions side by side:

```python
income_distribution.join(
    ["Population Percentage (%)", "Perfect Equality Income Share (%)"], 
    income_distribution2, ["Population Percentage (%)", "Perfect Equality Income Share (%)"]
).relabel(
    "Cumulative Income Share (%)", "Country 1 Cumulative Income Share (%)"
).relabel(
    "Cumulative Income Share (%)_2", "Country 2 Cumulative Income Share (%)"
)
```

Plotting both countries' Lorenz curves, can you tell which country has a higher level of income inequality?

[Following image has 2 Lorenz Curves for different countries in population share vs income share]

```python
plt.figure(figsize=[7,7])
plt.plot(income_distribution.column(0), income_distribution.column(1), "-o", c = 'b')
plt.plot(income_distribution.column(0), income_distribution2.column(1), "-o", c = 'r')
plt.legend(["Country 1", "Country 2"])
plt.plot([0,100], [0,100], color='k');
```

In this case, we can see that country 2's Lorenz curve is closer to the line of equality than that of country 1, which intuitively would suggest that country 2 is more equal. If we were to look at the numbers, we see that the bottom percentiles own a higher % of total national income in country 2 than in country 1, while top percentiles own less in country 2 than in country 1. This would suggest that country 2 is more equal in income than country 1, so that country 1 has a higher level of income inequality.

But now let's consider a different case; suppose country 3 has the following distribution: 

[Following image has 2 Lorenz Curves for different countries in population share vs income share]

```python
Country3 = make_array(0, 0.03, 0.07, 0.12, 0.18, 0.25, 0.33, 0.42, 0.54, 0.73, 1.0)
income_distribution3 = Table().with_columns(
    "Population Percentage (%)", np.arange(11) * 10, 
    "Cumulative Income Share (%)", Country3 * 100, 
    "Perfect Equality Income Share (%)", np.arange(11) * 10
)
income_distribution.join(
    ["Population Percentage (%)", "Perfect Equality Income Share (%)"], 
    income_distribution3, ["Population Percentage (%)", "Perfect Equality Income Share (%)"]
).relabel(
    "Cumulative Income Share (%)", "Country 1 Cumulative Income Share (%)"
).relabel(
    "Cumulative Income Share (%)_2", "Country 3 Cumulative Income Share (%)"
)
```

```python
plt.figure(figsize=[7,7])
plt.plot(income_distribution.column(0), income_distribution.column(1), "-o", c = 'b')
plt.plot(income_distribution.column(0), income_distribution3.column(1), "-o", c = 'r')
plt.legend(["Country 1", "Country 3"])
plt.plot([0,100], [0,100], color='k');
```

Now, ambiguity arises; while bottom income percentiles earn a larger share of national income in country 3, top income percentiles also have a larger share. We can visualize this phenomenon by the 'crossing' of Lorenz curves on the plot. As a result, we do cannot easily tell which country has a higher level of income inequality.

As you may see, the Lorenz curve is not able to produce a 'quantitative' measure of income inequality, making the scenario above hard for us to compare the 2 countries. For this, we turn to the Gini coefficient.

## The Gini Coefficient

We can use the Gini coefficeint to quantify the level of income inequality.

[Following image has two parts shaded within a 45 degree triangle, part A above Lorenz Curve and Part B below]

```{figure} Gini.png
---
width: 500px
name: gini-coefficient
---
The Gini coefficient
```

![](Gini.png)

```{admonition} Definition

The **Gini coefficient** is the ratio of the area between the line of equality and the Lorenz curve to the total area under the line of equality. Referring to $A$ and $B$ from {numref}`gini-coefficient`:

$$\text{Gini} = \frac{\text{Area between line of equality and Lorenz curve}}{\text{Area under line of equality}} = \frac{A}{A+B}$$

If we express the Lorenz curve as $L(x)$, we can use calculus to derive an equation for the Gini coefficient:

$$\text{Gini} = \frac{\frac{1}{2} - \int_0^1 L(x)\text{d}x}{\frac{1}{2}} = 1 - 2\int_0^1 L(x)\text{d}x$$
```

Intuitively, the closer the Lorenz curve is to the line of equality, the lower income inequality exists. Hence, the smaller the area of A, the lower the inequality. **This means that the smaller the Gini coefficient, the lower the income inequality.** Also note that the Gini coefficient will always be between 0 and 1. Mathematically, since $A$ and $B$ are both positive, $0<\frac{A}{A+B}<1$.

```python
# This function estimates the Gini coefficient. You don't have to understand how this code works below.
def gini(distribution):
    sorted_distribution = sorted(distribution)
    height = 0
    area = 0
    for i in sorted_distribution:
        height += i
        area += height - i / 2
    fair_area = height * len(distribution) / 2.
    return (fair_area - area) / fair_area
```

When we use our population as the parameter to the `gini` function, we get:

```python
gini_coefficient_country1 = gini(Country1)
gini_coefficient_country1
```

```python
gini_coefficient_country2 = gini(Country2)
gini_coefficient_country2
```

```python
gini_coefficient_country3 = gini(Country3)
gini_coefficient_country3
```

These results confirm our intuition from the analysis we did previously via Lorenz curves. Previously, we concluded that country 1 had a higher level of income inequality than country 2, and this is supported by country 1's higher gini coefficient. On the other hand, we had trouble comparing levels of inequality between country 1 and country 3. Here, the gini coefficient would indicate that country 1 has a higher level of income inequality than country 3.

## Other Forms of Measurement

The Gini coefficient is a fairly comprehensive and robust measure of inequality with just a single value: as we've seen above, we look at the entire population's income distribution to determine the Gini coefficient. As a result, calculating the Gini is often a challenging task. In reality, we will never observe the true population Lorenz curve without conducting a census to know how much exactly each household earns. As a result, economists will often attempt to interpolate the Lorenz curve and consequent Gini coefficient from the data they have available. The Gini coefficient is also not as easy to understand or explain; the value by itself does not have significant meaning.

Instead, another common measure of income inequality is the share of income earned by the "top $x$%" or the "bottom $y$%". These measures are more often used in the media or by politicians in discussing the extent of inequality, since they are much easier to understand. Consider the chart below, which plots the share of income earned by the top 1%, bottom 90%, and the 90-99%:

[Following image has line graphs for shares of wealth by wealth percentiles]

```{figure} alt_measure.png
---
---
Alternative forms of measurement
```

![](alt_measure.png)


--- END 06-inequality/inequality.md ---



--- START 07-game-theory/.ipynb_checkpoints/cournot-checkpoint.md ---

---
title: cournot-checkpoint
type: textbook
source_path: content/07-game-theory/.ipynb_checkpoints/cournot-checkpoint.ipynb
chapter: 7
---

```python
from datascience import *
import sympy
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import warnings
warnings.simplefilter("ignore")
%matplotlib inline
```

# Cournot Competition

One model of understanding oligopolies comes in Cournot competition, named for Aontoine Cournot who first proposed it. Cournot competition is a model describing a market in which firms compete by changing their output. In Cournot competition, there are a fixed number of firms in a market that produce the same product. Firms do not collude but have market power (i.e. each firm’s output decisions affect the price of the good). Each firm knows the number of firms in the market and has its own cost function that it uses to determine its level of output.

OPEC is a good example of a Cournot oligopoly: its participants affect prices by changing their output. OPEC also demonstrates a flaw in the reasoning behind Cournot competition: the equilibrium state of a Cournot oligopoly suggests that collusion by market participants is the rational policy, but in reality game theory shows us this cannot be the “true” equilibrium because cartel members undercut one another in a bid for market share.

## Cournot Profit Functions
For the purposes of this class, we will be analyzing the case of Cournot duopoly (i.e. when there are only 2 firms in a market). Throughout the course we will make these assumptions about the market: both firms have to follow a joint linear price function (meaning the market price is determined only by the quantity produced of both firms) and both firms have an equal constant marginal cost curve. To put this formally, $P=P(Q)=P(q_1+q_2)=m(q_1+q_2)+b$ (where $m$ and $b$ are parameters for the slope and intercept of the price functions). Costs for firm 1 can be represented as $C_1=cq_1$ and costs for firm 2 can be represented as $C_2=cq_2$. To find the equilibrium of this game we will need to set up the profit functions for both firms. Starting with firm 1 we can put the last two equations together to get:

$$
\begin{equation} \label{eq1}
\begin{split}
\pi_1 & =Pq_1-C_1 \\
 & =(m(q_1+q_2)+b)q_1-cq_1 \\
 & =mq_1^2+mq_1q_2+bq_1 -cq_1
\end{split}
\end{equation}
$$

Firm 2 follows a similar profit function:

$$
\begin{align*}
\pi_2 & =Pq_2-C_2\\
 & =mq_2^2+mq_1q_2+bq_2 -cq_2   
\end{align*}
$$

## Best Response Functions
Similar to how in the utility unit individuals seek to maximize utility, in Cournot firms seek to maximize profit. So we will be using the same mathematical tool we learned last week (partial derivatives) to derive the equilibrium of this game. Specifically we will be solving for a $q_1^{*}$ that satisfies $ \underset{q_1}{\mathrm{argmax}}\ \pi_1 \ \textrm{or} \ \underset{q_1}{\mathrm{argmax}}\ (mq_1^2+mq_1q_2+bq_1 -cq_1)$. In english, this means we will be finding the value (or values) of $q_1$ that maximize the profit function for player 1. We will also do the same thing for player 2. Starting with player 1, let's take the partial derivative with respect to $q_1$ and set that equation equal to 0. We can then solve that equation to obtain $q_1^{*}$. 

$$
\begin{align*}
   \frac{\partial{\pi_1}}{\partial{q_1}}= 2mq_1 + mq_2+b - c \\
   2mq_1 + mq_2+b - c = 0\\
  q_1^{*}= \frac{c-mq_2-b}{2m}
\end{align*}
 \
$$
This equation for $q_1$ is called a best response function because it tells us player 1's optimal output for every possible output player 2 can choose. Now, in order to get the equilibrium, we need to find player 2's best response function. We will use the same approach to do this (i.e., we will solve for a $q_2^{*}$ that satisfies $\underset{q_2}{\mathrm{argmax}}\ \pi_2 \ \textrm{or} \ \underset{q_2}{\mathrm{argmax}}\ mq_2^2+mq_1q_2+bq_2 -cq_2$ ).
 
$$
 \begin{align*}
   \frac{\partial{\pi_2}}{\partial{q_2}}= 2mq_2 + mq_1+b - c \\ 
   2mq_2 + mq_1+b - c = 0\\
   q_2^{*} = \frac{c-mq_1-b}{2m}
\end{align*}
$$

If $q_2$ was fixed at a certain price and we were to graph firm 1's best response equation, it would look something like this:

```python
m, b = -0.5, 9
q_2 = 4
c = 3
D_fn = lambda x: m * x + b + 2
P = lambda x: D_fn(x + q_2)
r_1_fn = lambda x: 2 * m * x + m * q_2 + b + 2
r_1_inv = lambda x: (x - m * q_2 - b - 2) / 2 / m

q_1 = np.linspace(0, 25, 1000)
D = D_fn(q_1)
d_1 = P(q_1)
r_1 = r_1_fn(q_1)

q_1_prime = r_1_inv(c)

plt.figure(figsize=[8,8])

# demand
plt.plot(q_1, D, label=r"Market Demand, $D$")
plt.text(5, b, r"$D$", size=16)



# marginal revenue
plt.plot(q_1, r_1, label=r"Marginal Revenue of Firm 1, $r_1$")
plt.text(5, b - 4.5, r"$r_1$", size=16)

# marginal cost
plt.plot(q_1, c * np.ones_like(q_1), label=r"Marginal Cost, $c$")
plt.text(20, 3.2, r"$c$", size=16)

# optimum quantity
plt.vlines(q_1_prime, 0, 3, linestyles="dashed")



plt.xlim([0,22])
plt.ylim([0,12])
plt.xticks([r_1_inv(c)], [r"$q_1^*$"], size=14)
plt.yticks([P(0)], [r"$P(q_2)$"], size=14)
plt.xlabel("Quantity", size=16)
plt.ylabel("Price", size=16)
plt.legend();
```

This may make intuitive sense as if you recall from the demand and supply chapters, firms seek to produce where marginal revenue equals marginal cost. The partial derivative we took before did that automatically. We can verify this with the following calculation:  

$$
\begin{align*}
    MR=MC  \\
    MR-MC=0  \\
    \frac{\partial}{\partial{q_1}} (\textrm{Revenue - Cost}) = 0 \\
    \frac{\partial}{\partial{q_1}} ((m(q_1+q_2)+b)q_1-cq_1) = 0 \\
    2mq_1 + mq_2+b - c = 0 
\end{align*}
$$

However, this graph assumes a fixed value for $q_2$. Let's try varying $q_2$ and see what happens.

```python
m, b = -0.5, 9
q_2 = 0
c = 3
D_fn = lambda x: m * x + b + 2
P = lambda x: D_fn(x + q_2)
r_1_fn = lambda x: 2 * m * x + m * q_2 + b + 2
r_1_inv = lambda x: (x - m * q_2 - b - 2) / 2 / m

q_1 = np.linspace(0, 25, 1000)
D = D_fn(q_1)
d_1 = P(q_1)
r_1 = r_1_fn(q_1)

q_1_prime = r_1_inv(c)

plt.figure(figsize=[8,8])

# demand
plt.plot(q_1, D, label=r"Market Demand, $D$")
plt.text(5, b, r"$D$", size=16)


# marginal revenue
plt.plot(q_1, r_1, label=r"Marginal Revenue of Firm 1 for $q_2=0$, $r_1$")
plt.text(5, b - 2.5, r"$r_1$", size=16)

# case when q_2 = q_c
m, b = -0.5, 9
q_2 = 16
c = 3
D_fn = lambda x: m * x + b + 2
P = lambda x: D_fn(x + q_2)
r_1_fn = lambda x: 2 * m * x + m * q_2 + b + 2
r_1_inv = lambda x: (x - m * q_2 - b - 2) / 2 / m

q_1 = np.linspace(0, 25, 1000)
D = D_fn(q_1)
d_1 = P(q_1)
r_1 = r_1_fn(q_1)


# marginal revenue
plt.plot(q_1, r_1, label=r"Marginal Revenue of Firm 1 for $q_2=q_m$, $r_1$")
plt.text(2.5, 0.8, r"$r_1$", size=16)

# marginal cost
plt.plot(q_1, c * np.ones_like(q_1), label=r"Marginal Cost, $c$")
plt.text(20, 3.2, r"$c$", size=16)


# q'_1(0)
plt.vlines(r_1_inv(-5), 0, 3, linestyles="dashed")

plt.xlim([0,22])
plt.ylim([0,12])
plt.xticks([r_1_inv(-5), 0], [r"$q_1^*(0) = q_m$", r"$q_1^*(q_m)$"], size=14)
plt.yticks([], [], size=14)
plt.xlabel("Quantity", size=16)
plt.ylabel("Price", size=16)
plt.legend();
```

The orange marginal revenue curve represents a scenario where Firm 2 produces so little that Firm 1 can overtake the market and become a monopoly. The green marginal revenue curve is a scenario where Firm 2 produces so much that it is too costly for Firm 1 to remain in this market, causing Firm 1's optimal output to become 0. 

Let's now try to solve for the Nash Equilibrium of this game.

## Cournot Equilibrium
Recall that a Nash Equilibrium is where both the players are giving the best response to each other's actions. Graphically, this is where the best response curves intersect.

```python
q_m = r_1_inv(-5)
q_c = D_fn(-10)

p1 = (0, q_m)
p2 = (q_c, 0)

slope = (p2[1] - p1[1]) / (p2[0]- p1[0])
intercept = p1[1] - slope * p1[0]

xs = np.linspace(0, 25, 1000)
ys1 = slope * xs + intercept
ys2 = (xs - intercept) / slope

plt.figure(figsize=[8,8])

plt.plot(xs, ys1, label=r"$q'_1$")
plt.plot(xs, ys2, label=r"$q'_2$")
plt.text(13, intercept - 6.3, r"$q_1^*(q_2)$", size=16)
plt.text(3, intercept + 2.5, r"$q_2^*(q_1)$", size=16)

x_star = - (slope + 1) * intercept / (slope**2 - 1)
y_star = slope * x_star + intercept
plt.vlines(x_star, 0, y_star, linestyle="dashed")
plt.hlines(y_star, 0, x_star, linestyle="dashed")

plt.xlim([0,22])
plt.ylim([0,12])
plt.xticks([x_star], [r"$q_2^*$"], size=14)
plt.yticks([y_star], [r"$q_1^*$"], size=14);
```

In this class, you will use Python to find that intersection. However, for future classes, it may be useful to see how you can find the intersection algebraically. Here's the equation we must solve:

$$
\begin{align*}
    q_1=\frac{c-b-m(\frac{c-mq_1-b}{2m})}{2m}
\end{align*}
\
$$

To find player 1's equilibrium strategy, we solve for $q_1$. 

$$
\begin{align*}
    q_1=\frac{c-b-m(\frac{c-mq_1-b}{2m})}{2m} \\ 
    2mq_1 = c-b-m(\frac{c-mq_1-b}{2m}) \\
    2mq_1 = c-b-\frac{c-mq_1-b}{2}\\
    2mq_1 - \frac{mq_1}{2} = c - \frac{c}{2} -b +\frac{b}{2} \\
    \frac{3mq_1}{2} = \frac{c-b}{2} \\ 
    3mq_1 = c-b \\
    q_1^{*} = \frac{c-b}{3m}
\end{align*}
\
$$

Phew, that was a lot of math. However, we are not quite done yet - we still need to find player 2's optimal output. The process for doing this is fairly similar to the process for finding player 1's optimal output, as can be seen below. 

$$
\begin{align*}
    q_2 = \frac{c-mq_1^{*}-b}{2m} = \frac{c-b-m(\frac{c-b}{3m})}{2m} \\
    2mq_2 = c -\frac{c}{3} - b +\frac{b}{3} \\
    2mq_2 = \frac{2(c-b)}{3}\\
    q_2^{*} = q_2 = \frac{c-b}{3m}
\end{align*}
\
$$
This shows that the unique Nash Equilibrium of Cournot Duopoly is $(q_1^{*},q_2^{*}) = (\frac{c-b}{3m},\frac{c-b}{3m})$

## Implications
The Cournot model implies that output is greater in a Cournot duopoly than in a monopoly but still lower than perfect competition. Prices are also lower in a Cournot duopoly, but higher than perfect competition. Cournot equilibria are also a subset of Nash equilibria, and so the equilibrium we just derived is one from which neither player will likely deviate. As noted earlier, Cournot also indicates that members of a duopoly could form a cartel and raise profits by colluding.

### Applying Cournot
Now that we have shown how to derive the Cournot equilibrium, let’s apply this to a problem. Consider the industry of airline manufacturing: there are two main competitors, Boeing and Airbus, and for this problem, we will think of this market as a Cournot duopoly. Suppose the market demand curve is given by $P=-1.89Q+148.89$ where the price is in millions of dollars and that the marginal cost of both firms is constant at $c=100$. Solve for the Nash Equilibrium. 


First we will want to derive both firms profit functions:

$$
\begin{align*} 
    \pi_1 & = Pq_1 -cq_1 \\
    \pi_1 & = (-1.89Q+148.89)q_1 - cq_1 \\
    \pi_1 & = (-1.89(q_1+q_2)+148.89)q_1 -cq_1 \\
    \pi_1 & = -1.89q_1^2-1.89q_1q_2+148.89q_1 - cq_1
\end{align*}
$$

For firm 2 the profit function similarly looks like:

$$
\begin{align*}
    \pi_2 = -1.89q_2^2-1.89q_1q_2+148.89q_2 - cq_2
\end{align*}
$$

Now we will take the partial derivative to find the best response functions for both players. For firm 1 this looks like:

$$
\begin{align*}
    \frac{\partial{\pi_1}}{\partial{q_1}}= -3.78q_1 - 1.89q_2+148.89 - 100 = 0\\
    q_1 = \frac{48.89-1.89q_2}{3.78}
\end{align*}
\
$$

For firm 2 this looks like:

$$
\begin{align*}
    \frac{\partial{\pi_2}}{\partial{q_2}}= -3.78q_2 - 1.89q_1+148.89 - 100 = 0\\
    q_2 = \frac{48.89-1.89q_1}{3.78}
\end{align*}
$$

Now while you can solve this algebraically, it takes much longer and leaves a lot of room for error, so as data scientists we will use Python to solve this problem. The code for this looks like this:

```python
q_1=sympy.Symbol("q_1") #Create a symbol for q1
q_2=sympy.Symbol("q_2")#Create a symbol for q2
Br_q_1 = (48.89 - 1.89*q_2)/3.78 #Create the best response function for firm 1
Br_q_2 = (48.89 - 1.89*q_1)/3.78 #Create the best response function for firm 2
q1_Br_2 = (48.89-3.78*q_2)/1.89 #Solve firm 2's best response function for q1
q_1_star = solve(Br_q_1,q1_Br_2) #Use the solve function to solve the system of equations 
q_2_star = Br_q_2.subs(q_1,q_1_star ) #Substiute in the optimal q_1_star into firm 2's best response function
(q_1_star,q_2_star) #Nash Equilibrium
```


--- END 07-game-theory/.ipynb_checkpoints/cournot-checkpoint.md ---



--- START 07-game-theory/.ipynb_checkpoints/equilibria-oligopolies-checkpoint.md ---

---
title: equilibria-oligopolies-checkpoint
type: textbook
source_path: content/07-game-theory/.ipynb_checkpoints/equilibria-oligopolies-checkpoint.ipynb
chapter: 7
---

```python
from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import warnings
warnings.simplefilter("ignore")
import plotly.graph_objects as go
from plotly.offline import plot
from IPython.display import display, HTML
%matplotlib inline
```

# Equilibria & Oligopolies

This section introduces the concept of equilibria in games, the paradigm of the prisoner's dilemma, and oligopolies. These concepts are essential to understanding the models we will be studying the next sections, and for relating game theory to its economic applications.

## Equilibrium

The [prisoner's dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) is a classic game first discussed by Merrill Flood and Melvin Dresher in 1950. In this game, there are two prisoners who have been captured and are being interrogated. The prisoners cannot contact each other in any way. They have two options: they can **defect** (betray the other prisoner to the police) or they can **cooperate** (maintain their silence). If both defect, both receive 4 years in prison. If one defects and the other does not, the defector goes free and the cooperator receives 5 years in prison. If both cooperate (meaning neither talks to the police), then they each receive 2 years in prison. We define **mutual defection** as the case when both prisoners defect and **mutual cooperation** as the case when both cooperate. The purpose of this game is to consider how a completely rational person would be best advised to proceed, and how different strategies for playing this game can be more or less effective.

<table class="payoff-matrix" style="text-align: center; table-layout: fixed;">
    
<tr>
    <td colspan="2" rowspan="2" width="170" style="border-width: 0 1px 1px 0;"></td>
    <td colspan="2">Prisoner B</td>
</tr>
<tr>
    <td width="85">Cooperate</td>
    <td width="85">Defect</td>
</tr>
<tr>
    <td rowspan="2">Prisoner A</td>
    <td>Cooperate</td>
    <td>(2, 2)</td>
    <td>(5, 0)</td>
</tr>
<tr>
    <td>Defect</td>
    <td>(0, 5)</td>
    <td>(4, 4)</td>
</tr>
    
</table>

In the above payoff matrix, we see that if Prisoner A cooperates and Prisoner B defects, then Prisoner A gets 5 years in prisoner and Prisoner B gets none. It is important to note that the above payoff matrix is inverted; because we are talking about _years in prison_ as opposed to utility, a higher value is **worse** for the player, and a player's goal is to _minimize_ their value in the payoff matrix, not maximize it as normally. We will use this payoff matrix convention when discussing the prisoner's dilemma, but _not_ with other games.

An important concept in game theory is finding the equilibrium of a game. There are different types of equilibria, but the most common one considered is the **Nash equilibrium**, named for the mathematician John Forbes Nash, Jr. (who you may remember as a character played by Russel Crowe in [_A Beautiful Mind_](https://en.wikipedia.org/wiki/A_Beautiful_Mind_(film))). A Nash equilibrium occurs when no player can increase their own payoff by changing only their own strategy.

```{admonition} Definition
A **Nash equilibrium** is a set of strategy choices in a non-cooperative game in which each player, assumed to know the equilibrium strategies of the other players, has a chosen strategy and there can be no monotonic improvement; that is, no player can increase their payoffs by changing their strategy without another player changing _their_ strategy.
```

Using this definition, what constitutes a Nash equilibrium for the prisoner's dilemma? Well let's consider the four possible combinations of strategies. (We use "D" as shorthand for "defect" and "C" for "cooperate" below.)

1. **Prisoner A: C, Prisoner B: C.** In this case, both Prisoner A and Prisoner B get 4 years. However, if Prisoner A's strategy remains unchanged, Prisoner B can get fewer years in prison by changing to D, and vice versa. Thus, this is **not** a Nash equilibrium.
2. **Prisoner A: D, Prisoner B: C.** In this case, Prisoner B can change to D and lower their years from 5 to 4. Thus, this is **not** a Nash equilibrium.
3. **Prisoner A: C, Prisoner B: D.** As with (2), Prisoner A can change to D and lower their years in prison. Thus, this is **not** a Nash equilibrium.
4. **Prisoner A: D, Prisoner B: D.** In this case, if Prisoner A changes to C, then their years in prison _increases_ from 4 to 5, as with Prisoner B. Neither player can increase their winnings (decrease their years in prison) by changing their strategy. Thus, this **is** a Nash equilibrium.

By describing the four states of the game, we see that only mutual defection is a Nash equilibrium of the prisoner's dilemma.

```{admonition} Nash equilibria in payoff matrices
:class: tip

It is easy to use a payoff matrix to  see which  states, if any, are Nash equilibria. To see if a game state is a Nash equilibrium, the first value in the cell must be the maximum among all first values in that column and the second value must be the maximum across all second values in that row. This is because moving up and down a column represents changing the row-player's strategy and  moving along a row represents changing the column-player's strategy. If these values are both maxima in their respective directions, then no player can obtain a better payoff by unilaterally changing their strategy, and we have a Nash equilibrium.

You can easily verify this by looking at the payoff matrix for the prisoner's dilemma: the  first 4 in mutual defection has the best payoff in the column and the second  4 has the best payoff in the row.
```

## Oligopolies

One of the most common applications of game theory in economics is the study of **oligopolies**, markets where the number of participants is limited. There are several examples of oligopolies that we experience without knowing in daily life: airlines, soft drinks, and telecom providers, to name a few. Oligopolies are different from regular markets in that they allow their participants to function similar to a monopoly by setting prices as a group; groups of participants conspiring on this kind of illicit activity are referred to as **cartels**. 

Within oligopolies, however, we can observe competition more like a normal market as firms attempt to take market share from one another. When cartels set prices (by limiting the production of the good or service provided by their market), a firm can make a bid for market share by ignoring the agreed-upon production level and producing more. This has the effect of lowering the price of the good but the increase in production by the renegade firm will allow them to make up for the lost marginal revenue through increased sales volume. In this way, oligopoly members can compete against each other, making the market more and more competitive.

A prime example of this type of within-cartel competition was observed in the [2020 oil price war between Russia and Saudi Arabia](https://en.wikipedia.org/wiki/2020_Russia%E2%80%93Saudi_Arabia_oil_price_war). Both countries are members of [OPEC (the Organization of Petroleum Exporting Countries)](https://en.wikipedia.org/wiki/OPEC), an oil cartel that consists of 12 member countries controlling 79% of the world's oil reserves and 44% of oil production. OPEC sets oil prices by limiting the output of its member countries (a decrease in production results in higher prices).

The price war began when Saudi Arabia discounted its oil in response to Russia's refusal to reduce production in accordance with OPEC's directive. OPEC's members had agreed to reduce oil production due to a low forecasted demand for oil due to the COVID-19 pandemic. When Russia (who was not an official member of OPEC but had agreed to cooperate with Saudi Arabia to manage oil prices) didn't abide  by OPEC's decision, Saudi Arabia announced discounts on its oil, starting the price war and leading to a massive drop in the price of oil. We can see the effects of this price war by looking at the price per barrel of OPEC's crude oil, plotted below.

```python
# data from https://www.quandl.com/data/OPEC/ORB-OPEC-Crude-Oil-Price
opec = pd.read_csv("opec_prices.csv")
opec["Date"] = opec["Date"].apply(pd.to_datetime)
opec = opec[opec["Date"] >= dt.datetime(2019, 7, 1)]
start_date = dt.datetime(2020, 3, 8)

fig = go.Figure()
fig.add_trace(go.Scatter(x=opec["Date"], y=opec["Value"], name="OPEC Crude Oil Price", showlegend=False))
fig.add_trace(go.Scatter(x=[start_date, start_date], y=[0, 120], mode="lines", name="Start of Price War", showlegend=False))
fig.update_layout(yaxis=dict(range=[8, 82], title="OPEC Crude Oil Price ($)"), xaxis=dict(title="Date"))
plot(fig, filename="fig0.html", auto_open=False)

display(HTML("fig0.html"))
```

```
old plot code -- this cell is removed
uso = Table.read_table("USO.csv")
dates = uso.apply(pd.to_datetime, "Date")
uso = uso.with_column("Date", dates)
uso.to_df().plot.line("Date", "Close", figsize=[15,7], legend=False)
plt.vlines(dt.datetime(2020, 3, 8), -5, 125, color="r")
plt.ylim(0, 120)
plt.xlim(min(uso.column("Date")), max(uso.column("Date")))
plt.ylabel("USO Closing Price");
```

The plot above marks the official start of the price war on March 8, 2020, in red. We see some precipitous drops both just before and for a while after the start of the price war, indicating the effect that it is having on oil prices. While there are some confounding variables here (hi there, COVID-19), we still see a serious drop in price that is not just attributable to the stock market volatility that experienced by the rest of the market in early 2020.



--- END 07-game-theory/.ipynb_checkpoints/equilibria-oligopolies-checkpoint.md ---



--- START 07-game-theory/.ipynb_checkpoints/expected-utility-checkpoint.md ---

---
title: expected-utility-checkpoint
type: textbook
source_path: content/07-game-theory/.ipynb_checkpoints/expected-utility-checkpoint.ipynb
chapter: 7
---

# Expected Utility Theory

Imagine you're offered a choice between \$1 guaranteed or \$100 with probability $\frac{1}{80}$ (i.e. with probability $\frac{79}{80}$, you get \$0). Which would you choose?

In game theory, we consider rationality by examining the utility of different outcomes to individuals. To do so, we calculate the **expected utility** of a set of outcomes, which is the average of the utilities of those outcomes weighted by their probabilities. In the example above, there are two outcomes:

* \$1 guaranteed. This occurs with probability $p_1=1$ and we'll say has utility $u_1 = 1$.
* \$100 with probability $p_2 = \frac{1}{80}$. We'll say this has utility $u_2 = 100$.

The expected utility of the first choice would be $p_1 \cdot u_1 = 1$ because there is only one possible outcome and it has utility 1. The expected utility of the second choice would be $p_2 \cdot u_2 + (1 - p_2) \cdot 0 = 1.25$ because we obtain utility $u_2$ with probability $p_2$ and utility 0 with probability $1-p_2$. Notice that the expected utility of the second choice is higher! This means that, had you chosen the \$1 guaranteed, you would have made the irrational choice, because it is _expected_ that you would get \$1.25 with the second choice.

The idea that individuals, when making a gamble, will choose the option that maximizes the expected utility based on their preferences is called the **expected utility hypothesis**.

## Expected Utility

The expected utility is a calculated value based on two pieces of information: an individual's preferences for different outcomes and the probability of those outcomes occurring. Let's illustrate this with an example: Suppose Alice is deciding whether to attend lecture today and her professor is deciding whether to take attendance today. If Alice goes to lecture, she will be bored but get her attendance counted if it's taken. If she doesn't, she won't be bored, but she  won't get her attendance point. We must define the utilities for Alice and her professor in order to construct a payoff matrix.

<table class="payoff-matrix" style="text-align: center; table-layout: fixed;">

<tr>
    <td colspan="2" rowspan="2" width="200" style="border-width: 0 1px 1px 0;"></td>
    <td colspan="2">Professor</td>
</tr>
<tr>
    <td width="125">Take Attendance</td>
    <td width="125">No Attendance</td>
</tr>
<tr>
    <td rowspan="2" width="100">Alice</td>
    <td>Attend</td>
    <td>(0, 2)</td>
    <td>(-2, 0)</td>
</tr>
<tr>
    <td>Don't Attend</td>
    <td>(-5, 5)</td>
    <td>(5, 3)</td>
</tr>

</table>

```{admonition} Reading a payoff matrix
:class: tip

A payoff matrix specifies the payoffs of two players. The 2-tuples in each cell define the payoff for both players according to the combination of strategies corresponding to the row and column. For example, the bottom right cell above corresponds to the payoffs for Alice not attending and the professor not taking attendance. The tuples are formatted as `(row player, column player)`, so the first element is Alice's payoff and the second is the professor's. The payoff of (5, 0) indicates that that outcome has a utility of 5 for Alice and 0 for the professor.
```

Let's say that we know the professor's strategy: he will take attendance randomly with probability 0.7. Then we can calculate the expected utility of Alice's two options (attending and not attending) by taking the expected utility of each:

$$\begin{aligned}
E[\text{attending}] &= 0.7 (0) + 0.3 (-2) \\
&= - 0.6 \\
E[\text{not attending}] &= 0.7 (-5) + 0.3 (5) \\
&= -2
\end{aligned}$$

By calculating out Alice's expected utilities, we see that her utility is maximized by attending, given that the professor's strategy is to take attendance with probability 0.7.

An important point here is that we rely on the professor's strategy for playing the game to determine how to maximize Alice's expected utility. If the professor had a different strategy, then it could be the case that not attending would be the better option.

Let's formalize our definition of the expected utility.

```{admonition} Definition
The **expected utility** of a set of $n$ outcomes $x_i$ is the average of the utility of each outcome $u(x_i)$ weighted by the probability of that outcome's occurrence $p_i$:

$$
E[u(x)] = \sum_{i=1}^n p_i u(x_i)
$$
```

### Strategies

One of the underpinnings of game theory is the idea of **strategies**, systematic methods of playing games. There are many different ways to conceptualize strategies, some of which we've already seen. The professor's randomness strategy from the last example is one such, maximizing expected utility is another. Strategies tell the players of a game what move to make based on available information, and can be conceived of as a probability distribution over a player's choices. 

There are many different types of strategies. Any strategy that puts probability 1 on a single choice is called a **pure** strategy; all others are called **mixed** strategies. If Alice's strategy had been "never attend class," this would have been a pure strategy, because the probability of not attending was always 1. The professor's strategy was a mixed strategy, since there wasn't a single option with probability 1.



--- END 07-game-theory/.ipynb_checkpoints/expected-utility-checkpoint.md ---



--- START 07-game-theory/.ipynb_checkpoints/index-checkpoint.md ---

---
title: index-checkpoint
type: textbook
source_path: content/07-game-theory/.ipynb_checkpoints/index-checkpoint.md
chapter: 7
---

# Game Theory

**Game theory** is a branch of mathematics concerned with the study of strategic interaction among rational decision-makers. While inherently mathematical at its foundation, game theory has numerous applications in several social science disciplines, including economics. Game theory originated with the study of equilibria in zero-sum games by John von Neumann and has since expanded into many other paradigms, including applications such as a method of examining and strategizing for interactions between the US and the USSR during the Cold War and explaining the evolution and prevalence of 1:1 sex ratios in biology. In this chapter, we will study a few particular aspects of game theory and their application to economics.

There are many different ways to conceive of games and to divide them along different attributes. 

* **Cooperative vs. non-cooperative:** A game is _cooperative_ if the players are able to form commitments that can be externally enforced. A game is _non-cooperative_ if the players cannot form agreements or if those agreements need to be self-enforced.
* **Symmetric vs. asymmetric:** A game is _symmetric_ if the payoffs depend only on the strategies used and not on the players using those strategies. It is _asymmetric_ if changing the identities of the players changes the payoffs.
* **Simultaneous vs. sequential:** A game is _simultaneous_ if players move at the same time without being aware of the other players' actions. A game is _sequential_ if moves occur one after the other and players have some knowledge of the earlier actions of their competitors.
* **Perfect vs. imperfect information:** Games of _perfect information_ occur when all players know the moves previously made by all other players. If this is not the case, the game is an _imperfect-information_ game.

In this chapter, we will concern ourselves with simultaneous games of imperfect information, examining both cooperative and non-cooperative games as well as symmetric and asymmetric ones. We will also discuss different theoretical aspects, like strategies and payoffs. We will look at various common game paradigms, including one of the most popular games: the prisoner's dilemma. Lastly, we will study the economic application of game theory, including topics like equilibria and oligopolies.


--- END 07-game-theory/.ipynb_checkpoints/index-checkpoint.md ---



--- START 07-game-theory/bertrand.md ---

---
title: bertrand
type: textbook
source_path: content/07-game-theory/bertrand.ipynb
chapter: 7
---

```python
from datascience import *
import sympy
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import warnings
warnings.simplefilter("ignore")
%matplotlib inline
```

# Bertrand Competition

Another model we consider is **Bertrand competition**, named for Joseph Louis Francois Bertrand, that is similar to Cournot competition but that firms compete using _prices_ rather than quantity. Under the assumptions of this model, consumers want to buy everything at the lowest price, and if the price is the same then demand is evenly split between those producers. One fundamental assumption is that all firms have the same unit cost of production, which means that as long as the price the firm sets is above the unit cost, it is willing to supply any amount that is demanded. 

An example of a Bertrand oligopoly comes form the soft drink industry: Coke and Pepsi (which form a **duopoly**, a market with only two participants). Both firms compete by changing their prices based on a function that takes into account the price charged by their competitor. This model predicts that even this small competition will result in prices being reduced to the marginal cost level, the same outcome as perfect competition.

## Bertrand Equilibrium

To find the Bertrand equilibrium, let $c$ be the (constant) marginal cost, $p_1$ be firm 1's price level, $p_2$ be firm 2's price level, and $p_m$ be the monopoly price level. Firm 1's price depends on what it believes firm 2 will set its prices to be. Because consumers always buy at the lowest price and the firm will fulfill any level of demand, pricing just below firm 2 will obtain full market demand for firm 1. Why might this not be a good idea? If firm 2 is pricing below the level of marginal cost, then firm 1 will incur losses because they would need to sell at a price lower than the cost of production. 

Let $p'_1(p_2)$ be firm 1's optimal price based on price $p_2$ set by firm 2. The graph below shows $p'_1(p_2)$. Note that when $p_2 < c$, $p'_1$ is equal to $c$, that $p'_1$ rises linearly along but _just below_ the line $p_1 = p_2$ with $p_2$ until $p_2$ reaches $p_m$ (the monopoly price level), and that it then levels off at $p_m$. In this way, firm 1's price stays below firm 2's price when it is not operating at a loss and does not exceed $p_m$ (because $p_m$ is the profit-maximizing amount for a monopoly and producing more actually results in less profit). This piecewise function has the formula

$$
p'_1(p_2) = \begin{cases}
c & \text{if } p_2 < c + h \\
p_2 - h & \text{if } c + h \le p_2 < p_m + h \\
p_m & \text{otherwise}
\end{cases}
$$

where $h$ is a small positive value and indicates the vertical distance between $p'_1$ and the line $p_1 = p_2$. We can think of $h$ as the amount by which firm 1 will undercut firm 2: as long as firm 1 will be operating at a profit and not exceeding $p_m$, they will sell at $h$ dollars below $p_2$.

[Following image has line graphs for price response functions between two firms]

```python
p_m = 18
c = 6
dist = 0.4 # the distance from p_1 = p_2 to p'_1(p_2)

xs1 = np.linspace(0, c, 100)
ys1 = c * np.ones_like(xs1)

xs2 = np.linspace(c + dist, p_m + dist, 100)
ys2 = xs2.copy() - dist

xs3 = np.linspace(p_m + dist, 25, 100)
ys3 = p_m * np.ones_like(xs3)

xs = np.append(np.append(xs1, xs2), xs3)
ys = np.append(np.append(ys1, ys2), ys3)

plt.figure(figsize=[8,8])

plt.plot(xs, ys, label=r"$p'_1(p_2)$")

y_equals_x_xs = np.linspace(0, 25, 100)
y_equals_x_ys = y_equals_x_xs.copy()

plt.plot(y_equals_x_xs, y_equals_x_ys, label=r"$p_1 = p_2$")

plt.vlines(c, 0, c, linestyle="dashed")
plt.vlines(p_m, 0, p_m, linestyle="dashed")
plt.hlines(p_m, 0, p_m, linestyle="dashed")

plt.text(2, 6.5, r"$p'_1(p_2)$", size=16)
plt.text(17, 20, r"$p_1 = p_2$", size=16)

plt.xlim([0,22])
plt.ylim([0,22])
plt.xlabel(r"$p_2$", size=16)
plt.ylabel(r"$p_1$", size=16)
plt.xticks([c, p_m], [r"$c$", r"$p_m$"], size=14)
plt.yticks([c, p_m], [r"$c$", r"$p_m$"], size=14)
plt.legend();
```

Because firm 2 has the same marginal cost $c$ as firm 1, its reaction function $p'_2(p_1)$ is symmetrical to firm 1's about the line $p_1 = p_2$:

[Following image has line graphs for price response functions between two firms]

```python
p_m = 18
c = 6
dist = 0.4 # the distance from p_1 = p_2 to p'_1(p_2)

xs1 = np.linspace(0, c, 100)
ys1 = c * np.ones_like(xs1)

xs2 = np.linspace(c + dist, p_m + dist, 100)
ys2 = xs2.copy() - dist

xs3 = np.linspace(p_m + dist, 25, 100)
ys3 = p_m * np.ones_like(xs3)

xs_1 = np.append(np.append(xs1, xs2), xs3)
ys_1 = np.append(np.append(ys1, ys2), ys3)

ys1 = np.linspace(0, c, 100)
xs1 = c * np.ones_like(xs1)

ys2 = np.linspace(c + dist, p_m + dist, 100)
xs2 = xs2.copy() - dist

ys3 = np.linspace(p_m + dist, 25, 100)
xs3 = p_m * np.ones_like(xs3)

xs_2 = np.append(np.append(xs1, xs2), xs3)
ys_2 = np.append(np.append(ys1, ys2), ys3)

plt.figure(figsize=[8,8])

plt.plot(xs_1, ys_1, label=r"$p'_1(p_2)$")
plt.plot(xs_2, ys_2, label=r"$p'_2(p_1)$")

y_equals_x_xs = np.linspace(0, 25, 100)
y_equals_x_ys = y_equals_x_xs.copy()

plt.plot(y_equals_x_xs, y_equals_x_ys, label=r"$p_1 = p_2$")

plt.vlines(p_m, 0, p_m, linestyle="dashed")
plt.hlines(p_m, 0, p_m, linestyle="dashed")

plt.text(2, 6.5, r"$p'_1(p_2)$", size=16)
plt.text(6.5, 2, r"$p'_2(p_1)$", size=16)
plt.text(18.2, 21, r"$p_1 = p_2$", size=16)

plt.xlim([0,22])
plt.ylim([0,22])
plt.xlabel(r"$p_2$", size=16)
plt.ylabel(r"$p_1$", size=16)
plt.xticks([c, p_m], [r"$c$", r"$p_m$"], size=14)
plt.yticks([c, p_m], [r"$c$", r"$p_m$"], size=14)
plt.legend();
```

These two strategies form a Nash equilibrium because neither firm can increase profits by changing their own strategy unilaterally. The equilibrium occurs where $p_1 = p'_1(p_2)$ and $p_2 = p'_2(p_1)$, at the intersection of the two reaction curves. Notably, this means that the Bertrand equilibrium occurs when both firms are producing _at marginal cost_. 

This makes intuitive sense: say that the two firms both set equal prices at a price above $c$ where they split demand equally. Then both firms have incentive to reduce their price slightly and take the other half of the market share from their competitor. Thus, both firms are tempted to lower prices as much as possible, but lowering below the level of marginal cost makes no sense because then they're operating at a loss. Thus, both firms sell at the price level $c$.

## Implications

The Bertrand model implies that even a duopoly in a market is enough to push prices down to the level of perfect competition. It does, however, rely on some serious assumptions. For example, there are many reasons why consumers might not buy the lowest-priced item (e.g. non-price competition, search costs). When these factors are included in the Bertrand model, the same result is no longer reached. It also ignores the fact that firms may not be able to supply the entire market demand; including these capacity constraints in the model can result in the system having no Nash equilibrium. Lastly, the Bertrand model demonstrates big incentives to cooperate and raise prices to the monopoly level; however, this state is not a Nash equilibrium, and in fact, the only Nash equilibrium of this model is the non-cooperative one with prices at marginal cost.

## Applying Bertrand

Now that we have derived the Bertrand equilibrium, let's apply it to a problem. Consider the Coke-Pepsi duopoly we mentioned above. Suppose that the only product in the soft-drink market is the 12-oz. can, that the market demand for cans is given by $P = -0.05 Q + 5.05$, and that the marginal cost for Coke and Pepsi is constant at $c = 0.25$. To find the equilibrium price for Coke based in it's belief that Pepsi will sell at $p_2 = 1$, we need to start by finding the monopoly price level $p_m$; recall from Cournot that this occurs when the marginal revenue curve of the market demand intersects the marginal cost. The marginal revenue is $r(q) = -0.1 q + 5.05$:

[Following image has line graphs for Demand, Marginal Revenue and Cost]

```python
P_fn = lambda x: -0.05 * x + 5.05
r_fn = lambda x: 2 * P_fn(x) - P_fn(0)
Qs = np.linspace(-10, 1000, 1000)
Ps = P_fn(Qs)
rs = r_fn(Qs)

plt.figure(figsize=[7,7])
plt.plot(Qs, Ps, label=r"$P(Q)$")
plt.plot(Qs, rs, label=r"$r(Q)$")
plt.hlines(.25, -10, 150, color="r", label=r"$c$")
plt.xlim([0, 110])
plt.ylim([0, 5.5])
plt.xlabel(r"Quantity $Q$", size=16)
plt.ylabel(r"Price $P$", size=16)
plt.legend();
```

Using SymPy, we can find the quantity $q$ at which $r(q) = c$. This value, denoted $q_m$, is the monopoly quantity.

```python
c = 0.25
q = sympy.Symbol("q")
r = -.1 * q + 5.05

q_m = solve(r, c)
q_m
```

The monopoly price $p_m$ is the price from the market demand curve that this level of output:

```python
Q = sympy.Symbol("Q")
P = -.05 * Q + 5.05

p_m = P.subs(Q, q_m)
p_m
```

Now that we have found $p_m$, we can use this to construct Coke's reaction function $p'_1(p_2)$ to Pepsi's choice of price. Assuming Coke selects $h=0.1$ (that is, Coke will sell at \$0.10 below Pepsi as long as they operate at a profit), the formula for $p'_1$ is

$$
p'_1(p_2) = \begin{cases}
0.25 & \text{if } p_2 < 0.25 + 0.1 \\
p_2 - 0.1 & \text{if } 0.25 + 0.1 \le p_2 < 2.65 + 0.1 \\
2.65 & \text{otherwise}
\end{cases}
$$

[Following image has piecewise line graphs for how one firm reacts to anothers price setting]

```python
c = 0.25
def p1_fn(p_2, h):
    if p_2 < c + h:
        return c
    elif c + h <= p_2 < p_m + h:
        return p_2 - h
    else:
        return p_m

p1_fn = np.vectorize(p1_fn)

xs = np.linspace(-1, 4, 1000)
ys = xs.copy()
p1s =  p1_fn(xs, .1)

plt.figure(figsize=[7,7])
plt.plot(xs, p1s, label=r"$p'_1(p_2)$")
plt.plot(xs, ys, label=r"$p_1 = p_2$")
plt.hlines(c, 0, 5, color="r", label=r"$c$")
plt.vlines(c, 0, 5, color="r")
plt.hlines(p_m, 0, 5, color="g", label=r"$p_m$")
plt.vlines(p_m, 0, 5, color="g")
plt.xlim([0,3])
plt.ylim([0,3])
plt.xlabel(r"$p_2$", size=16)
plt.ylabel(r"$p_1$", size=16)
plt.legend();
```

Finally, to find Coke's selling price, we find $p'_1(1)$, since Coke believes Pepsi will sell at \$1.

```python
c = 0.25
h = 0.1
p_2 = sympy.Symbol("p_2")
p_1_prime = sympy.Piecewise(
    (c, p_2 < c + h),
    (p_2 - h, p_2 < p_m + h),
    (p_m, True)
)

p_1_prime.subs(p_2, 1)
```

Thus, if Coke believes that Pepsi will sell cans at \$1, it should sell at \$0.90. This should make intuitive sense: we showed that $p'_1(p_2)$ was below the line $p_1=p_2$ by a vertical distance of $h=0.1$, so it makes sense that Coke would sell at \$0.10 below Pepsi. If Coke had believed that Pepsi was going to sell at, say, \$2.70, then it would have been better for them to sell at the monopoly price level $p_m = 2.65$. If Pepsi was selling below margin cost, at \$0.20 maybe, then Coke's best bet would have been to sell at $c = 0.25$, although they would have sold 0 units of output because consumers buy from the lowest-priced vendor.



--- END 07-game-theory/bertrand.md ---



--- START 07-game-theory/cournot.md ---

---
title: cournot
type: textbook
source_path: content/07-game-theory/cournot.ipynb
chapter: 7
---

```python
from datascience import *
import sympy
solve = lambda x,y: sympy.solve(x-y)[0] if len(sympy.solve(x-y))==1 else "Not Single Solution"
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import warnings
warnings.simplefilter("ignore")
%matplotlib inline
```

# Cournot Competition

One model of understanding oligopolies comes in Cournot competition, named for Aontoine Cournot who first proposed it. Cournot competition is a model describing a market in which firms compete by changing their output. In Cournot competition, there are a fixed number of firms in a market that produce the same product. Firms do not collude but have market power (i.e. each firm’s output decisions affect the price of the good). Each firm knows the number of firms in the market and has its own cost function that it uses to determine its level of output.

OPEC is a good example of a Cournot oligopoly: its participants affect prices by changing their output. OPEC also demonstrates a flaw in the reasoning behind Cournot competition: the equilibrium state of a Cournot oligopoly suggests that collusion by market participants is the rational policy, but in reality game theory shows us this cannot be the “true” equilibrium because cartel members undercut one another in a bid for market share.

## Cournot Profit Functions
For the purposes of this class, we will be analyzing the case of Cournot duopoly (i.e. when there are only 2 firms in a market). Throughout the course we will make these assumptions about the market: both firms have to follow a joint linear price function (meaning the market price is determined only by the quantity produced of both firms) and both firms have an equal constant marginal cost curve. To put this formally, $P=P(Q)=P(q_1+q_2)=m(q_1+q_2)+b$ (where $m$ and $b$ are parameters for the slope and intercept of the price functions). Costs for firm 1 can be represented as $C_1=cq_1$ and costs for firm 2 can be represented as $C_2=cq_2$. To find the equilibrium of this game we will need to set up the profit functions for both firms. Starting with firm 1 we can put the last two equations together to get:

$$
\begin{equation} \label{eq1}
\begin{split}
\pi_1 & =Pq_1-C_1 \\
 & =(m(q_1+q_2)+b)q_1-cq_1 \\
 & =mq_1^2+mq_1q_2+bq_1 -cq_1
\end{split}
\end{equation}
$$

Firm 2 follows a similar profit function:

$$
\begin{align*}
\pi_2 & =Pq_2-C_2\\
 & =mq_2^2+mq_1q_2+bq_2 -cq_2   
\end{align*}
$$

## Best Response Functions
Similar to how in the utility unit individuals seek to maximize utility, in Cournot firms seek to maximize profit. So we will be using the same mathematical tool we learned last week (partial derivatives) to derive the equilibrium of this game. Specifically we will be solving for a $q_1^{*}$ that satisfies $ \underset{q_1}{\mathrm{argmax}}\ \pi_1 \ \textrm{or} \ \underset{q_1}{\mathrm{argmax}}\ (mq_1^2+mq_1q_2+bq_1 -cq_1)$. In english, this means we will be finding the value (or values) of $q_1$ that maximize the profit function for player 1. We will also do the same thing for player 2. Starting with player 1, let's take the partial derivative with respect to $q_1$ and set that equation equal to 0. We can then solve that equation to obtain $q_1^{*}$. 

$$
\begin{align*}
   \frac{\partial{\pi_1}}{\partial{q_1}}= 2mq_1 + mq_2+b - c \\
   2mq_1 + mq_2+b - c = 0\\
  q_1^{*}= \frac{c-mq_2-b}{2m}
\end{align*}
 \
$$
This equation for $q_1$ is called a best response function because it tells us player 1's optimal output for every possible output player 2 can choose. Now, in order to get the equilibrium, we need to find player 2's best response function. We will use the same approach to do this (i.e., we will solve for a $q_2^{*}$ that satisfies $\underset{q_2}{\mathrm{argmax}}\ \pi_2 \ \textrm{or} \ \underset{q_2}{\mathrm{argmax}}\ mq_2^2+mq_1q_2+bq_2 -cq_2$ ).
 
$$
 \begin{align*}
   \frac{\partial{\pi_2}}{\partial{q_2}}= 2mq_2 + mq_1+b - c \\ 
   2mq_2 + mq_1+b - c = 0\\
   q_2^{*} = \frac{c-mq_1-b}{2m}
\end{align*}
$$

If $q_2$ was fixed at a certain price and we were to graph firm 1's best response equation, it would look something like this:

[Following image has line graphs for downward sloping market demand and marginal revenue functions]

```python
m, b = -0.5, 9
q_2 = 4
c = 3
D_fn = lambda x: m * x + b + 2
P = lambda x: D_fn(x + q_2)
r_1_fn = lambda x: 2 * m * x + m * q_2 + b + 2
r_1_inv = lambda x: (x - m * q_2 - b - 2) / 2 / m

q_1 = np.linspace(0, 25, 1000)
D = D_fn(q_1)
d_1 = P(q_1)
r_1 = r_1_fn(q_1)

q_1_prime = r_1_inv(c)

plt.figure(figsize=[8,8])

# demand
plt.plot(q_1, D, label=r"Market Demand, $D$")
plt.text(5, b, r"$D$", size=16)



# marginal revenue
plt.plot(q_1, r_1, label=r"Marginal Revenue of Firm 1, $r_1$")
plt.text(5, b - 4.5, r"$r_1$", size=16)

# marginal cost
plt.plot(q_1, c * np.ones_like(q_1), label=r"Marginal Cost, $c$")
plt.text(20, 3.2, r"$c$", size=16)

# optimum quantity
plt.vlines(q_1_prime, 0, 3, linestyles="dashed")



plt.xlim([0,22])
plt.ylim([0,12])
plt.xticks([r_1_inv(c)], [r"$q_1^*$"], size=14)
plt.yticks([P(0)], [r"$P(q_2)$"], size=14)
plt.xlabel("Quantity", size=16)
plt.ylabel("Price", size=16)
plt.legend();
```

This may make intuitive sense as if you recall from the demand and supply chapters, firms seek to produce where marginal revenue equals marginal cost. The partial derivative we took before did that automatically. We can verify this with the following calculation:  

$$
\begin{align*}
    MR=MC  \\
    MR-MC=0  \\
    \frac{\partial}{\partial{q_1}} (\textrm{Revenue - Cost}) = 0 \\
    \frac{\partial}{\partial{q_1}} ((m(q_1+q_2)+b)q_1-cq_1) = 0 \\
    2mq_1 + mq_2+b - c = 0 
\end{align*}
$$

However, this graph assumes a fixed value for $q_2$. Let's try varying $q_2$ and see what happens.

[Following image has line graphs for downward sloping market demand and marginal revenue functions]

```python
m, b = -0.5, 9
q_2 = 0
c = 3
D_fn = lambda x: m * x + b + 2
P = lambda x: D_fn(x + q_2)
r_1_fn = lambda x: 2 * m * x + m * q_2 + b + 2
r_1_inv = lambda x: (x - m * q_2 - b - 2) / 2 / m

q_1 = np.linspace(0, 25, 1000)
D = D_fn(q_1)
d_1 = P(q_1)
r_1 = r_1_fn(q_1)

q_1_prime = r_1_inv(c)

plt.figure(figsize=[8,8])

# demand
plt.plot(q_1, D, label=r"Market Demand, $D$")
plt.text(5, b, r"$D$", size=16)


# marginal revenue
plt.plot(q_1, r_1, label=r"Marginal Revenue of Firm 1 for $q_2=0$, $r_1$")
plt.text(5, b - 2.5, r"$r_1$", size=16)

# case when q_2 = q_c
m, b = -0.5, 9
q_2 = 16
c = 3
D_fn = lambda x: m * x + b + 2
P = lambda x: D_fn(x + q_2)
r_1_fn = lambda x: 2 * m * x + m * q_2 + b + 2
r_1_inv = lambda x: (x - m * q_2 - b - 2) / 2 / m

q_1 = np.linspace(0, 25, 1000)
D = D_fn(q_1)
d_1 = P(q_1)
r_1 = r_1_fn(q_1)


# marginal revenue
plt.plot(q_1, r_1, label=r"Marginal Revenue of Firm 1 for $q_2=q_m$, $r_1$")
plt.text(2.5, 0.8, r"$r_1$", size=16)

# marginal cost
plt.plot(q_1, c * np.ones_like(q_1), label=r"Marginal Cost, $c$")
plt.text(20, 3.2, r"$c$", size=16)


# q'_1(0)
plt.vlines(r_1_inv(-5), 0, 3, linestyles="dashed")

plt.xlim([0,22])
plt.ylim([0,12])
plt.xticks([r_1_inv(-5), 0], [r"$q_1^*(0) = q_m$", r"$q_1^*(q_m)$"], size=14)
plt.yticks([], [], size=14)
plt.xlabel("Quantity", size=16)
plt.ylabel("Price", size=16)
plt.legend();
```

The orange marginal revenue curve represents a scenario where Firm 2 produces so little that Firm 1 can overtake the market and become a monopoly. The green marginal revenue curve is a scenario where Firm 2 produces so much that it is too costly for Firm 1 to remain in this market, causing Firm 1's optimal output to become 0. 

Let's now try to solve for the Nash Equilibrium of this game.

## Cournot Equilibrium
Recall that a Nash Equilibrium is where both the players are giving the best response to each other's actions. Graphically, this is where the best response curves intersect. 

[Following image has line graphs for the intersecting lines of response functions between two firms]

```python
q_m = r_1_inv(-5)
q_c = D_fn(-10)

p1 = (0, q_m)
p2 = (q_c, 0)

slope = (p2[1] - p1[1]) / (p2[0]- p1[0])
intercept = p1[1] - slope * p1[0]

xs = np.linspace(0, 25, 1000)
ys1 = slope * xs + intercept
ys2 = (xs - intercept) / slope

plt.figure(figsize=[8,8])

plt.plot(xs, ys1, label=r"$q'_1$")
plt.plot(xs, ys2, label=r"$q'_2$")
plt.text(13, intercept - 6.3, r"$q_1^*(q_2)$", size=16)
plt.text(3, intercept + 2.5, r"$q_2^*(q_1)$", size=16)

x_star = - (slope + 1) * intercept / (slope**2 - 1)
y_star = slope * x_star + intercept
plt.vlines(x_star, 0, y_star, linestyle="dashed")
plt.hlines(y_star, 0, x_star, linestyle="dashed")

plt.xlim([0,22])
plt.ylim([0,12])
plt.xticks([x_star], [r"$q_2^*$"], size=14)
plt.yticks([y_star], [r"$q_1^*$"], size=14);
```

In this class, you will use Python to find that intersection. However, for future classes, it may be useful to see how you can find the intersection algebraically. Here's the equation we must solve:

$$
\begin{align*}
    q_1=\frac{c-b-m(\frac{c-mq_1-b}{2m})}{2m}
\end{align*}
\
$$

To find player 1's equilibrium strategy, we solve for $q_1$. 

$$
\begin{align*}
    q_1=\frac{c-b-m(\frac{c-mq_1-b}{2m})}{2m} \\ 
    2mq_1 = c-b-m(\frac{c-mq_1-b}{2m}) \\
    2mq_1 = c-b-\frac{c-mq_1-b}{2}\\
    2mq_1 - \frac{mq_1}{2} = c - \frac{c}{2} -b +\frac{b}{2} \\
    \frac{3mq_1}{2} = \frac{c-b}{2} \\ 
    3mq_1 = c-b \\
    q_1^{*} = \frac{c-b}{3m}
\end{align*}
\
$$

Phew, that was a lot of math. However, we are not quite done yet - we still need to find player 2's optimal output. The process for doing this is fairly similar to the process for finding player 1's optimal output, as can be seen below. 

$$
\begin{align*}
    q_2 = \frac{c-mq_1^{*}-b}{2m} = \frac{c-b-m(\frac{c-b}{3m})}{2m} \\
    2mq_2 = c -\frac{c}{3} - b +\frac{b}{3} \\
    2mq_2 = \frac{2(c-b)}{3}\\
    q_2^{*} = q_2 = \frac{c-b}{3m}
\end{align*}
\
$$
This shows that the unique Nash Equilibrium of Cournot Duopoly is $(q_1^{*},q_2^{*}) = (\frac{c-b}{3m},\frac{c-b}{3m})$

## Implications
The Cournot model implies that output is greater in a Cournot duopoly than in a monopoly but still lower than perfect competition. Prices are also lower in a Cournot duopoly, but higher than perfect competition. Cournot equilibria are also a subset of Nash equilibria, and so the equilibrium we just derived is one from which neither player will likely deviate. As noted earlier, Cournot also indicates that members of a duopoly could form a cartel and raise profits by colluding.

### Applying Cournot
Now that we have shown how to derive the Cournot equilibrium, let’s apply this to a problem. Consider the industry of airline manufacturing: there are two main competitors, Boeing and Airbus, and for this problem, we will think of this market as a Cournot duopoly. Suppose the market demand curve is given by $P=-1.89Q+148.89$ where the price is in millions of dollars and that the marginal cost of both firms is constant at $c=100$. Solve for the Nash Equilibrium. 


First we will want to derive both firms profit functions:

$$
\begin{align*} 
    \pi_1 & = Pq_1 -cq_1 \\
    \pi_1 & = (-1.89Q+148.89)q_1 - cq_1 \\
    \pi_1 & = (-1.89(q_1+q_2)+148.89)q_1 -cq_1 \\
    \pi_1 & = -1.89q_1^2-1.89q_1q_2+148.89q_1 - cq_1
\end{align*}
$$

For firm 2 the profit function similarly looks like:

$$
\begin{align*}
    \pi_2 = -1.89q_2^2-1.89q_1q_2+148.89q_2 - cq_2
\end{align*}
$$

Now we will take the partial derivative to find the best response functions for both players. For firm 1 this looks like:

$$
\begin{align*}
    \frac{\partial{\pi_1}}{\partial{q_1}}= -3.78q_1 - 1.89q_2+148.89 - 100 = 0\\
    q_1 = \frac{48.89-1.89q_2}{3.78}
\end{align*}
\
$$

For firm 2 this looks like:

$$
\begin{align*}
    \frac{\partial{\pi_2}}{\partial{q_2}}= -3.78q_2 - 1.89q_1+148.89 - 100 = 0\\
    q_2 = \frac{48.89-1.89q_1}{3.78}
\end{align*}
$$

Now while you can solve this algebraically, it takes much longer and leaves a lot of room for error, so as data scientists we will use Python to solve this problem. The code for this looks like this:

```python
q_1=sympy.Symbol("q_1") #Create a symbol for q1
q_2=sympy.Symbol("q_2")#Create a symbol for q2
Br_q_1 = (48.89 - 1.89*q_2)/3.78 #Create the best response function for firm 1
Br_q_2 = (48.89 - 1.89*q_1)/3.78 #Create the best response function for firm 2
q1_Br_2 = (48.89-3.78*q_2)/1.89 #Solve firm 2's best response function for q1
q_1_star = solve(Br_q_1,q1_Br_2) #Use the solve function to solve the system of equations 
q_2_star = Br_q_2.subs(q_1,q_1_star ) #Substiute in the optimal q_1_star into firm 2's best response function
(q_1_star,q_2_star) #Nash Equilibrium
```


--- END 07-game-theory/cournot.md ---



--- START 07-game-theory/equilibria-oligopolies.md ---

---
title: equilibria-oligopolies
type: textbook
source_path: content/07-game-theory/equilibria-oligopolies.ipynb
chapter: 7
---

```python
from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import datetime as dt
import warnings
warnings.simplefilter("ignore")
import plotly.graph_objects as go
from plotly.offline import plot
from IPython.display import display, HTML
%matplotlib inline
```

# Equilibria & Oligopolies

This section introduces the concept of equilibria in games, the paradigm of the prisoner's dilemma, and oligopolies. These concepts are essential to understanding the models we will be studying the next sections, and for relating game theory to its economic applications.

## Equilibrium

The [prisoner's dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) is a classic game first discussed by Merrill Flood and Melvin Dresher in 1950. In this game, there are two prisoners who have been captured and are being interrogated. The prisoners cannot contact each other in any way. They have two options: they can **defect** (betray the other prisoner to the police) or they can **cooperate** (maintain their silence). If both defect, both receive 4 years in prison. If one defects and the other does not, the defector goes free and the cooperator receives 5 years in prison. If both cooperate (meaning neither talks to the police), then they each receive 2 years in prison. We define **mutual defection** as the case when both prisoners defect and **mutual cooperation** as the case when both cooperate. The purpose of this game is to consider how a completely rational person would be best advised to proceed, and how different strategies for playing this game can be more or less effective.

![payoff2.png](payoff2.png)

In the above payoff matrix, we see that if Prisoner A cooperates and Prisoner B defects, then Prisoner A gets 5 years in prisoner and Prisoner B gets none. It is important to note that the above payoff matrix is inverted; because we are talking about _years in prison_ as opposed to utility, a higher value is **worse** for the player, and a player's goal is to _minimize_ their value in the payoff matrix, not maximize it as normally. We will use this payoff matrix convention when discussing the prisoner's dilemma, but _not_ with other games.

An important concept in game theory is finding the equilibrium of a game. There are different types of equilibria, but the most common one considered is the **Nash equilibrium**, named for the mathematician John Forbes Nash, Jr. (who you may remember as a character played by Russel Crowe in [_A Beautiful Mind_](https://en.wikipedia.org/wiki/A_Beautiful_Mind_(film))). A Nash equilibrium occurs when no player can increase their own payoff by changing only their own strategy.

```{admonition} Definition
A **Nash equilibrium** is a set of strategy choices in a non-cooperative game in which each player, assumed to know the equilibrium strategies of the other players, has a chosen strategy and there can be no monotonic improvement; that is, no player can increase their payoffs by changing their strategy without another player changing _their_ strategy.
```

Using this definition, what constitutes a Nash equilibrium for the prisoner's dilemma? Well let's consider the four possible combinations of strategies. (We use "D" as shorthand for "defect" and "C" for "cooperate" below.)

1. **Prisoner A: C, Prisoner B: C.** In this case, both Prisoner A and Prisoner B get 2 years. However, if Prisoner A's strategy remains unchanged, Prisoner B can get fewer years in prison by changing to D, and vice versa. Thus, this is **not** a Nash equilibrium.
2. **Prisoner A: D, Prisoner B: C.** In this case, Prisoner B can change to D and lower their years from 5 to 4. Thus, this is **not** a Nash equilibrium.
3. **Prisoner A: C, Prisoner B: D.** As with (2), Prisoner A can change to D and lower their years in prison. Thus, this is **not** a Nash equilibrium.
4. **Prisoner A: D, Prisoner B: D.** In this case, if Prisoner A changes to C, then their years in prison _increases_ from 4 to 5, as with Prisoner B. Neither player can increase their winnings (decrease their years in prison) by changing their strategy. Thus, this **is** a Nash equilibrium, with both Prisoner A and Prisoner B getting 4 years.

By describing the four states of the game, we see that only mutual defection is a Nash equilibrium of the prisoner's dilemma.

```{admonition} Nash equilibria in payoff matrices
:class: tip

It is easy to use a payoff matrix to  see which  states, if any, are Nash equilibria. To see if a game state is a Nash equilibrium, the first value in the cell must be the maximum among all first values in that column and the second value must be the maximum across all second values in that row. This is because moving up and down a column represents changing the row-player's strategy and  moving along a row represents changing the column-player's strategy. If these values are both maxima in their respective directions, then no player can obtain a better payoff by unilaterally changing their strategy, and we have a Nash equilibrium.

You can easily verify this by looking at the payoff matrix for the prisoner's dilemma: the  first 4 in mutual defection has the best payoff in the column and the second  4 has the best payoff in the row.
```

## Oligopolies

One of the most common applications of game theory in economics is the study of **oligopolies**, markets where the number of participants is limited. There are several examples of oligopolies that we experience without knowing in daily life: airlines, soft drinks, and telecom providers, to name a few. Oligopolies are different from regular markets in that they allow their participants to function similar to a monopoly by setting prices as a group; groups of participants conspiring on this kind of illicit activity are referred to as **cartels**. 

Within oligopolies, however, we can observe competition more like a normal market as firms attempt to take market share from one another. When cartels set prices (by limiting the production of the good or service provided by their market), a firm can make a bid for market share by ignoring the agreed-upon production level and producing more. This has the effect of lowering the price of the good but the increase in production by the renegade firm will allow them to make up for the lost marginal revenue through increased sales volume. In this way, oligopoly members can compete against each other, making the market more and more competitive.

A prime example of this type of within-cartel competition was observed in the [2020 oil price war between Russia and Saudi Arabia](https://en.wikipedia.org/wiki/2020_Russia%E2%80%93Saudi_Arabia_oil_price_war). Both countries are members of [OPEC (the Organization of Petroleum Exporting Countries)](https://en.wikipedia.org/wiki/OPEC), an oil cartel that consists of 12 member countries controlling 79% of the world's oil reserves and 44% of oil production. OPEC sets oil prices by limiting the output of its member countries (a decrease in production results in higher prices).

The price war began when Saudi Arabia discounted its oil in response to Russia's refusal to reduce production in accordance with OPEC's directive. OPEC's members had agreed to reduce oil production due to a low forecasted demand for oil due to the COVID-19 pandemic. When Russia (who was not an official member of OPEC but had agreed to cooperate with Saudi Arabia to manage oil prices) didn't abide  by OPEC's decision, Saudi Arabia announced discounts on its oil, starting the price war and leading to a massive drop in the price of oil. We can see the effects of this price war by looking at the price per barrel of OPEC's crude oil, plotted below.

```python
# data from https://www.quandl.com/data/OPEC/ORB-OPEC-Crude-Oil-Price
opec = pd.read_csv("opec_prices.csv")
opec["Date"] = opec["Date"].apply(pd.to_datetime)
opec = opec[opec["Date"] >= dt.datetime(2019, 7, 1)]
start_date = dt.datetime(2020, 3, 8)

fig = go.Figure()
fig.add_trace(go.Scatter(x=opec["Date"], y=opec["Value"], name="OPEC Crude Oil Price", showlegend=False))
fig.add_trace(go.Scatter(x=[start_date, start_date], y=[0, 120], mode="lines", name="Start of Price War", showlegend=False))
fig.update_layout(yaxis=dict(range=[8, 82], title="OPEC Crude Oil Price ($)"), xaxis=dict(title="Date"))
plot(fig, filename="fig0.html", auto_open=False)

display(HTML("fig0.html"))
```

```
old plot code -- this cell is removed
uso = Table.read_table("USO.csv")
dates = uso.apply(pd.to_datetime, "Date")
uso = uso.with_column("Date", dates)
uso.to_df().plot.line("Date", "Close", figsize=[15,7], legend=False)
plt.vlines(dt.datetime(2020, 3, 8), -5, 125, color="r")
plt.ylim(0, 120)
plt.xlim(min(uso.column("Date")), max(uso.column("Date")))
plt.ylabel("USO Closing Price");
```

The plot above marks the official start of the price war on March 8, 2020, in red. We see some precipitous drops both just before and for a while after the start of the price war, indicating the effect that it is having on oil prices. While there are some confounding variables here (hi there, COVID-19), we still see a serious drop in price that is not just attributable to the stock market volatility that experienced by the rest of the market in early 2020.



--- END 07-game-theory/equilibria-oligopolies.md ---



--- START 07-game-theory/expected-utility.md ---

---
title: expected-utility
type: textbook
source_path: content/07-game-theory/expected-utility.ipynb
chapter: 7
---

# Expected Utility Theory

Imagine you're offered a choice between \$1 guaranteed or \$100 with probability $\frac{1}{80}$ (i.e. with probability $\frac{79}{80}$, you get \$0). Which would you choose?

In game theory, we consider rationality by examining the utility of different outcomes to individuals. To do so, we calculate the **expected utility** of a set of outcomes, which is the average of the utilities of those outcomes weighted by their probabilities. In the example above, there are two outcomes:

* \$1 guaranteed. This occurs with probability $p_1=1$ and we'll say has utility $u_1 = 1$.
* \$100 with probability $p_2 = \frac{1}{80}$. We'll say this has utility $u_2 = 100$.

The expected utility of the first choice would be $p_1 \cdot u_1 = 1$ because there is only one possible outcome and it has utility 1. The expected utility of the second choice would be $p_2 \cdot u_2 + (1 - p_2) \cdot 0 = 1.25$ because we obtain utility $u_2$ with probability $p_2$ and utility 0 with probability $1-p_2$. Notice that the expected utility of the second choice is higher! This means that, had you chosen the \$1 guaranteed, you would have made the irrational choice, because it is _expected_ that you would get \$1.25 with the second choice.

The idea that individuals, when making a gamble, will choose the option that maximizes the expected utility based on their preferences is called the **expected utility hypothesis**.

## Expected Utility

The expected utility is a calculated value based on two pieces of information: an individual's preferences for different outcomes and the probability of those outcomes occurring. Let's illustrate this with an example: Suppose Alice is deciding whether to attend lecture today and her professor is deciding whether to take attendance today. If Alice goes to lecture, she will be bored but get her attendance counted if it's taken. If she doesn't, she won't be bored, but she  won't get her attendance point. We must define the utilities for Alice and her professor in order to construct a payoff matrix.

![payoff1.png](payoff1.png)

```{admonition} Reading a payoff matrix
:class: tip

A payoff matrix specifies the payoffs of two players. The 2-tuples in each cell define the payoff for both players according to the combination of strategies corresponding to the row and column. For example, the bottom right cell above corresponds to the payoffs for Alice not attending and the professor not taking attendance. The tuples are formatted as `(row player, column player)`, so the first element is Alice's payoff and the second is the professor's. The payoff of (5, 0) indicates that that outcome has a utility of 5 for Alice and 0 for the professor.
```

Let's say that we know the professor's strategy: he will take attendance randomly with probability 0.7. Then we can calculate the expected utility of Alice's two options (attending and not attending) by taking the expected utility of each:

$$\begin{aligned}
E[\text{attending}] &= 0.7 (0) + 0.3 (-2) \\
&= - 0.6 \\
E[\text{not attending}] &= 0.7 (-5) + 0.3 (5) \\
&= -2
\end{aligned}$$

By calculating out Alice's expected utilities, we see that her utility is maximized by attending, given that the professor's strategy is to take attendance with probability 0.7.

An important point here is that we rely on the professor's strategy for playing the game to determine how to maximize Alice's expected utility. If the professor had a different strategy, then it could be the case that not attending would be the better option.

Let's formalize our definition of the expected utility.

```{admonition} Definition
The **expected utility** of a set of $n$ outcomes $x_i$ is the average of the utility of each outcome $u(x_i)$ weighted by the probability of that outcome's occurrence $p_i$:

$$
E[u(x)] = \sum_{i=1}^n p_i u(x_i)
$$
```

### Strategies

One of the underpinnings of game theory is the idea of **strategies**, systematic methods of playing games. There are many different ways to conceptualize strategies, some of which we've already seen. The professor's randomness strategy from the last example is one such, maximizing expected utility is another. Strategies tell the players of a game what move to make based on available information, and can be conceived of as a probability distribution over a player's choices. 

There are many different types of strategies. Any strategy that puts probability 1 on a single choice is called a **pure** strategy; all others are called **mixed** strategies. If Alice's strategy had been "never attend class," this would have been a pure strategy, because the probability of not attending was always 1. The professor's strategy was a mixed strategy, since there wasn't a single option with probability 1.



--- END 07-game-theory/expected-utility.md ---



--- START 07-game-theory/index.md ---

---
title: index
type: textbook
source_path: content/07-game-theory/index.md
chapter: 7
---

# Game Theory

**Game theory** is a branch of mathematics concerned with the study of strategic interaction among rational decision-makers. While inherently mathematical at its foundation, game theory has numerous applications in several social science disciplines, including economics. Game theory originated with the study of equilibria in zero-sum games by John von Neumann and has since expanded into many other paradigms, including applications such as a method of examining and strategizing for interactions between the US and the USSR during the Cold War and explaining the evolution and prevalence of 1:1 sex ratios in biology. In this chapter, we will study a few particular aspects of game theory and their application to economics.

There are many different ways to conceive of games and to divide them along different attributes. 

* **Cooperative vs. non-cooperative:** A game is _cooperative_ if the players are able to form commitments that can be externally enforced. A game is _non-cooperative_ if the players cannot form agreements or if those agreements need to be self-enforced.
* **Symmetric vs. asymmetric:** A game is _symmetric_ if the payoffs depend only on the strategies used and not on the players using those strategies. It is _asymmetric_ if changing the identities of the players changes the payoffs.
* **Simultaneous vs. sequential:** A game is _simultaneous_ if players move at the same time without being aware of the other players' actions. A game is _sequential_ if moves occur one after the other and players have some knowledge of the earlier actions of their competitors.
* **Perfect vs. imperfect information:** Games of _perfect information_ occur when all players know the moves previously made by all other players. If this is not the case, the game is an _imperfect-information_ game.

In this chapter, we will concern ourselves with simultaneous games of imperfect information, examining both cooperative and non-cooperative games as well as symmetric and asymmetric ones. We will also discuss different theoretical aspects, like strategies and payoffs. We will look at various common game paradigms, including one of the most popular games: the prisoner's dilemma. Lastly, we will study the economic application of game theory, including topics like equilibria and oligopolies.


--- END 07-game-theory/index.md ---



--- START 07-game-theory/python-classes.md ---

---
title: python-classes
type: textbook
source_path: content/07-game-theory/python-classes.ipynb
chapter: 7
---

# Python Classes

Because Python is an [**object-oriented** programming language](https://en.wikipedia.org/wiki/Object-oriented_programming), you can create custom structures for storing data and methods called **classes**. A class represents an object and stores variables related to and functions that operate on that object. You're already familiar with Python classes, even if you don't know it: the `Table`s you work with in Data 8 are Python classes, as are NumPy arrays.

We use classes because they allow us to store data in a rigorously structured way and provide standardized methods of accessing and interacting with that data. For example, let's say you want to create a program that manages a person's banking information. You need to store their name, account number, and balance. You might do something like create an array for each individual, where the first element is their name, the second is their account number, and the third is their balance:

```python
account1 = make_array("Jane Doe", 123456, 100)
account2 = make_array("John Doe", 234567, 80)
```

But what happens if you need to track more data? Or suppose the structure of this data changes? Then you need to go to _every_ place where you access an element of the array and update it! It's really easy to forget things like this or to have instances fall through the cracks. Instead, we might create an `Account` class, so that whenever we need to update the structure, we need only do so once. (This is a very simplified version of a complex topic called [data abstraction](https://www.composingprograms.com/pages/22-data-abstraction.html) that demonstrates the need for complex, templated data structures and methods of accessing their data without violating [abstraction barriers](https://www.composingprograms.com/pages/22-data-abstraction.html#abstraction-barriers).)

Some terminology: a **class** is the abstract definition of one such data structure, the definition from which class instances are created. When refer to an **instance**, we mean a single copy of one of these objects. It's kind-of like cookies and cookie cutters: the class is the cookie cutter, the template from which we make instances, the cookies. Think about tables: `Table` is the class from which we create table instances:

```python
Table # this is the class
tbl = Table() # this is an instance
```

Instances are created by calling the **constructor** (more below) as if it were a function (e.g. `Table()`).

## Creating Instances

Classes can be created using a `class` statement. Inside the statement, you put the variables and methods that define the class. The first and most important of these methods is the `__init__` method which is called when an instance of a class is created. `__init__` is an example of Python's [dunder (double-underscore) methods](https://www.geeksforgeeks.org/dunder-magic-methods-python/), which are used to allow classes to interact with built-in functions and operators.

The `__init__` method should take any arguments needed for the class and create all of the _instance variables_ that the instance tracks. Consider the `Car` class:

```python
class Car:
    def __init__(self, make, model, year, color):
        self.make = make
        self.model = model
        self.year = year
        self.color = color
```

Note that the first argument to the `__init__` method is a variable called `self`; this argument will be filled by Python with the instance of class that is being called. For example, when we call an instance's **method** (a function included in the class), we might have something like:

```python
class Foo:
    def bar(self):
        return None

foo = Foo()
foo.bar()
```

When we run `foo.bar()`, the function `Foo.bar` is called and the first argument (`self`) is filled with the instance `foo`. 

In the `__init__` method (or any method, for that matter), we create instance variables (variables tied to a single instance of a class) using `<instance>.<variable name>` syntax, e.g.

```python
self.some_variable = "some value"
```

If we're outside of a method, we can use the same syntax using the variable name:

```python
foo.some_variable = "some value"
```

When you create a `Car`, `Car.__init__` is called by Python. We can create a `Car` and access the values of its instance variables using the dot syntax.

```python
car = Car("Honda", "Civic", 2018, "blue")
car.make
```

## Class Representations

Now let's see what our `car` object (an instance of the `Car` class) looks like.

```python
car
```

Hmm, that representation isn't very descriptive. Another dunder method of Python's is `__repr__`, which defines a string representation of an object. Let's define one for our `Car` class.

```python
class Car:
    def __init__(self, make, model, year, color):
        self.make = make
        self.model = model
        self.year = year
        self.color = color
        
    def __repr__(self):
        return self.color + " " + str(self.year) + " " +self.make + " " + self.model
```

Now that we have defined `Car.__repr__`, we can get a nicer representation of `car`.

```python
car = Car("Honda", "Civic", 2018, "blue")
car
```

## Operators

Now let's create two of the same cars and compare them. They should be equal, right...?

```python
car_1 = Car("Honda", "Civic", 2018, "blue")
car_2 = Car("Honda", "Civic", 2018, "blue")

car_1 == car_2
```

They aren't equal! That's because, by default, the custom classes are only equal if they are the *same instance*, so `car_1 == car_1` is `True` but `car_1 == car_2` is `False`. For this reason, we need to define the `__eq__` dunder method of `Car` which Python will call when we use the `==` operator on a `Car`. We'll say and object is equal to a `Car` if the other object is also a `Car` (determined using the `isinstance` function) and has the same four attributes as the current car.

```python
class Car:
    def __init__(self, make, model, year, color):
        self.make = make
        self.model = model
        self.year = year
        self.color = color
        
    def __repr__(self):
        return f"{self.color} {self.year} {self.make} {self.model}"
    
    def __eq__(self, other):
        if isinstance(other, Car):
            return self.make == other.make and self.model == other.model and \
                self.year == other.year and self.color == other.color
        return False
```

Now our call from above will work:

```python
car_1 = Car("Honda", "Civic", 2018, "blue")
car_2 = Car("Honda", "Civic", 2018, "blue")

car_1 == car_2
```

Other important dunder methods include

| Method Name | Description |
|-----|-----|
| `__str__` | the string representation of an object |
| `__len__` | length of an object (`len(obj)` |
| `__lt__`, `__gt__`, `__lte__`, `__gte__` | less than, greater than, less than or equal to, and greater than or equal to operators, resp. |
| `__hash__` | [hash function](https://en.wikipedia.org/wiki/Hash_function) value (`hash(obj)`) |
| `__getitem__`, `__setitem__` | getter and setter (resp.) for indexes (`obj[idx]`) |
| `__getattr__`, `__setattr__` | getter and setter (resp.) for dot syntax (`obj.attr`) |

Note that when using comparison operators the object to the **left** of the operator has its comparison operator method called. In the below example, the first comparison calls `point_1.__lt__` and the second calls `point_2.__lt__`.

```python
point_1 = Point()
point_2 = Point()

point_1 < point_2      # calls point_1.__lt__
point_2 < point_1      # calls point_2.__lt__
```

## Instance Methods

Now let's define some methods for a `Car`. We'll add a few more instance variables:

* `car.mileage` is the number of miles driven by the car
* `car.gas` is number of gallons of gas in the tank
* `car.mpg` is the number of miles to a gallon that the car gets.

Note that `car.mileage` and `car.gas` are initialized to 0 when we create the car in `__init__`. We'll first define the `fill_tank` method, which fills the gas tank to 10 gallons.

```python
class Car:
    def __init__(self, make, model, year, color, mpg):
        self.make = make
        self.model = model
        self.year = year
        self.color = color
        self.mpg = mpg
        self.mileage =  0
        self.gas = 0
        
    def __repr__(self):
        return f"{self.color} {self.year} {self.make} {self.model}"
    
    def __eq__(self, other):
        if isinstance(other, Car):
            return self.make == other.make and self.model == other.model and \
                self.year == other.year and self.color == other.color
        return False
    
    def fill_tank(self):
        self.gas = 10
```

We can create a car and fill its take by calling `car.fill_tank`.

```python
car = Car("Honda", "Civic", 2018, "blue", 18)
car.fill_tank()
car.gas
```

### Assertions

Now we'll define the `car.drive` method that drives `miles` miles and ensures that we have enough gas to drive that far by throwing an `AssertionError` if we don't. 

We throw assertion errors using an `assert` statement which takes two arguments: a boolean expression and a string telling the user what caused the error. For example, if we want to make sure that a string has no spaces, we might write

```python
assert " " not in string, "Spaces found in string"
```

Then, if `string` has a space, the user would see:

```python
string = "some string"
assert " " not in string, "Spaces found in string"
```

### Reassignment Operators

Another new syntax needed for the `Car.drive` method is `+=` and `-=`. An operator followed by `=` tells Python to perform the operation combining the values on the left and right sides of the operator and then reassigns this value to the variable on the left side. This means that the expression `x += 2` is the exact same as `x = x + 2`.

```python
x = 2

x += 1
print(x) # x is now 3

x -= 4
print(x) # x is now -1

x *= 100
print(x) # x is now -100

x /= -100
print(x) # x is now 1
```

Now let's define `Car.drive`.

```python
class Car:
    def __init__(self, make, model, year, color, mpg):
        self.make = make
        self.model = model
        self.year = year
        self.color = color
        self.mpg = mpg
        self.mileage =  0
        self.gas = 0
        
    def __repr__(self):
        return f"{self.color} {self.year} {self.make} {self.model}"
    
    def __eq__(self, other):
        if isinstance(other, Car):
            return self.make == other.make and self.model == other.model and \
                self.year == other.year and self.color == other.color
        return False
    
    def fill_tank(self):
        self.gas = 10
        
    def drive(self, miles):
        assert miles <= self.gas * self.mpg, f"not enough gas to drive {self.miles} miles"
        self.mileage += miles
        self.gas -= miles / self.mpg
```

Let's drive our `Car` 100 miles.

```python
car = Car("Honda", "Civic", 2018, "blue", 18)
car.fill_tank()
car.drive(100)
car.mileage, car.gas
```

Now let's see how many miles we have left to drive by defining `Car.miles_to_empty`.

```python
class Car:
    def __init__(self, make, model, year, color, mpg):
        self.make = make
        self.model = model
        self.year = year
        self.color = color
        self.mpg = mpg
        self.mileage =  0
        self.gas = 0
        
    def __repr__(self):
        return f"{self.color} {self.year} {self.make} {self.model}"
    
    def __eq__(self, other):
        if isinstance(other, Car):
            return self.make == other.make and self.model == other.model and \
                self.year == other.year and self.color == other.color
        return False
    
    def fill_tank(self):
        self.gas = 10
        
    def drive(self, miles):
        assert miles <= self.gas * self.mpg, f"not enough gas to drive {self.mileage} miles"
        self.mileage += miles
        self.gas -= miles / self.mpg
        
    def miles_to_empty(self):
        return self.gas * self.mpg
    
car = Car("Honda", "Civic", 2018, "blue", 18)
car.fill_tank()
car.drive(100)
car.miles_to_empty()
```

We have 80 miles left before we're empty, so we see that if we try to drive 90 miles, the car will thrown an error:

```python
car.drive(90)
```

For more information on Python classes, check out [Sections 2.5-2.7 of Composing Programs](https://www.composingprograms.com/pages/25-object-oriented-programming.html), the CS 61A/CS 88 textbook.

```python

```

```python

```



--- END 07-game-theory/python-classes.md ---



--- START 08-development/.ipynb_checkpoints/index-checkpoint.md ---

---
title: index-checkpoint
type: textbook
source_path: content/08-development/.ipynb_checkpoints/index-checkpoint.md
chapter: 8
---

# Development

A fundamental aspect of inferential thinking and statistical measurement is how to design experiments in order to be measuring causation instead of correlation. An intervention can be thought of as some sort of change that is thought to bring about some desired outcome.   Correlation between two measurements can be due to many underlying factors besides the intervention being studied.  A properly designed experiment can isolate the effect of the intervention being studied.  

The main idea is to take an overall population, create two subsets and randomly assign which of the subsets of the population gets an intervention (called *treatment*) and another subset who does not get an intervention ( called *control*).  The population can be studied before and after an intervention is rolled out. The application of this methodology in Development Economics has caused a revolution in how the field measures the impacts of different development interventions.   

Historically many of the early statistical models came from agriculture, where a scientist might be looking to increase the yield of a crop being planted. The experiment might be to plant two fields side by side with the same seeds, but one field gets two times the fertilizer of the second field. The amount that the yield increases due to fertilizer can be measured by the yield on the treated field (more fertilizer) minus the yield on the control field with baseline fertilizer.  (*And an economist would then balance the cost of the extra fertilizer against the revenue from the additional crop yield*) 

More recently, most people are familiar with drug trials, where new medicines are evaluated using an experimental design. In 2020 the whole world waited for the results of randomized controlled trials of the COVID-19 vaccines to be carried out before the vaccines could be used.  In these trials, 30,000 - 40,000 volunteers were recruited from a diverse set of ages and ethnicities and randomized into two groups, treatment and control. In the treatment groups the volunteers received the vaccine, in the control groups the volunteers received an injection of a neutral saline solution.  The study continued until a certain number of people in the control group had gotten COVID-19, and the number of people in the two study arms were compared.  

In the current construction of the internet as we use it, A/B testing is a very common tool for UX design, and a very common work for data scientists working in internet companies.  Every time you surf the web or visit an internet site you may be unknowingly part of a test of the size or color of a button, flow of the page, or other details. The internet companies constantly divide site visitors into treatment and control groups and measure site engagement metrics to calibrate changes to their site.  A manager of any site, no matter how small, has tools such as Google Analytics that can allow them to measure changes in site design on user interaction metrics.  

In the world of Development Economics, this has taken the form of evaluating interventions that are usually aimed at reducing poverty, or improving health and education outcomes. This field is generally called Randomized Controlled Trials (RCTs), or also sometimes Impact Evaluation.  In many cases these are pilot programs for new interventions that can be measured well in a pilot case, and if measured well and a strong effect is found, can be used as evidence to make the case for a broader rollout. In many cases the pilot study may only have resources to provide the intervention to a subset of the overall population and this could lead naturally to a control subset and treatment subset, where the control subset will eventually get the treatment in future time periods.

Some examples of treatments that were tested out in pilot RCT are:
* Malaria bednets to reduce childhood malaria
* Household water treatment to reduce child diarrhea
* Clean cooking stoves to reduce child respiratory illness
* Conditional payments to incentivize health clinic visits
* Payments for completion of school to incentivize girls schooling

In each of these we can think of reasons that randomization can be needed to screen out confounding factors. For example if malaria bednets were just sold at the store, it might be households with more wealth, more connected with social capital, or more education who might be the ones to purchase and use them.  These *confounding factors* would make it hard for researchers to know what the effect of bednets were alone.  

[One seminal RCT was a study of deworming](http://emiguel.econ.berkeley.edu/research/worms-identifying-impacts-on-education-and-health-in-the-presence-of-treatment-externalities) - giving out an anti-helminthic pill to kill intestinal parasites - to primary school children in Rural Kenya.  This study was carried out to help a local non-profit that was working on increasing the rate of primary school attendance where there was high absenteeism due to childhood illness.  The economics professors who carried this study out (and actually continue to carry it out) are Edward Miguel and Michael Kremer.  Edward Miguel is now a Professor at UC Berkeley who teaches the Economics of Development Course (ECON 172) and Michael Kremer, Miguel's PhD thesis advisor at Harvard, won the Nobel Prize for Economics in 2019.  The randomization of populations was more elaborate than just two groups and the comparison of students who got the deworming pill and who did not showed positive health  and school attendance effects not just for the students who got it, but also for students at the same school who didn't get it and students who just happened to live in nearby villages to where the deworming was carried out.   The researchers were also able to carefully document these effects using the correct study design and thus argue for the expansion of the program, which has since been carried out nationally in Kenya and in [several other countries with high parasite burden](https://www.evidenceaction.org/dewormtheworld/) "with over 1 billion treatments delivered since 2014 and 280 million treatments in 2019"   

The researchers involve in this seminal study later moved on to study other interventions that could reduce the burden of childhood diseases by focusing on water-borne diseases such as diarrhea.  Water-borne diarrhea is a leading cause of mortality in sub-Saharan Africa but is fatal primarily in infants under 5 years old. So the target of these studies was measuring health outcomes in children under 5.  These interventions included the protection of water sources, *[Spring Protection](http://emiguel.econ.berkeley.edu/research/spring-cleaning-rural-water-impacts-valuation-and-property-rights-institutions)*,  and the [promotion of the household use of Chlorine](http://emiguel.econ.berkeley.edu/research/social-engineering-evidence-from-a-suite-of-take-up-experiments-in-kenya), sold in Kenya as *Water Guard* to purify drinking water.  A subset of this study is the focus of this week's lab. 

The pedagogical purpose of this week's lab is also for students to think about the process that goes on behind the scenes of an economics journal article. The researchers work with the local NGO to map a set of villages where Spring Protection or Water Guard Promotion are going to be carried out. The populations are divided into a control arm, which will get the intervention in a future time period, and a set of comparable populations who get different treatment interventions, or even combinations of interventions.  The NGO employs surveyors, who visit the households at baseline, before the intervention begins, and at various intervals later on after the intervention has been deployed.  This household survey data is used to create a data set that is used to measure the effects of the intervention.  


--- END 08-development/.ipynb_checkpoints/index-checkpoint.md ---



--- START 08-development/index.md ---

---
title: index
type: textbook
source_path: content/08-development/index.md
chapter: 8
---

# Development

A fundamental aspect of inferential thinking and statistical measurement is how to design experiments in order to be measuring causation instead of correlation. An intervention can be thought of as some sort of change that is thought to bring about some desired outcome.   Correlation between two measurements can be due to many underlying factors besides the intervention being studied.  A properly designed experiment can isolate the effect of the intervention being studied.  

The main idea is to take an overall population, create two subsets and randomly assign which of the subsets of the population gets an intervention (called *treatment*) and another subset who does not get an intervention ( called *control*).  The population can be studied before and after an intervention is rolled out. The application of this methodology in Development Economics has caused a revolution in how the field measures the impacts of different development interventions.   

Historically many of the early statistical models came from agriculture, where a scientist might be looking to increase the yield of a crop being planted. The experiment might be to plant two fields side by side with the same seeds, but one field gets two times the fertilizer of the second field. The amount that the yield increases due to fertilizer can be measured by the yield on the treated field (more fertilizer) minus the yield on the control field with baseline fertilizer.  (*And an economist would then balance the cost of the extra fertilizer against the revenue from the additional crop yield*) 

More recently, most people are familiar with drug trials, where new medicines are evaluated using an experimental design. In 2020 the whole world waited for the results of randomized controlled trials of the COVID-19 vaccines to be carried out before the vaccines could be used.  In these trials, 30,000 - 40,000 volunteers were recruited from a diverse set of ages and ethnicities and randomized into two groups, treatment and control. In the treatment groups the volunteers received the vaccine, in the control groups the volunteers received an injection of a neutral saline solution.  The study continued until a certain number of people in the control group had gotten COVID-19, and the number of people in the two study arms were compared.  

In the current construction of the internet as we use it, A/B testing is a very common tool for UX design, and a very common work for data scientists working in internet companies.  Every time you surf the web or visit an internet site you may be unknowingly part of a test of the size or color of a button, flow of the page, or other details. The internet companies constantly divide site visitors into treatment and control groups and measure site engagement metrics to calibrate changes to their site.  A manager of any site, no matter how small, has tools such as Google Analytics that can allow them to measure changes in site design on user interaction metrics.  

In the world of Development Economics, this has taken the form of evaluating interventions that are usually aimed at reducing poverty, or improving health and education outcomes. This field is generally called Randomized Controlled Trials (RCTs), or also sometimes Impact Evaluation.  In many cases these are pilot programs for new interventions that can be measured well in a pilot case, and if measured well and a strong effect is found, can be used as evidence to make the case for a broader rollout. In many cases the pilot study may only have resources to provide the intervention to a subset of the overall population and this could lead naturally to a control subset and treatment subset, where the control subset will eventually get the treatment in future time periods.

Some examples of treatments that were tested out in pilot RCT are:
* Malaria bednets to reduce childhood malaria
* Household water treatment to reduce child diarrhea
* Clean cooking stoves to reduce child respiratory illness
* Conditional payments to incentivize health clinic visits
* Payments for completion of school to incentivize girls schooling

In each of these we can think of reasons that randomization can be needed to screen out confounding factors. For example if malaria bednets were just sold at the store, it might be households with more wealth, more connected with social capital, or more education who might be the ones to purchase and use them.  These *confounding factors* would make it hard for researchers to know what the effect of bednets were alone.  

[One seminal RCT was a study of deworming](http://emiguel.econ.berkeley.edu/research/worms-identifying-impacts-on-education-and-health-in-the-presence-of-treatment-externalities) - giving out an anti-helminthic pill to kill intestinal parasites - to primary school children in Rural Kenya.  This study was carried out to help a local non-profit that was working on increasing the rate of primary school attendance where there was high absenteeism due to childhood illness.  The economics professors who carried this study out (and actually continue to carry it out) are Edward Miguel and Michael Kremer.  Edward Miguel is now a Professor at UC Berkeley who teaches the Economics of Development Course (ECON 172) and Michael Kremer, Miguel's PhD thesis advisor at Harvard, won the Nobel Prize for Economics in 2019.  The randomization of populations was more elaborate than just two groups and the comparison of students who got the deworming pill and who did not showed positive health  and school attendance effects not just for the students who got it, but also for students at the same school who didn't get it and students who just happened to live in nearby villages to where the deworming was carried out.   The researchers were also able to carefully document these effects using the correct study design and thus argue for the expansion of the program, which has since been carried out nationally in Kenya and in [as can be seen on the Deworm the World project page](https://www.evidenceaction.org/dewormtheworld/) "with over 1 billion treatments delivered since 2014 and 280 million treatments in 2019"   

The researchers involve in this seminal study later moved on to study other interventions that could reduce the burden of childhood diseases by focusing on water-borne diseases such as diarrhea.  Water-borne diarrhea is a leading cause of mortality in sub-Saharan Africa but is fatal primarily in infants under 5 years old. So the target of these studies was measuring health outcomes in children under 5.  These interventions included the protection of water sources, *[Spring Cleaning](http://emiguel.econ.berkeley.edu/research/spring-cleaning-rural-water-impacts-valuation-and-property-rights-institutions)*,  and the [as can be seen in the Social Engineering: Evidence from a Suite of Take-up Experiments in Kenya](http://emiguel.econ.berkeley.edu/research/social-engineering-evidence-from-a-suite-of-take-up-experiments-in-kenya), sold in Kenya as *Water Guard* to purify drinking water.  A subset of this study is the focus of this week's lab. 

The pedagogical purpose of this week's lab is also for students to think about the process that goes on behind the scenes of an economics journal article. The researchers work with the local NGO to map a set of villages where Spring Protection or Water Guard Promotion are going to be carried out. The populations are divided into a control arm, which will get the intervention in a future time period, and a set of comparable populations who get different treatment interventions, or even combinations of interventions.  The NGO employs surveyors, who visit the households at baseline, before the intervention begins, and at various intervals later on after the intervention has been deployed.  This household survey data is used to create a data set that is used to measure the effects of the intervention.  


--- END 08-development/index.md ---



--- START 09-macro/.ipynb_checkpoints/CentralBanks-checkpoint.md ---

---
title: CentralBanks-checkpoint
type: textbook
source_path: content/09-macro/.ipynb_checkpoints/CentralBanks-checkpoint.ipynb
chapter: 9
---

# Central Banks

Understanding the levels and relationships between the four main macro indicators mentioned in the previous section is of particular interest to a country’s central bank. 

One way to think about a central bank is that it acts as the “bankers’ bank”. Wells Fargo, Barclays, and many of the other commercial banks that we’re familiar with are regulated and overseen by a given country’s central bank. That’s just a simple way to consider it, but in practice central banks play a much larger role beyond this.

Diving deeper, a central bank is a politically-independent financial institution that is responsible for overseeing the monetary system of a country. This mainly involves doing so by setting interest rates and regulating how much money circulates throughout the economy. Central banks closely monitor and forecast the aforementioned indicators in order to carry out these responsibilities and conduct monetary policy as needed. An important feature of central banks is that they exist independently from the government, meaning the actions they take are not influenced by political pressure nor do they have to go through a congressional body to carry out their policies. 

While central banks across countries differ slightly in their mandates, for the most part they share a common vision in their functions and responsibilities. For example, these include
- Keeping unemployment low
- Maintaining price stability while managing a healthy level of inflation
- Stimulating the economy in times of recession
- Overseeing the commercial banking system and serving as a lender of last resort
- Carrying out economic research and statistical analysis (yay data science!) 
- and [much more](https://www.federalreserve.gov/aboutthefed/the-fed-explained.htm)

When you hear the terms monetary policy or macroeconomic policy, you can think of it as encompassing what’s listed above. In short, **monetary policy** is the set of actions that a central bank takes to achieve sustainable economic growth, and much of it revolves around adjusting interest rates and the money supply. 

In this chapter we’ll take a closer look at the central bank that serves the United States, known as the **Federal Reserve System.** 

The Fed was established in 1913 to promote stability and flexibility within the United States’ monetary and financial system.Led by the Chair and the Board of Governors, the Fed is comprised of a network of 12 regional banks that serve as the operating arms of the system. Together they guide monetary policy action as well as analyze domestic and international economic conditions.

![regionalbanks.png](regionalbanks.png)

*Here’s a graphic showing the locations of the 12 different regional banks. If you’re ever interested you can go take a tour of the Federal Reserve Branch in SF right on Market St!*

The current Federal Chairman is Jerome Powell, who you’ve probably heard about in the news lately. He’s got quite a bit on his plate with inflation running the highest it’s been in over four decades. Before him came Berkeley’s own Janet Yellen who served from 2014-2018 and was the first woman to hold the position, Ben Bernanke (2006-2014) who helped the US recover from the Great Recession, Alan Greenspan (1987-2006), the “rock star of economics, and Paul Volcker (1979-1987) who is famously known for setting incredibly high interest rates to combat high inflation in the 80s. There have been many [other Chairs](https://www.federalreservehistory.org/people/federal-reserve-chair) as well since the Fed was first established in the early twentieth century.

![chairs.png](chairs.png)

*The Federal Funds Rate is a nominal interest rate like we discussed earlier in the chapter. Interest rates, which can be thought of as the cost of borrowing, are set low to stimulate the economy, and hiked up in times of inflation in order to “cool off” the economy. Inflation was incredibly high in the 80s after oil supply shocks in the 70s.*

While these Chairs have served as the face of monetary policy throughout history, there is a lot more that goes on behind the curtains. Namely, the **Federal Open Market Committee** is the team of 12 economists that serves as the Fed’s monetary policy-making body, and the Chair stands as the leader of this team. The other voting members of the FOMC include the 6 other governors from the Board of Governors, the president of the New York branch, and 4 of the presidents from the 11 other regional banks. 

The FOMC has 8 scheduled meetings throughout the year, at which they discuss the short and long run outlooks for the US economy as well as monetary policy options. Sometimes they have to hold emergency meetings when the situation appears to be particularly dire, for example when the coronavirus pandemic first hit, prompting a lowering of interest rates to near 0 percent. 

What is particularly important about these FOMC meetings is that the committee designs a course of action for staying committed to the Federal Reserve’s **dual mandate** and releases a statement outlining how they plan to do so. An example of this can be seen in an excerpt from the press release below,

![FOMC.png](FOMC.png)

Note how the FOMC begins by describing the current state of the economy’s health in terms of the indicators that we learned about earlier in the chapter. They then outline and forecast what they plan to do in response to the situation at hand. This is important because it anchors peoples’ expectations. 

Employment and inflation are particularly relevant because these are the drivers that govern how the Fed designs monetary policy actions according to their dual mandate, which is to **achieve maximum employment** and **maintain price stability**. Adjusting interest rates is directly related to reaching a delicate balance between these two goals. For example, at this snapshot of time in January 2021, the Fed wanted to keep the federal funds rate low in order to provide a boost to an economy that was experiencing “weaker demand”. 

At first glance it is probably pretty confusing how all of these moving parts work hand in hand. But the intuition behind how employment, inflation and interest rates are all related to the dual mandate can best be captured by the **The Taylor Rule**. 

The Taylor Rule was first proposed in 1993 by the American economist John B. Taylor. It’s called a “rule” because it serves as a guide to monetary policy for how to set interest rates based on the inflation gap and output gap at a given time. The equation is as follows

$$i = 2\% + \pi + 0.5(\pi - 2\%) + 0.5(\frac{Y - Y^P}{Y^P})$$

And these are the variables
- i is the nominal interest rate (like we learned about earlier in the chapter) 
- The first 2% is the equilibrium interest rate. You can think of this as the ideal interest rate when production has reached its potential and inflation is stable in the long run. 
- $\pi$ is the current inflation rate
- The second 2% is the target inflation rate. Note how this was mentioned in the FOMC press release above! The Fed wants to have some stable inflation in the long run because that is indicative of a growing economy
$\frac{Y - Y^P}{Y^P}$ is the output gap. $Y^P$ is the potential output if we are at full employment, so this gap tells us how far off we are from that. 
- The 0.5 coefficients reflect the relative balance between how much the Fed cares about inflation versus output. In the newspaper, you will often hear the terms “hawkish” or “dove-ish”. Those who support high rates to combat inflation are hawks, for example Chairman Volcker, whereas doves tend to favor lower interest rates.

In English, you can think of the Taylor Rule as a means of prescribing how nominal interest rates should adjust in response to the observed gaps. For example, the annual inflation rate as of April 2022 is currently 8.5%. Thus the Taylor Rule says that nominal interest rates should go up by 

$$8.5\% + 0.5(8.5\% - 2\%)$$
$$= 8.5\% + 0.5(6.5\%)$$
$$= 11.75\%$$


Now recall the Fisher equation that we learned about earlier the chapter, $r_t = i_t - \pi_t$. Plugging 11.75% and 8.5% in, we see that the Taylor Rule suggests real interest rates should increase to 3.25%. Indeed, this is similar to Jay Powell’s vision as the Federal Reserve is tentatively planning to hike up rates by 0.25% - 0.5% at 6 different occasions over the course of the next year.

Another way that we can directly see how the Taylor Rule is related to the Fed’s dual mandate is to make a simplification using what is called **Okun’s Law**. It was proposed in 1962 by Yale professor Arthur Okun who studied the relationship between unemployment and production.

$$\frac{Y - Y^P}{Y^P}= -C_{okun} (U - U^N)$$

where
- $\frac{Y - Y^P}{Y^P}$ is the output gap as seen before  
- $(U - U^N)$ is the unemployment gap with $U$ being the current level of unemployment and $U^N$ being the natural rate of employment
- and $C_{okun}$ is a coefficient that represents the sensitivity of the unemployment gap to changes in the output gap

The intuition behind Okun’s Law is that the less unemployment there is, the more output that will be produced. This is because there will be more workers contributing to the country’s GDP. Thus the coefficient is negative, and empirically $C_{okun}$ is approximately 2. By plugging Okun’s Law into our equation for the Taylor Rule we get,  

$$i = 2\% + \pi + 0.5(\pi - 2\%) - 0.5C_{okun}(U - U^N)$$

Recall that the Federal Reserve’s dual mandate is to achieve maximum employment and a stable level of inflation close to their target. Therefore we can clearly see how this is captured by the Taylor Rule with Okun’s Law substituted into it. Moreover, this also illustrates how the nominal interest rate is the conventional tool that can allow the Fed to reach a balance between their two goals. 

In the next section we’ll talk about some of the graphs the Federal Reserve looks at to guide their understanding of the economy and its outlook in both the short and long run.

```python

```


--- END 09-macro/.ipynb_checkpoints/CentralBanks-checkpoint.md ---



--- START 09-macro/.ipynb_checkpoints/Indicators-checkpoint.md ---

---
title: Indicators-checkpoint
type: textbook
source_path: content/09-macro/.ipynb_checkpoints/Indicators-checkpoint.ipynb
chapter: 9
---

# Macroeconomic Indicators

The process of conducting macroeconomic policy often starts with studying and projecting the behavior of a variety of indicators that measure the economy’s current or expected future performance. While there are many different variables one could look to depending on the interests and goals of the individuals or institutions involved, in this chapter we will focus specifically on four main indicators that capture the overall health of the economy and thus tend to play a critical role in policy decisions. These are GDP, the unemployment rate, the inflation rate and the real interest rate.

## Gross Domestic Product (𝑌)

Earlier in the semester, when studying Production, we introduced the concept of GDP briefly. To recap, we looked at how GDP serves as a means of capturing a given country’s overall production over a given period of time. In theory it can then be used as a way to measure a country’s economic performance in a given quarter or year; the higher its GDP, the more that it’s produced, the better it’s doing economically, and vice versa. In this chapter, we’ll go into more detail on its significance and how it’s measured. By definition, GDP, often denoted as 𝑌, is measured as the:

**market value** of  
**final goods and services**  
**newly produced**  
in the **domestic economy**   
over a **specified period of time**  

Let’s dive into each of these individually:

First, the *market value* refers to the market price of goods and services, which sets a standard for how we value goods and services. This is especially useful when we are trying to add together products that may be quite different from one another.

Second, *final goods and services* refer to those which are not used up in the production process. We only consider these in our calculation of GDP, because we don’t want to end up overrepresenting the overall level of production. Intermediate goods, which are used up in the production process, inherently add some value to the final product, and therefore if we were to consider these in our calculation, we would end up double counting and overstating the level of production.

Third, *newly produced* refers  that we are only interested in goods and services that were made during the time period we are looking at. Since we are using GDP as a way to measure the level of production in the economy, it wouldn’t make a whole lot of sense to include products that were produced outside the time period that we are looking at in our calculation.

Fourth, in the *domestic economy* refers to the fact that in our calculation of GDP, we only include goods and services that were produced within the geographical area of the area that we are looking at. Given that we are trying to measure the level of production of a certain country, we wouldn’t want to consider products or services that are produced outside of the country in our calculation. 

Lastly, over a *specified period of time* simply points out that GDP is measured within an interval of time. This is important to consider when comparing countries based on their GDP, as we would want to make sure to consider the same time period for all of them. This is also related to the newly produced aspect that we touched on earlier. 

The above definition and approach to calculating GDP considers the production of goods and services in the economy. However, it’s important to note that GDP can also be calculated by looking at the total spending on those goods and services or the total income earned from producing those goods and services. You may recall that in our discussion on Production, we often referred to output and income synonymously. This is because in theory, each of these approaches to calculating GDP (production, expenditure, and income) should all yield the same result, and indeed we find that for the most part they do.

## **Unemployment Rate (𝑈)**

The unemployment rate is also an important measure of a country’s economic performance, as it gives us some insight on the supply and demand of labor in the economy. By definition, the unemployment rate measures the percentage of the labor force – the sum of all employed and unemployed people – that are not currently employed, but are willing to, able to, and looking for work. Mathematically, this can be expressed by the following equation:

$$ Unemployment Rate = \frac{Unemployed}{Labor Force}$$

As stated before, the labor force is just the sum of all employed and unemployed persons in the population, so we can simplify the equation above to be

$$ Unemployment Rate = \frac{Unemployed}{Employed + Unemployed}$$


An important thing to remember is that in order to be considered unemployed, a person must be able to, willing to, and currently looking for work. This means that anyone who is unable to work or has stopped looking for a job is no longer considered part of the labor force and is therefore not included in the unemployment rate. Intuitively this makes sense for the most part, as it probably wouldn’t be very helpful to include retirees or stay-at-home parents or even students for that matter when calculating the unemployment rate. It’s worth considering, however, that this also means that people who have been unable to find any work and therefore stopped looking – often referred to as discouraged workers – would not be represented in the unemployment rate either.

## **Inflation Rate (𝜋)**

Generally speaking, the inflation rate in an economy measures the percent change in prices over a specified period of time, and it is usually calculated using a price index. While there are many different price indices that can be used to calculate inflation, one of the more common ones is the Consumer Price Index (CPI). The CPI measures the average price for a consistent basket of goods and services relative to some defined base year. It is calculated by taking the value of said basket in any given year, dividing it by its value in the base year, and multiplying that by 100. Mathematically, this can be expressed as


$$CPI_t = \frac{Price of basket_t}{Price of basket_0} * 100 $$
where t = 0 refers to the base year.

We can then calculate the inflation rate for a given year as

$$𝜋_t = \frac{CPI_t - CPI_{t-1}}{CPI_{t-1}} * 100 $$

The inflation rate is yet another key indicator that macroeconomists look at, as it can reflect where or not the economy is growing. Generally, a small, positive inflation rate is considered a good thing, as it’s usually indicative of a growing economy. However, inflation rates that are negative or too high can create a lot of problems, and as such the inflation rate tends to play an important role in guiding monetary policy decisions – something we will discuss more later in this chapter.

## **Real Interest Rate (𝑟)**

If you have a bank account or own a credit card or have ever taken a loan, chances are that you’ve come across an interest rate at some point. In general, interest rates represent the cost of borrowing. On the flip side, it also represents the return on saving or lending. In other words, you can think of interest rates as the opportunity cost of holding money. 

There are many different interest rates that can be found in the economy from interest rates for savings accounts to interest rates charged for mortgages to interest rates set by the Central Bank. All of the interest rates that we observe in the economy are known as nominal interest rates – meaning rates that are not adjusted for inflation.

As discussed earlier, inflation measures the change in prices over a given time and can be used in some sense to measure the relative value of a dollar (or other unit of currency) over time. It makes sense then, that we might want to take inflation into account, when deciding what interest rate would make sense to lend/borrow at. To do so, we use real interest rates, which are calculated by taking nominal interest rates and subtracting inflation. This relationship between inflation, nominal interest rates, and real interest rates is captured by what is known as the Fisher Equation

$$r_t = i_t - 𝜋_t$$



where $r_t$ refers to the real interest rate, $i_t$ refers to the nominal interest rate, and $𝜋_𝑡$ refers to the inflation rate, all in a given time period.

As a final note, we find that generally speaking all of the nominal interest rates present in the economy tend to be pretty strongly correlated with one another. Therefore, for purposes of simplification, macroeconomists will often refer to these rates in singular form in their models as simply the nominal interest rate or the real interest rate. As seen in the Fisher equation above, we use 𝑖 to denote the nominal interest rate, and 𝑟 to denote the real interest rate.


--- END 09-macro/.ipynb_checkpoints/Indicators-checkpoint.md ---



--- START 09-macro/.ipynb_checkpoints/fiscal_policy-checkpoint.md ---

---
title: fiscal_policy-checkpoint
type: textbook
source_path: content/09-macro/.ipynb_checkpoints/fiscal_policy-checkpoint.ipynb
chapter: 9
---

# Fiscal Policy

In this section, we will give a broad overview of fiscal policy. 

Fiscal policy is the use of government spending and taxation to influence the economy. Governments typically use fiscal policy to promote strong and sustainable growth and reduce poverty. 

$$ Y = C + I + G + NX $$

**There are many fiscal policy tools at the hands of the government to stabilize the economy. These primarily include changes to levels of taxation and government spending. To stimulate growth, taxes are lowered and government spending is increased, often involving borrowing through issuing government debt. To cool down an overheated economy, the government can use the opposite tools.**

## Classical vs Keynesian View

Why is fiscal policy an effective tool to stabilize the economy? We must first look at two views of macroeconomics and what each of them implies for fiscal policy. 

**Classical economics** claims economic fluctuations arise from “supply 
shocks'' such as fluctuations in productivity, and the aggregate supply is the determinant of economic output. So, the Classical view places little emphasis on the use of fiscal policy to manage aggregate demand. Classical theory is the basis for Monetarism, which only concentrates on managing the money supply, through monetary policy.

**Keynesian economics** suggests recessions often arise from “aggregate demand” shocks, and governments need to use fiscal policy, especially in a recession.

In the AD–AS diagram below, we can see a visual comparison between the economy under the Classical view and the Keynesian view. In the Classical view, the AS curve is vertical. So any increase or decrease in aggregate demand (such as government spending) will not affect the output. On the other hand, in the Keynesian view, increase or decrease in aggregate demand will result in a corresponding change in the output.

![classical_vs_keynesian.png](classical_vs_keynesian.png)
<center> Classical view vs. Keynesian view </center>

Empirically speaking, our economy lies between the Classical view and the Keynesian view. So fiscal policy has the ability to adjust the economy to some extent.

```python

```


--- END 09-macro/.ipynb_checkpoints/fiscal_policy-checkpoint.md ---



--- START 09-macro/.ipynb_checkpoints/index-checkpoint.md ---

---
title: index-checkpoint
type: textbook
source_path: content/09-macro/.ipynb_checkpoints/index-checkpoint.md
chapter: 9
---

# Macroeconomic Policy

## Student Learning Outcomes:
- Learn about the 4 main macro indicators and how they relate to one another
- Learn how to graphically represent these relationships and understand historical trends
- Learn about central banks and the tools they use for monetary and fiscal policy
- Improve macro literacy 


--- END 09-macro/.ipynb_checkpoints/index-checkpoint.md ---



--- START 09-macro/.ipynb_checkpoints/is_curve-checkpoint.md ---

---
title: is_curve-checkpoint
type: textbook
source_path: content/09-macro/.ipynb_checkpoints/is_curve-checkpoint.ipynb
chapter: 9
---

# IS-Curve

In this section we will introduce the IS curve (“investment–saving” curve), an important macroeconomics model that characterizes the relationship between real interest rates and output. **The IS curve is downward sloping. When the real interest rate falls, output will increase.** We will see why this inverse relationship is true and what this relationship implies for the economy.

![is_curve.png](is_curve.png)
<center> IS Curve </center>

## Keynesian Cross

First, we will discuss a model of aggregate demand and aggregate output. The Keynesian cross diagram, determines the equilibrium level of real GDP by the point where the total or aggregate demand in the economy is equal to the amount of output produced.

![keynesian_cross.png](keynesian_cross.png)
<center> Keynesian Cross Diagram </center>

The axes of the Keynesian cross diagram show output / national income (or real GDP) on the horizontal axis and output / aggregate demand on the vertical axis. The blue line represents the aggregate demand, and the black 45° line represents the aggregate output.

- **Why is the slope of the aggregate output line equal to 1?**   
Both the horizontal axis and the vertical axis are the output of the economy, so they should be the same. 


- **Why is the slope of the aggregate demand line less than 1, and the intercept greater than 0?**  
We will first consider how demand increases when national income rises. People can do two things with their income: consume it or save it (let’s ignore taxes for now). Each person who receives an additional dollar faces this choice. 
The marginal propensity to consume (MPC), is the share of the additional dollar of income a person decides to devote to consumption expenditures. Since the marginal propensity to consume is usually less than 1 (which means not all income is consumed), for every unit increase in national income, we will expect the aggregate demand to increase by less than 1 unit. So, the slope is less than 1.   
However, even when the economy is not producing any output, people still need to consume. They may do so by using their savings or borrowing. So we have a positive intercept.

## Dynamics of Keynesian Cross and Derivation of IS Curve

Now, we are ready to derive the IS curve. 

What will happen if the real interest rate decreases? First, people have less incentive to save money, since the gains from interest income are lower. Second, people have more incentive to spend (especially borrow money to spend) because the opportunity costs of spending are lower. 

**So, when the real interest rate decreases, people will tend to save less and consume more, shifting the aggregate demand curve in the Keynesian Cross upward. The equilibrium output level will therefore increase. The opposite direction will also hold true.** 

This precisely describes the inverse relationship between real interest rate and output.

![is_derivation.png](is_derivation.png)
<center> Keynesian Cross and IS Curve </center>

## Implication of IS Curve

The IS curve explains the inverse relationship between real interest rate and output. 

A more comprehensive model of how the money markets interact with the goods market can be illustrated by the IS–LM model. The IS–LM model, or Hicks–Hansen model, is a two-dimensional macroeconomic tool that shows the relationship between interest rates and the asset market (also known as real output in goods and services market plus money market). The intersection of the "investment–saving" (IS) and "liquidity preference–money supply" (LM) curves models a "general equilibrium" where equilibria simultaneously occur in both the goods and the asset markets. Hence, this tool is sometimes used not only to analyze economic fluctuations but also to suggest potential levels for appropriate stabilization policies. 

The chart below illustates how each monetary and fiscal policy will affect the economy.

![islm_fiscalmonetarypolicy.png](islm_fiscalmonetarypolicy.png)


<center> Source: Lev Lafayette, http://levlafayette.com/node/629 </center>

```python

```


--- END 09-macro/.ipynb_checkpoints/is_curve-checkpoint.md ---



--- START 09-macro/.ipynb_checkpoints/phillips_curve-checkpoint.md ---

---
title: phillips_curve-checkpoint
type: textbook
source_path: content/09-macro/.ipynb_checkpoints/phillips_curve-checkpoint.ipynb
chapter: 9
---

# Phillips Curve

The Phillips Curve describes an inverse relationship between inflation and unemployment: when the inflation is low, the level of unemployment tends to be high; when the level of unemployment is low, price level tends to increase more rapidly. 

In essence, the Phillips Curve characterizes a tradeoff between economic growth and inflation–we cannot have the best of both worlds. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment. 

However, empirical evidence calls Phillips Curve into question, including the stagflation period that happened during the 1970s when the economy was suffering from both a high inflation and a high unemployment rate. A neoclassical model was then introduced to address this issue.

## Classical Phillips Curve

**The classical Phillips Curve describes the relationship between inflation and unemployment: Inflation is higher when unemployment is low and lower when unemployment is high.**

$$ \pi = - h \cdot ( u - u^{*} ) $$

where $\pi$ is the inflation rate, $u - u^{*}$ is the unemployment gap. 

Often also simplified as

$$ \pi = - h \cdot u $$

where $\pi$ is the inflation rate, $u$ is the unemployment rate.

The relationship was originally described by New Zealand economist A.W. Phillips in his paper titled The Relation between Unemployment and the Rate of Change of Money Wage Rates in 1958, who examined data on unemployment and wages for the United Kingdom from 1861 to 1957.

![original_pc.png](original_pc.png)
<center> The original curve drawn for pre-WW1 data </center>

The underlying logic of this inverse relationship can be explained as follows: **when the demand for labor increases, the pool of unemployed workers subsequently decreases and companies increase wages to compete and attract a smaller talent pool. The cost of wages increases and companies pass along those costs to consumers in the form of price increases. Thus, a lower unemployment rate ultimately translates to a higher inflation.** 

Theoretically, the Phillips curve presents a menu of options for policymakers–if higher inflation actually causes lower levels of unemployment, then the government could control unemployment via monetary policy as long as it was willing to accept changes in the level of inflation.

## Expectation-Augmented Phillips Curve

Unfortunately, economists soon learned that the relationship between inflation and unemployment was not as stable as they had previously thought.

![pc_1960s.png](pc_1960s.png)
![pc_1980s.png](pc_1980s.png)
![pc_2010s.png](pc_2010s.png)
<center> Relationship between inflation and unemployment during three periods of time </center>

**What economists initially failed to realize in constructing the Phillips curve was that people and firms take the expected level of inflation into account when deciding how much to produce and how much to consume.** When workers expect prices to rise, they demand higher wages. When firms expect costs to rise, they set higher prices.

**Therefore, economists introduced inflation expectation into the original Phillips Curve.**

$$ \pi = \pi^{e} - h \cdot ( u - u^{*} ) $$

where $\pi$ is the inflation rate, $\pi^{e}$ is the expected inflation rate, $u - u^{*}$ is the unemployment gap.

Therefore, a given level of inflation will eventually be incorporated into the decision-making process and not affect the level of unemployment in the long run. The long-run Phillips curve is vertical, since moving from one constant rate of inflation to another doesn't affect unemployment in the long run.

What this implies is that if the Central Bank would like to pin down inflation, they cannot do it by simply raising the interest rate (even though this is still an effective way in the short run). What is more important is to anchor people’s inflation expectations at a relatively low level. If they succeed, they can control inflation. This idea also appears to have been remarkably successful so far. Whenever it has shot above target, it has, soon enough, fallen back. 

While the idea of inflation expectation looks concrete, interestingly enough, a recent paper (Rudd 2021) sets off a round of debate about the role of expectations in shaping prices in the economy, where the author argued the theory of inflation expectation “rests on extremely shaky foundations”. But still the majority of economists are optimistic about this new Keynesian model. This is an ongoing research field.

> ***Thought experiment***: Spring 2022 marks a season with high inflation. The US inflation rate was sitting at 7.87% in February 2022, compared to 1.68% last year. What would happen if people thought this high level of inflation would persist?

```python

```


--- END 09-macro/.ipynb_checkpoints/phillips_curve-checkpoint.md ---



--- START 09-macro/.ipynb_checkpoints/visualizations-checkpoint.md ---

---
title: visualizations-checkpoint
type: textbook
source_path: content/09-macro/.ipynb_checkpoints/visualizations-checkpoint.ipynb
chapter: 9
---

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

# Visualizations

```python
# IS curve
x = np.linspace(0.5, 4.5, 100)
r = list(map(lambda x: -3 * x + 15, x))

plt.figure(figsize=(4, 4))
plt.plot(x, r)
plt.xlim(0, 5)
plt.ylim(0, 15)
plt.ylabel("Real Interest Rate (R)")
plt.xlabel("Output (Y)")
plt.savefig("is_curve.png", dpi=200)
plt.show()
```

```python
# Keynesian Cross
x = np.linspace(0, 5, 100)
output = list(map(lambda x: x, x))
demand = list(map(lambda x: 0.4 * x + 1, x))

plt.figure(figsize=(4, 4))
ax = plt.subplot(111)
ax.plot(x, output)
ax.plot(x, demand)
ax.set_xlim(0, 5)
ax.set_ylim(0, 5)
ax.set_xlabel("Output (Y)")
ax.set_ylabel("AD/Output (Y)")

# plt.grid()
plt.savefig("keynesian_cross.png", dpi=200)
plt.show()
```

```python
# Dynamics of Keynesian Cross
x = np.linspace(0, 5, 100)
output = list(map(lambda x: x, x))
demand_high_r = list(map(lambda x: 0.4 * x + 1, x))
demand_low_r = list(map(lambda x: 0.4 * x + 2, x))

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(4,8), sharex=True)
ax1.plot(x, output)
ax1.plot(x, demand_high_r, label="R = 10%")
ax1.plot(x, demand_low_r, label="R = 5%")
ax1.axvline(x=5/3, ymin=0, ymax=1/3, color='0', ls=':')
ax1.axvline(x=10/3, ymin=0, ymax=2/3, color='0', ls=':')
ax1.set_xlim(0, 5)
ax1.set_ylim(0, 5)

ax1.set_ylabel("AD/Output")
# ax1.spines['right'].set_visible(False)
# ax1.spines['top'].set_visible(False)

ax1.legend(title="Real interest rate")

r = list(map(lambda x: -3 * x + 15, x))
ax2.plot(x, r)
ax2.axvline(x=5/3, ymin=0, ymax=1, color='0', ls=':')
ax2.axvline(x=10/3, ymin=0, ymax=1, color='0', ls=':')
ax2.axhline(y=5, xmin=0, xmax=2/3, color='0', ls=':')
ax2.axhline(y=10, xmin=0, xmax=1/3, color='0', ls=':')
ax2.set_xlim(0, 5)
ax2.set_ylim(0, 15)
ax2.set_ylabel("Real Interest Rate (R)")
ax2.set_xlabel("Output (Y)")
plt.savefig("is_derivation.png", dpi=200)

plt.show()
```

```python
x = np.linspace(0, 5, 100)
ad1 = list(map(lambda x: -x + 5.5, x))
ad2 = list(map(lambda x: -x + 4.5, x))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4), sharey = True)
ax1.plot(x, ad1)
ax1.plot(x, ad2)
ax1.axvline(x=3, color='0')
ax1.axhline(y=1.5, xmin=0, xmax=0.6, color='0', ls=':')
ax1.axhline(y=2.5, xmin=0, xmax=0.6, color='0', ls=':')

ax1.set_xlim(0, 5)
ax1.set_ylim(0, 5)

ax1.set_ylabel("Price level ($\pi$)")
ax1.set_xlabel("Output (Y) \n \n Classical view")

ax2.plot(x, ad1)
ax2.plot(x, ad2)
ax2.axhline(y=3, color='0')
ax2.axvline(x=1.5, ymin=0, ymax=0.6, color='0', ls=':')
ax2.axvline(x=2.5, ymin=0, ymax=0.6, color='0', ls=':')

ax2.set_xlim(0, 5)
ax2.set_ylim(0, 5)
ax2.set_xlabel("Output (Y) \n \n Keynesian view")

plt.savefig("classical_vs_keynesian.png", dpi=200)

plt.show()
```

```python

```


--- END 09-macro/.ipynb_checkpoints/visualizations-checkpoint.md ---



--- START 09-macro/CentralBanks.md ---

---
title: CentralBanks
type: textbook
source_path: content/09-macro/CentralBanks.ipynb
chapter: 9
---

# Central Banks

Understanding the levels and relationships between the four main macro indicators mentioned in the previous section is of particular interest to a country’s central bank. 

One way to think about a central bank is that it acts as the “bankers’ bank”. Wells Fargo, Barclays, and many of the other commercial banks that we’re familiar with are regulated and overseen by a given country’s central bank. That’s just a simple way to consider it, but in practice central banks play a much larger role beyond this.

Diving deeper, a central bank is a politically-independent financial institution that is responsible for overseeing the monetary system of a country. This mainly involves doing so by setting interest rates and regulating how much money circulates throughout the economy. Central banks closely monitor and forecast the aforementioned indicators in order to carry out these responsibilities and conduct monetary policy as needed. An important feature of central banks is that they exist independently from the government, meaning the actions they take are not influenced by political pressure nor do they have to go through a congressional body to carry out their policies. 

While central banks across countries differ slightly in their mandates, for the most part they share a common vision in their functions and responsibilities. For example, these include
- Keeping unemployment low
- Maintaining price stability while managing a healthy level of inflation
- Stimulating the economy in times of recession
- Overseeing the commercial banking system and serving as a lender of last resort
- Carrying out economic research and statistical analysis (yay data science!) 
- and [tHere is more in this article called the "Fed Explained"](https://www.federalreserve.gov/aboutthefed/the-fed-explained.htm)

When you hear the terms monetary policy or macroeconomic policy, you can think of it as encompassing what’s listed above. In short, **monetary policy** is the set of actions that a central bank takes to achieve sustainable economic growth, and much of it revolves around adjusting interest rates and the money supply. 

In this chapter we’ll take a closer look at the central bank that serves the United States, known as the **Federal Reserve System.** 

The Fed was established in 1913 to promote stability and flexibility within the United States’ monetary and financial system.Led by the Chair and the Board of Governors, the Fed is comprised of a network of 12 regional banks that serve as the operating arms of the system. Together they guide monetary policy action as well as analyze domestic and international economic conditions. 

[Figure: Map of Federal Reserve Regions]

![regionalbanks.png](regionalbanks.png)

*Here’s a graphic showing the locations of the 12 different regional banks. If you’re ever interested you can go take a tour of the Federal Reserve Branch in SF right on Market St!*

The current Federal Chairman is Jerome Powell, who you’ve probably heard about in the news lately. He’s got quite a bit on his plate with inflation running the highest it’s been in over four decades. Before him came Berkeley’s own Janet Yellen who served from 2014-2018 and was the first woman to hold the position, Ben Bernanke (2006-2014) who helped the US recover from the Great Recession, Alan Greenspan (1987-2006), the “rock star of economics, and Paul Volcker (1979-1987) who is famously known for setting incredibly high interest rates to combat high inflation in the 80s. There have been many [other Chairs](https://www.federalreservehistory.org/people/federal-reserve-chair) as well since the Fed was first established in the early twentieth century. 

[Figure - cartoon of different Fed chairs, comparing their heights to interest rates in their tenure]

![chairs.png](chairs.png)

*The Federal Funds Rate is a nominal interest rate like we discussed earlier in the chapter. Interest rates, which can be thought of as the cost of borrowing, are set low to stimulate the economy, and hiked up in times of inflation in order to “cool off” the economy. Inflation was incredibly high in the 80s after oil supply shocks in the 70s.*

While these Chairs have served as the face of monetary policy throughout history, there is a lot more that goes on behind the curtains. Namely, the **Federal Open Market Committee** is the team of 12 economists that serves as the Fed’s monetary policy-making body, and the Chair stands as the leader of this team. The other voting members of the FOMC include the 6 other governors from the Board of Governors, the president of the New York branch, and 4 of the presidents from the 11 other regional banks. 

The FOMC has 8 scheduled meetings throughout the year, at which they discuss the short and long run outlooks for the US economy as well as monetary policy options. Sometimes they have to hold emergency meetings when the situation appears to be particularly dire, for example when the coronavirus pandemic first hit, prompting a lowering of interest rates to near 0 percent. 

What is particularly important about these FOMC meetings is that the committee designs a course of action for staying committed to the Federal Reserve’s **dual mandate** and releases a statement outlining how they plan to do so. An example of this can be seen in an excerpt from the press release below, 


[Figure - Graphic of a Federal Reserve press release from January 2021]

![FOMC.png](FOMC.png)

Note how the FOMC begins by describing the current state of the economy’s health in terms of the indicators that we learned about earlier in the chapter. They then outline and forecast what they plan to do in response to the situation at hand. This is important because it anchors peoples’ expectations. 

Employment and inflation are particularly relevant because these are the drivers that govern how the Fed designs monetary policy actions according to their dual mandate, which is to **achieve maximum employment** and **maintain price stability**. Adjusting interest rates is directly related to reaching a delicate balance between these two goals. For example, at this snapshot of time in January 2021, the Fed wanted to keep the federal funds rate low in order to provide a boost to an economy that was experiencing “weaker demand”. 

At first glance it is probably pretty confusing how all of these moving parts work hand in hand. But the intuition behind how employment, inflation and interest rates are all related to the dual mandate can best be captured by the **The Taylor Rule**. 

The Taylor Rule was first proposed in 1993 by the American economist John B. Taylor. It’s called a “rule” because it serves as a guide to monetary policy for how to set interest rates based on the inflation gap and output gap at a given time. The equation is as follows

$$i = 2\% + \pi + 0.5(\pi - 2\%) + 0.5(\frac{Y - Y^P}{Y^P})$$

And these are the variables
- i is the nominal interest rate (like we learned about earlier in the chapter) 
- The first 2% is the equilibrium interest rate. You can think of this as the ideal interest rate when production has reached its potential and inflation is stable in the long run. 
- $\pi$ is the current inflation rate
- The second 2% is the target inflation rate. Note how this was mentioned in the FOMC press release above! The Fed wants to have some stable inflation in the long run because that is indicative of a growing economy
$\frac{Y - Y^P}{Y^P}$ is the output gap. $Y^P$ is the potential output if we are at full employment, so this gap tells us how far off we are from that. 
- The 0.5 coefficients reflect the relative balance between how much the Fed cares about inflation versus output. In the newspaper, you will often hear the terms “hawkish” or “dove-ish”. Those who support high rates to combat inflation are hawks, for example Chairman Volcker, whereas doves tend to favor lower interest rates.

In English, you can think of the Taylor Rule as a means of prescribing how nominal interest rates should adjust in response to the observed gaps. For example, the annual inflation rate as of April 2022 is currently 8.5%. Thus the Taylor Rule says that nominal interest rates should go up by 

$$8.5\% + 0.5(8.5\% - 2\%)$$
$$= 8.5\% + 0.5(6.5\%)$$
$$= 11.75\%$$


Now recall the Fisher equation that we learned about earlier the chapter, $r_t = i_t - \pi_t$. Plugging 11.75% and 8.5% in, we see that the Taylor Rule suggests real interest rates should increase to 3.25%. Indeed, this is similar to Jay Powell’s vision as the Federal Reserve is tentatively planning to hike up rates by 0.25% - 0.5% at 6 different occasions over the course of the next year.

Another way that we can directly see how the Taylor Rule is related to the Fed’s dual mandate is to make a simplification using what is called **Okun’s Law**. It was proposed in 1962 by Yale professor Arthur Okun who studied the relationship between unemployment and production.

$$\frac{Y - Y^P}{Y^P}= -C_{okun} (U - U^N)$$

where
- $\frac{Y - Y^P}{Y^P}$ is the output gap as seen before  
- $(U - U^N)$ is the unemployment gap with $U$ being the current level of unemployment and $U^N$ being the natural rate of employment
- and $C_{okun}$ is a coefficient that represents the sensitivity of the unemployment gap to changes in the output gap

The intuition behind Okun’s Law is that the less unemployment there is, the more output that will be produced. This is because there will be more workers contributing to the country’s GDP. Thus the coefficient is negative, and empirically $C_{okun}$ is approximately 2. By plugging Okun’s Law into our equation for the Taylor Rule we get,  

$$i = 2\% + \pi + 0.5(\pi - 2\%) - 0.5C_{okun}(U - U^N)$$

Recall that the Federal Reserve’s dual mandate is to achieve maximum employment and a stable level of inflation close to their target. Therefore we can clearly see how this is captured by the Taylor Rule with Okun’s Law substituted into it. Moreover, this also illustrates how the nominal interest rate is the conventional tool that can allow the Fed to reach a balance between their two goals. 

In the next section we’ll talk about some of the graphs the Federal Reserve looks at to guide their understanding of the economy and its outlook in both the short and long run.

```python

```


--- END 09-macro/CentralBanks.md ---



--- START 09-macro/Indicators.md ---

---
title: Indicators
type: textbook
source_path: content/09-macro/Indicators.ipynb
chapter: 9
---

# Macroeconomic Indicators

The process of conducting macroeconomic policy often starts with studying and projecting the behavior of a variety of indicators that measure the economy’s current or expected future performance. While there are many different variables one could look to depending on the interests and goals of the individuals or institutions involved, in this chapter we will focus specifically on four main indicators that capture the overall health of the economy and thus tend to play a critical role in policy decisions. These are GDP, the unemployment rate, the inflation rate and the real interest rate.

## Gross Domestic Product (𝑌)

Earlier in the semester, when studying Production, we introduced the concept of GDP briefly. To recap, we looked at how GDP serves as a means of capturing a given country’s overall production over a given period of time. In theory it can then be used as a way to measure a country’s economic performance in a given quarter or year; the higher its GDP, the more that it’s produced, the better it’s doing economically, and vice versa. In this chapter, we’ll go into more detail on its significance and how it’s measured. By definition, GDP, often denoted as 𝑌, is measured as the:

**market value** of  
**final goods and services**  
**newly produced**  
in the **domestic economy**   
over a **specified period of time**  

Let’s dive into each of these individually:

First, the *market value* refers to the market price of goods and services, which sets a standard for how we value goods and services. This is especially useful when we are trying to add together products that may be quite different from one another.

Second, *final goods and services* refer to those which are not used up in the production process. We only consider these in our calculation of GDP, because we don’t want to end up overrepresenting the overall level of production. Intermediate goods, which are used up in the production process, inherently add some value to the final product, and therefore if we were to consider these in our calculation, we would end up double counting and overstating the level of production.

Third, *newly produced* refers  that we are only interested in goods and services that were made during the time period we are looking at. Since we are using GDP as a way to measure the level of production in the economy, it wouldn’t make a whole lot of sense to include products that were produced outside the time period that we are looking at in our calculation.

Fourth, in the *domestic economy* refers to the fact that in our calculation of GDP, we only include goods and services that were produced within the geographical area of the area that we are looking at. Given that we are trying to measure the level of production of a certain country, we wouldn’t want to consider products or services that are produced outside of the country in our calculation. 

Lastly, over a *specified period of time* simply points out that GDP is measured within an interval of time. This is important to consider when comparing countries based on their GDP, as we would want to make sure to consider the same time period for all of them. This is also related to the newly produced aspect that we touched on earlier. 

The above definition and approach to calculating GDP considers the production of goods and services in the economy. However, it’s important to note that GDP can also be calculated by looking at the total spending on those goods and services or the total income earned from producing those goods and services. You may recall that in our discussion on Production, we often referred to output and income synonymously. This is because in theory, each of these approaches to calculating GDP (production, expenditure, and income) should all yield the same result, and indeed we find that for the most part they do.

## **Unemployment Rate (𝑈)**

The unemployment rate is also an important measure of a country’s economic performance, as it gives us some insight on the supply and demand of labor in the economy. By definition, the unemployment rate measures the percentage of the labor force – the sum of all employed and unemployed people – that are not currently employed, but are willing to, able to, and looking for work. Mathematically, this can be expressed by the following equation:

$$ Unemployment Rate = \frac{Unemployed}{Labor Force}$$

As stated before, the labor force is just the sum of all employed and unemployed persons in the population, so we can simplify the equation above to be

$$ Unemployment Rate = \frac{Unemployed}{Employed + Unemployed}$$


An important thing to remember is that in order to be considered unemployed, a person must be able to, willing to, and currently looking for work. This means that anyone who is unable to work or has stopped looking for a job is no longer considered part of the labor force and is therefore not included in the unemployment rate. Intuitively this makes sense for the most part, as it probably wouldn’t be very helpful to include retirees or stay-at-home parents or even students for that matter when calculating the unemployment rate. It’s worth considering, however, that this also means that people who have been unable to find any work and therefore stopped looking – often referred to as discouraged workers – would not be represented in the unemployment rate either.

## **Inflation Rate (𝜋)**

Generally speaking, the inflation rate in an economy measures the percent change in prices over a specified period of time, and it is usually calculated using a price index. While there are many different price indices that can be used to calculate inflation, one of the more common ones is the Consumer Price Index (CPI). The CPI measures the average price for a consistent basket of goods and services relative to some defined base year. It is calculated by taking the value of said basket in any given year, dividing it by its value in the base year, and multiplying that by 100. Mathematically, this can be expressed as


$$CPI_t = \frac{Price of basket_t}{Price of basket_0} * 100 $$
where t = 0 refers to the base year.

We can then calculate the inflation rate for a given year as

$$𝜋_t = \frac{CPI_t - CPI_{t-1}}{CPI_{t-1}} * 100 $$

The inflation rate is yet another key indicator that macroeconomists look at, as it can reflect where or not the economy is growing. Generally, a small, positive inflation rate is considered a good thing, as it’s usually indicative of a growing economy. However, inflation rates that are negative or too high can create a lot of problems, and as such the inflation rate tends to play an important role in guiding monetary policy decisions – something we will discuss more later in this chapter.

## **Real Interest Rate (𝑟)**

If you have a bank account or own a credit card or have ever taken a loan, chances are that you’ve come across an interest rate at some point. In general, interest rates represent the cost of borrowing. On the flip side, it also represents the return on saving or lending. In other words, you can think of interest rates as the opportunity cost of holding money. 

There are many different interest rates that can be found in the economy from interest rates for savings accounts to interest rates charged for mortgages to interest rates set by the Central Bank. All of the interest rates that we observe in the economy are known as nominal interest rates – meaning rates that are not adjusted for inflation.

As discussed earlier, inflation measures the change in prices over a given time and can be used in some sense to measure the relative value of a dollar (or other unit of currency) over time. It makes sense then, that we might want to take inflation into account, when deciding what interest rate would make sense to lend/borrow at. To do so, we use real interest rates, which are calculated by taking nominal interest rates and subtracting inflation. This relationship between inflation, nominal interest rates, and real interest rates is captured by what is known as the Fisher Equation

$$r_t = i_t - 𝜋_t$$



where $r_t$ refers to the real interest rate, $i_t$ refers to the nominal interest rate, and $𝜋_𝑡$ refers to the inflation rate, all in a given time period.

As a final note, we find that generally speaking all of the nominal interest rates present in the economy tend to be pretty strongly correlated with one another. Therefore, for purposes of simplification, macroeconomists will often refer to these rates in singular form in their models as simply the nominal interest rate or the real interest rate. As seen in the Fisher equation above, we use 𝑖 to denote the nominal interest rate, and 𝑟 to denote the real interest rate.


--- END 09-macro/Indicators.md ---



--- START 09-macro/fiscal_policy.md ---

---
title: fiscal_policy
type: textbook
source_path: content/09-macro/fiscal_policy.ipynb
chapter: 9
---

# Fiscal Policy

In this section, we will give a broad overview of fiscal policy. 

Fiscal policy is the use of government spending and taxation to influence the economy. Governments typically use fiscal policy to promote strong and sustainable growth and reduce poverty. 

$$ Y = C + I + G + NX $$

**There are many fiscal policy tools at the hands of the government to stabilize the economy. These primarily include changes to levels of taxation and government spending. To stimulate growth, taxes are lowered and government spending is increased, often involving borrowing through issuing government debt. To cool down an overheated economy, the government can use the opposite tools.**

## Classical vs Keynesian View

Why is fiscal policy an effective tool to stabilize the economy? We must first look at two views of macroeconomics and what each of them implies for fiscal policy. 

**Classical economics** claims economic fluctuations arise from “supply 
shocks'' such as fluctuations in productivity, and the aggregate supply is the determinant of economic output. So, the Classical view places little emphasis on the use of fiscal policy to manage aggregate demand. Classical theory is the basis for Monetarism, which only concentrates on managing the money supply, through monetary policy.

**Keynesian economics** suggests recessions often arise from “aggregate demand” shocks, and governments need to use fiscal policy, especially in a recession.

In the AD–AS diagram below, we can see a visual comparison between the economy under the Classical view and the Keynesian view. In the Classical view, the AS curve is vertical. So any increase or decrease in aggregate demand (such as government spending) will not affect the output. On the other hand, in the Keynesian view, increase or decrease in aggregate demand will result in a corresponding change in the output.

![classical_vs_keynesian.png](classical_vs_keynesian.png)
<center> Classical view vs. Keynesian view </center>

Empirically speaking, our economy lies between the Classical view and the Keynesian view. So fiscal policy has the ability to adjust the economy to some extent.

```python

```


--- END 09-macro/fiscal_policy.md ---



--- START 09-macro/index.md ---

---
title: index
type: textbook
source_path: content/09-macro/index.md
chapter: 9
---

# Macroeconomic Policy

## Student Learning Outcomes:
- Learn about the 4 main macro indicators and how they relate to one another
- Learn how to graphically represent these relationships and understand historical trends
- Learn about central banks and the tools they use for monetary and fiscal policy
- Improve macro literacy 


--- END 09-macro/index.md ---



--- START 09-macro/is_curve.md ---

---
title: is_curve
type: textbook
source_path: content/09-macro/is_curve.ipynb
chapter: 9
---

# IS-Curve

In this section we will introduce the IS curve (“investment–saving” curve), an important macroeconomics model that characterizes the relationship between real interest rates and output. **The IS curve is downward sloping. When the real interest rate falls, output will increase.** We will see why this inverse relationship is true and what this relationship implies for the economy. 

[Figure - a downward slopign curve with Real Interest Rate on Y-Axis and Output on X-Axis]

![is_curve.png](is_curve.png)
<center> IS Curve </center>

## Keynesian Cross

First, we will discuss a model of aggregate demand and aggregate output. The Keynesian cross diagram, determines the equilibrium level of real GDP by the point where the total or aggregate demand in the economy is equal to the amount of output produced. 

[Figure - Keynesian cross diagram with two upward sloping lines, Aggregate Demand on Y-xis and Output on X-axis]



```python

```

![keynesian_cross.png](keynesian_cross.png)
<center> Keynesian Cross Diagram </center>

The axes of the Keynesian cross diagram show output / national income (or real GDP) on the horizontal axis and output / aggregate demand on the vertical axis. The orange line represents the aggregate demand, and the blue 45° line represents the aggregate output.

- **Why is the slope of the aggregate output line equal to 1?**   
Both the horizontal axis and the vertical axis are the output of the economy, so they should be the same. 


- **Why is the slope of the aggregate demand line less than 1, and the intercept greater than 0?**  
We will first consider how demand increases when national income rises. People can do two things with their income: consume it or save it (let’s ignore taxes for now). Each person who receives an additional dollar faces this choice. 
The marginal propensity to consume (MPC), is the share of the additional dollar of income a person decides to devote to consumption expenditures. Since the marginal propensity to consume is usually less than 1 (which means not all income is consumed), for every unit increase in national income, we will expect the aggregate demand to increase by less than 1 unit. So, the slope is less than 1.   
However, even when the economy is not producing any output, people still need to consume. They may do so by using their savings or borrowing. So we have a positive intercept.

## Dynamics of Keynesian Cross and Derivation of IS Curve

Now, we are ready to derive the IS curve. 

What will happen if the real interest rate decreases? First, people have less incentive to save money, since the gains from interest income are lower. Second, people have more incentive to spend (especially borrow money to spend) because the opportunity costs of spending are lower. 

**So, when the real interest rate decreases, people will tend to save less and consume more, shifting the aggregate demand curve in the Keynesian Cross upward. The equilibrium output level will therefore increase. The opposite direction will also hold true.** 

This precisely describes the inverse relationship between real interest rate and output. 

[Figure - Kenesian cross diagram with interest rates at 10% or 5%]

![is_derivation.png](is_derivation.png)
<center> Keynesian Cross and IS Curve </center>

## Implication of IS Curve



The IS curve explains the inverse relationship between real interest rate and output. 

A more comprehensive model of how the money markets interact with the goods market can be illustrated by the IS–LM model. The IS–LM model, or Hicks–Hansen model, is a two-dimensional macroeconomic tool that shows the relationship between interest rates and the asset market (also known as real output in goods and services market plus money market). The intersection of the "investment–saving" (IS) and "liquidity preference–money supply" (LM) curves models a "general equilibrium" where equilibria simultaneously occur in both the goods and the asset markets. Hence, this tool is sometimes used not only to analyze economic fluctuations but also to suggest potential levels for appropriate stabilization policies. 

The chart below illustates how each monetary and fiscal policy will affect the economy. 

[Figure - four panel chart with different cases - IS downward sloping , LM upward sloping, interest reate on Y-axis, Income on X-axis, cases are A: Fiscal Expansion, B: Fiscal contration, C:Monetary Expansion, D: Monetary Contraction]

![islm_fiscalmonetarypolicy.png](islm_fiscalmonetarypolicy.png)


<center> </center>

Source: Lev Lafayette, [Chapter 2: The IS–LM model](http://levlafayette.com/node/629)



--- END 09-macro/is_curve.md ---



--- START 09-macro/phillips_curve.md ---

---
title: phillips_curve
type: textbook
source_path: content/09-macro/phillips_curve.ipynb
chapter: 9
---

# Phillips Curve

The Phillips Curve describes an inverse relationship between inflation and unemployment: when the inflation is low, the level of unemployment tends to be high; when the level of unemployment is low, price level tends to increase more rapidly. 

In essence, the Phillips Curve characterizes a tradeoff between economic growth and inflation–we cannot have the best of both worlds. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment. 

However, empirical evidence calls Phillips Curve into question, including the stagflation period that happened during the 1970s when the economy was suffering from both a high inflation and a high unemployment rate. A neoclassical model was then introduced to address this issue.

## Classical Phillips Curve

**The classical Phillips Curve describes the relationship between inflation and unemployment: Inflation is higher when unemployment is low and lower when unemployment is high.**

$$ \pi = - h \cdot ( u - u^{*} ) $$

where $\pi$ is the inflation rate, $u - u^{*}$ is the unemployment gap. 

Often also simplified as

$$ \pi = - h \cdot u $$

where $\pi$ is the inflation rate, $u$ is the unemployment rate.

The relationship was originally described by New Zealand economist A.W. Phillips in his paper titled The Relation between Unemployment and the Rate of Change of Money Wage Rates in 1958, who examined data on unemployment and wages for the United Kingdom from 1861 to 1957.

[Figure - Historic Phillip's Curve downward sloping, Rate of change of money on Y-axis, Unemployment on X-axis]

![original_pc.png](original_pc.png)
<center> The original curve drawn for pre-WW1 data </center>

The underlying logic of this inverse relationship can be explained as follows: **when the demand for labor increases, the pool of unemployed workers subsequently decreases and companies increase wages to compete and attract a smaller talent pool. The cost of wages increases and companies pass along those costs to consumers in the form of price increases. Thus, a lower unemployment rate ultimately translates to a higher inflation.** 

Theoretically, the Phillips curve presents a menu of options for policymakers–if higher inflation actually causes lower levels of unemployment, then the government could control unemployment via monetary policy as long as it was willing to accept changes in the level of inflation.

## Expectation-Augmented Phillips Curve

Unfortunately, economists soon learned that the relationship between inflation and unemployment was not as stable as they had previously thought. 

[ 3 Figures with different decades - 1960-1970, 1980-1990, 2010-2020 with Inflation on Y-axis and Unemployment on X-axis]

![pc_1960s.png](pc_1960s.png)
![pc_1980s.png](pc_1980s.png)
![pc_2010s.png](pc_2010s.png)
<center> Relationship between inflation and unemployment during three periods of time </center>

**What economists initially failed to realize in constructing the Phillips curve was that people and firms take the expected level of inflation into account when deciding how much to produce and how much to consume.** When workers expect prices to rise, they demand higher wages. When firms expect costs to rise, they set higher prices.

**Therefore, economists introduced inflation expectation into the original Phillips Curve.**

$$ \pi = \pi^{e} - h \cdot ( u - u^{*} ) $$

where $\pi$ is the inflation rate, $\pi^{e}$ is the expected inflation rate, $u - u^{*}$ is the unemployment gap.

Therefore, a given level of inflation will eventually be incorporated into the decision-making process and not affect the level of unemployment in the long run. The long-run Phillips curve is vertical, since moving from one constant rate of inflation to another doesn't affect unemployment in the long run.

What this implies is that if the Central Bank would like to pin down inflation, they cannot do it by simply raising the interest rate (even though this is still an effective way in the short run). What is more important is to anchor people’s inflation expectations at a relatively low level. If they succeed, they can control inflation. This idea also appears to have been remarkably successful so far. Whenever it has shot above target, it has, soon enough, fallen back. 

While the idea of inflation expectation looks concrete, interestingly enough, a recent paper (Rudd 2021) sets off a round of debate about the role of expectations in shaping prices in the economy, where the author argued the theory of inflation expectation “rests on extremely shaky foundations”. But still the majority of economists are optimistic about this new Keynesian model. This is an ongoing research field.

> ***Thought experiment***: Spring 2022 marks a season with high inflation. The US inflation rate was sitting at 7.87% in February 2022, compared to 1.68% last year. What would happen if people thought this high level of inflation would persist?

```python

```


--- END 09-macro/phillips_curve.md ---



--- START 10-finance/.ipynb_checkpoints/options-checkpoint.md ---

---
title: options-checkpoint
type: textbook
source_path: content/10-finance/.ipynb_checkpoints/options-checkpoint.ipynb
chapter: 10
---

# Options

Before we discuss options, it's important to understand some basics regarding stocks. A stock is a share in a company. By owning stock you are owning a small part of a company. Stocks trade on a stock exchange, where people come together to buy and sell shares to one another. People who want to buy a stock place a *bid*, or a price at which they want to buy. Others who want to sell place an *ask*, or a price that they want to sell. The market price of a stock is where these bids and asks come together.

If I purchase and hold onto a stock, I am said to have a *long* position on the stock. If the stock goes up in value, I profit. If it goes down in value, I lose. It is also possible to have inverse exposure to the price of a stock, meaning that if the price of the stock goes down I profit, and if it goes up I lose. This is called shorting the stock, or having a *short* position on the stock. This is accomplished by borrowing stock from a stock broker and selling it today, and then buying it back sometime in the future to pay off your "loan" of borrowed stock from the broker. 

You can see how you can profit from this if you sell stock at \$100. Then, if the value of the stock goes down to \$70 in the future, you can buy back the stock using the \$100 you made earlier, thus finishing your loan from the broker. However, you have \$30 left over. You made \$100 selling the stock, yet it only cost \$70 to buy it back. Thus, you have made a profit from the stock going down in value.

Recall that in a short position, you never actually owned the stock to begin with. You borrowed the stock from a stock broker, and you paid back that stock in the future. In reality, the stock broker wants to be paid for the service it provides you, and someone shorting stock will have to effectively pay interest on that "loan", just like a normal loan.

## Puts

Suppose you own some stock in an investment account. You want your stock to be able to increase in value over time, but you also don't want its value to decrease too much. One way to think about this is that you want to own some asset that has asymmetrical payoff; you want all the potential upside of owning the asset with as little of the downside as possible.

One way to achieve this is to buy "insurance" on your stock. You might want an insurance contract that will cover losses if the value of your stock goes below a certain number. This type of contract exists, and they are called *puts*.

The simplest way to think about a put is that it is a contract that pays you a dollar for each dollar that your stock does below some specified number. So for instance, if you own a stock that trades at \$110, and you don't want to lose more than \$10 in value from owning the stock, you might buy a put with a *strike* of \$100. The strike of the put is this pre-specified number below which you don't want to lose money. Now, let's say your stock starts going down in value. Going down from \$110 to \$100, there's nothing that your put can do. But starting at \$100, each dollar that your stock goes down in value, your put pays you one dollar. Therefore, when you own this put *in combination* with the stock, the overall value of this combination cannot go below \$100.

Let's call this combination of stock and put a *portfolio*. Your portfolio's value depends on the value of the stock, and the portfolio's value looks like this:

![title](figure1.png)

Now that we have the basic intuition down, let's get more specific. While a put behaves like insurance, the way a put is technically defined is a bit different. A put contract says the following:

> "The holder of this contract has the right, *but not the obligation*, to sell 100 shares of an underlying stock at a specified strike price, from now until some expiration date."

You can see how this behaves essentially as insurance. If the strike price is \$100 as above, and your stock goes below \$100 in value, you might want to exercise your right to sell your shares at \$100. Regardless of how far below \$100 your stock is, the put allows you to sell at \$100. Additionally, if your stock is valued above \$100, there's no reason to exercise your right to sell at \$100; you could just sell at whatever price your stock is trading.

## Calls

The opposite of a put, in a sense, is a *call*. A call contract specifies the following:

> "The holder of this contract has the right, *but not the obligation*, to buy 100 shares of an underlying stock at a specified strike price, from now until some expiration date."

You could technically interpret calls as insurance for people who are *short* some stock, but this isn't the most helpful way to think about them. A better way to think about calls is the following: Suppose you want to buy and hold a stock, but you aren't sure if the value of the stock will go up in a desired time frame. Instead of buying the stock and risking that its value will decrease, you could buy a call that gives you the *opportunity* to purchase the stock at some price that you want, say \$100. That way, if the value of the stock goes above \$100, instead of missing out on that increase in value, your call gives you the right to purchase the stock at \$100, even though it is actually worth more than \$100. If the stock is worth less than \$100 after some time, you don't have to do anything and you didn't lose the value you would have lost if you had purchased the stock.

## Payoff Diagrams

Instead of thinking about the value of a portfolio with a stock and a put, let's just think about the value of a put given the price of the underlying stock. In other words, you don't actually own the stock, you just own the put. Let's use the example of a put with a strike of \$100 as above. If the stock is trading somewhere below \$100, say \$90, then the put has a payoff of \$10. The reason for this is because you could purchase the stock for \$90, and then use your put, which gives you the right to sell a stock for \$100. That's a profit of \$10, and therefore the put has a payoff of \$10.

Similarly, if the stock is trading above \$100, the put has no payoff. If you were to buy the stock at its price and use your put to sell at \$100, you would lose money. This is because it cost you more than \$100 to buy the stock, but you only sold for \$100. Since you are never actually obligated to use a put, you would not use it in this case, and it has 0 payoff.

**Notice that all of this occured without you ever owning the stock to begin with.**

Now let's plot the payoff of this put with a strike of \$100, given the price of the underlying stock.

![title](figure2.png)

The payoff of a call works similarly. Remember that a call gives you the right to buy a stock at a certain price. So if you own a call with a strike of \$100, and the underlying stock is trading at \$110, the call has a payoff of \$10. This is because you could use your call to buy the stock for \$100, and then sell it for \$110, since this is the market price of the stock. And again, if the stock is trading below \$100 the call has no payoff. This is because if you were to use your call to purchase the stock at \$100, you could only sell it for less than \$100, thus losing money. A rational investor would never use the call for this, and therefore it has no payoff. Below is the payoff diagram for a call with a strike of \$100.

![title](figure3.png)

And quickly, here are the payoff diagrams for being long and short a stock, which should appear trivial by now. These diagrams ignore the price of the stock when you bought/shorted it. In other words, if you purchased the stock at \$100, and sold at \$100, the diagram implies a payoff of \$100. This is to be consistent with the above diagrams, in which we implicitly assumed that there is no cost in buying an option, a claim which we will examine below.

![title](figure4.png)

![title](figure5.png)

Now, we see how we can generate the payoff diagram for the portfolio of a stock and a put from above. The diagram for that portfolio is copied below.

![title](figure1.png)

Notice how we can generate this by adding the payoff diagrams of a stock and a put. We have shown that the payoff diagram of a portfolio can be represented as the sum of payoff diagrams of its components.

![title](figure6.png)

## Pricing Options

We have only been studying one side of an option contract: the holder of the option. Of course, in order to own a contract that gives you the right to buy or sell a stock at a certain price, someone has to be willing to guarantee you that right. Offering you that right comes with some risk, because whoever sells you the contract might be obligated to buy or sell a stock from you at a price that is not favorable. Because of this, they will ask for payment in return. Therefore, just like any other form of insurance, options are not free.

Let's think about what could contribute to an option's price. We know that someone is taking on risk by selling you an option, so whatever puts that person at an increased risk of losing money should make the option more expensive. For this lesson let's only think about the price of a call.

* **Strike price**. Suppose a stock is trading at \$100, and I am interested in a call with a strike of \$110. The person who sells me that call bears some risk, because the price of the stock might go above \$110 sometime in the future, in which case I would profit and the person who sold me the option would lose money. Now suppose I look at a call with a strike of \$120. The person selling me the \$120 call bears some risk, but not as much as the first person, because it is less likely that the stock's price exceeds \$120 sometime in the future compared to the chance that it exceeds \$110. So which of these two options should cost more, holding all else equal? Naturally, the call with the strike of \$110 should cost more, because it is more likely that I make money with this call as opposed to the \$120 call, and therefore more likely that the person selling it to me loses money. **This shows that for calls, the lower the strike price, the more expensive the call becomes**.
* **Time until expiration**. We briefly mentioned earlier in the definitions for calls and puts that options are only active for a certain time period. So if a call is only active for 1 week, while an otherwise identical call is active for 1 year, which should cost more? Using similar logic as above, the call lasting 1 year puts the option seller at a higher risk of losing money, since there is more time for the underlying stock price to move in such a way that is disadvantageous. The call lasting 1 week doesn't have much time at all to move in such a way that the option seller loses money. **This shows that for calls, the farther away the expiration, the more expensive the call becomes**.
* **Volatility**. Imagine a stock is trading at \$100, and historically this stock's price does not move very much at all. Now, imagine another stock that is also trading at \$100, but has a history of wild price swings. In this example, if there exist two calls with otherwise identical attributes on these two stocks, which one should cost more? We say that the stock with a history of price swings is more volatile than the more tame stock. The more volatile stock has a higher chance of jumping up in price to a point where you can make a profit compared to the more tame stock. **This shows that for calls, the higher the volatility of the underlying stock, the more expensive the call becomes**.
* **Underlying stock price**. This should be the most obvious factor that affects the prices of options. If someone is offering to sell a call with a strike of \$100 on some stock, and the stock's price is \$90, then the call option will have some price. But if the stock jumps in value to \$95, then clearly the call option will be worth more. After the stock's price increases to \$95, it becomes more likely that the stock' price can exceed \$100 sometime in the future. This exposes the person selling you the call to a higher risk of losing money, and therefore the person will charge more for the call. **This shows that for calls, the higher the price of the underlying stock, the more expensive the call becomes**.
* **The risk-free interest rate**. This one is a bit less intuitive and we will not be discussing this in depth, but the prevailing interest rate of risk-free deposits also affects the prices of options.

### Black-Scholes

So how would we calculate the fair price of an option? Intuitively, the fair price of an option should be the payoff of the option for every possible price of the stock, weighted by the probability of that stock being at that particular price, discounted into the present (since you receive some payment in the future). It turns out that this is a pretty complicated problem to solve, and we will not ask you derive the following formulae. Below is an expression for the price of a call and a put.

$$\begin{aligned}
C &= S \cdot N(d_1) - K \cdot e^{-r_{rf}T} \cdot N(d_2) \\
P &= K \cdot e^{-r_{rf}T} \cdot N(-d_2) - S \cdot N(-d_1)
\end{aligned}$$

where $S$ is the price of the stock, $K$ is the strike of the call, $r_{rf}$ is the risk-free interest rate, $T$ is time till expiration, $N()$ is the normal CDF, $d_1 = \frac{ln(S/K) - (r_{rf} + \sigma^2/2)T}{\sigma\sqrt{T}}$, and $d_2 = d_1 - \sigma\sqrt{T}$. This assumes that stock returns are normally distributed.

```{admonition} Disclaimer
:class: attention

These expressions are actually the prices **European options**. A European option is an option that only allows you to exercise (use the option) exactly on the expiration date, and not before. The definitions for options that we gave earlier correspond to **American options**, which allow you to exercise the option at any time, even before expiration. The reason we have the prices for European options and not American options is because the prices for American options are actually even more difficult to find. For example, American puts are priced quite differently from European puts. However, for a stock that does not pay dividends, the prices for American calls and European calls tend to be similar.
```

## Trading Options

Options can be traded on an exchange just like stocks. You can buy an option today, and if something happens that makes you not want to hold the option anymore, you can sell it tomorrow. Options therefore have some prevailing market price that is determined by buyers and sellers, but in a rational market the prices of options must follow the rules/trends defined above.

Importantly, you aren't restricted to just buying options. You can also assume the role of the person taking on risk by selling an option to someone else. Let's think about how this would work.

Suppose you want to buy a call option from me on some underlying stock currently trading at \$90. You want the option to have a strike of \$100, and you want it to expire in 1 month. You would have to pay me whatever the fair market price for this option is, but there's one more step that I would then have to take.

By owning a call, you have the right to buy the underlying stock from me at \$100, even if the price of the stock goes above \$100. From my perspective, that means that I might have the *obligation* to *sell* you stock at a price of \$100. In order to cover this obligation, I will have to put up some collateral to guarantee that I will be able to pay my obligation, should the time come. The specific rules for collateral vary with different brokers and other conditions.

Suppose I only needed to put up enough collateral to cover a \$20 increase in stock price. That means that I need to be able to afford buying stock at \$110, and selling it to you at \$100. That's a difference of \$10. But recall that option contracts actually deal with increments of 100 shares. So I will actually have to post \$10 x 100 = \$1000 in collateral. If the stock price does in fact jump to \$110, my broker may then ask me to put even more money down for collateral, in the event that it continues to go up.

If for some reason the rules specify that I need to completely cover all the risk of selling this call, no amount of money will be sufficient to cover the cost of buying shares at some unknown price in the future and selling at \$100. This is because there is no theoretical limit to how high the price of a stock can reach. In this situation, I would actually have to post 100 shares of the stock as collateral at the same time that I sell you the call. This way, I am guaranteed to have 100 shares of stock to sell to you in the event that you use your call.

Selling calls works similarly, but instead I need to have enough money down as collateral to afford to *buy* shares from you at the strike price.

## Returns 

This portion of the lecture is fairly simple. A return on security is essentially the money made or lost by investing in the security over a period of time.

The return on security has two components:
1. The change in price of the security 
2. Any cash flows associated with the security (dividends for stock, coupons for bonds)

If we write the change in price over a time interval τ as
 $∆x_t = x_{t+τ} − x_t = x(t + τ) − x(t)$,
then we can write the rate of return r as
$r(t) = \frac{x(t + τ) −x(t) + income − costs}{x(t)}$

### Calculating Returns Using an API

```python
#importing packages needed to access the API
!pip install yfinance
import yfinance as yf
import numpy as np
```

```python
data = yf.download("^GSPC", start="1993-01-01", end="2021-01-01")
# In code above, we input the ticker symbol for S&P500 and specify the start and end time intervals
```

```python
data
```

Let's plot the graph for the closing price of S&P500.

```python
data['Close'].plot(color="purple",figsize=(10,8), title = 'S&P500 Returns');
```

The graph below shows the returns from S&P500 as a function of time over the previous time interval.

![title](figure9.png)


--- END 10-finance/.ipynb_checkpoints/options-checkpoint.md ---



--- START 10-finance/index.md ---

---
title: index
type: textbook
source_path: content/10-finance/index.md
chapter: 10
---

# Finance

This week, we will be covering some of the greatest hits of Financial Economics - most of which you can learn by taking Econ 136. We will begin our discussion with an important introduction to interest rates and the time value of money. After, we will pivot to stock options. These are ways investors can bet on stock value movements through purchasing or selling contracts that only have value at certain stock prices.


--- END 10-finance/index.md ---



--- START 10-finance/options.md ---

---
title: options
type: textbook
source_path: content/10-finance/options.ipynb
chapter: 10
---

# Options

Before we discuss options, it's important to understand some basics regarding stocks. A stock is a share in a company. By owning stock you are owning a small part of a company. Stocks trade on a stock exchange, where people come together to buy and sell shares to one another. People who want to buy a stock place a *bid*, or a price at which they want to buy. Others who want to sell place an *ask*, or a price that they want to sell. The market price of a stock is where these bids and asks come together.

If I purchase and hold onto a stock, I am said to have a *long* position on the stock. If the stock goes up in value, I profit. If it goes down in value, I lose. It is also possible to have inverse exposure to the price of a stock, meaning that if the price of the stock goes down I profit, and if it goes up I lose. This is called shorting the stock, or having a *short* position on the stock. This is accomplished by borrowing stock from a stock broker and selling it today, and then buying it back sometime in the future to pay off your "loan" of borrowed stock from the broker. 

You can see how you can profit from this if you sell stock at \$100. Then, if the value of the stock goes down to \$70 in the future, you can buy back the stock using the \$100 you made earlier, thus finishing your loan from the broker. However, you have \$30 left over. You made \$100 selling the stock, yet it only cost \$70 to buy it back. Thus, you have made a profit from the stock going down in value.

Recall that in a short position, you never actually owned the stock to begin with. You borrowed the stock from a stock broker, and you paid back that stock in the future. In reality, the stock broker wants to be paid for the service it provides you, and someone shorting stock will have to effectively pay interest on that "loan", just like a normal loan.

## Puts

Suppose you own some stock in an investment account. You want your stock to be able to increase in value over time, but you also don't want its value to decrease too much. One way to think about this is that you want to own some asset that has asymmetrical payoff; you want all the potential upside of owning the asset with as little of the downside as possible.

One way to achieve this is to buy "insurance" on your stock. You might want an insurance contract that will cover losses if the value of your stock goes below a certain number. This type of contract exists, and they are called *puts*.

The simplest way to think about a put is that it is a contract that pays you a dollar for each dollar that your stock does below some specified number. So for instance, if you own a stock that trades at \$110, and you don't want to lose more than \$10 in value from owning the stock, you might buy a put with a *strike* of \$100. The strike of the put is this pre-specified number below which you don't want to lose money. Now, let's say your stock starts going down in value. Going down from \$110 to \$100, there's nothing that your put can do. But starting at \$100, each dollar that your stock goes down in value, your put pays you one dollar. Therefore, when you own this put *in combination* with the stock, the overall value of this combination cannot go below \$100.

Let's call this combination of stock and put a *portfolio*. Your portfolio's value depends on the value of the stock, and the portfolio's value looks like this:

[Figure Payoff Diagram - Value on Y-axis, price of underlying stock on X-axis, line is flat then upward sloping ] 


![title](figure1.png)

Now that we have the basic intuition down, let's get more specific. While a put behaves like insurance, the way a put is technically defined is a bit different. A put contract says the following:

> "The holder of this contract has the right, *but not the obligation*, to sell 100 shares of an underlying stock at a specified strike price, from now until some expiration date."

You can see how this behaves essentially as insurance. If the strike price is \$100 as above, and your stock goes below \$100 in value, you might want to exercise your right to sell your shares at \$100. Regardless of how far below \$100 your stock is, the put allows you to sell at \$100. Additionally, if your stock is valued above \$100, there's no reason to exercise your right to sell at \$100; you could just sell at whatever price your stock is trading.

## Calls

The opposite of a put, in a sense, is a *call*. A call contract specifies the following:

> "The holder of this contract has the right, *but not the obligation*, to buy 100 shares of an underlying stock at a specified strike price, from now until some expiration date."

You could technically interpret calls as insurance for people who are *short* some stock, but this isn't the most helpful way to think about them. A better way to think about calls is the following: Suppose you want to buy and hold a stock, but you aren't sure if the value of the stock will go up in a desired time frame. Instead of buying the stock and risking that its value will decrease, you could buy a call that gives you the *opportunity* to purchase the stock at some price that you want, say \$100. That way, if the value of the stock goes above \$100, instead of missing out on that increase in value, your call gives you the right to purchase the stock at \$100, even though it is actually worth more than \$100. If the stock is worth less than \$100 after some time, you don't have to do anything and you didn't lose the value you would have lost if you had purchased the stock.

## Payoff Diagrams

Instead of thinking about the value of a portfolio with a stock and a put, let's just think about the value of a put given the price of the underlying stock. In other words, you don't actually own the stock, you just own the put. Let's use the example of a put with a strike of \$100 as above. If the stock is trading somewhere below \$100, say \$90, then the put has a payoff of \$10. The reason for this is because you could purchase the stock for \$90, and then use your put, which gives you the right to sell a stock for \$100. That's a profit of \$10, and therefore the put has a payoff of \$10.

Similarly, if the stock is trading above \$100, the put has no payoff. If you were to buy the stock at its price and use your put to sell at \$100, you would lose money. This is because it cost you more than \$100 to buy the stock, but you only sold for \$100. Since you are never actually obligated to use a put, you would not use it in this case, and it has 0 payoff.

**Notice that all of this occured without you ever owning the stock to begin with.**

Now let's plot the payoff of this put with a strike of \$100, given the price of the underlying stock.

[Figure Payoff Diagram - Value on Y-axis, price of underlying stock on X-axis, line is downward sloping then flat at zero ] 

![title](figure2.png)

The payoff of a call works similarly. Remember that a call gives you the right to buy a stock at a certain price. So if you own a call with a strike of \$100, and the underlying stock is trading at \$110, the call has a payoff of \$10. This is because you could use your call to buy the stock for \$100, and then sell it for \$110, since this is the market price of the stock. And again, if the stock is trading below \$100 the call has no payoff. This is because if you were to use your call to purchase the stock at \$100, you could only sell it for less than \$100, thus losing money. A rational investor would never use the call for this, and therefore it has no payoff. Below is the payoff diagram for a call with a strike of \$100.

[Figure Payoff Diagram - Value on Y-axis, price of underlying stock on X-axis, line is flat at zero then upward sloping ] 

![title](figure3.png)

And quickly, here are the payoff diagrams for being long and short a stock, which should appear trivial by now. These diagrams ignore the price of the stock when you bought/shorted it. In other words, if you purchased the stock at \$100, and sold at \$100, the diagram implies a payoff of \$100. This is to be consistent with the above diagrams, in which we implicitly assumed that there is no cost in buying an option, a claim which we will examine below.

[Figure Payoff Diagram - Value on Y-axis, price of underlying stock on X-axis, line is upward sloping ] 

![title](figure4.png)

[Figure Payoff Diagram - Value on Y-axis, price of underlying stock on X-axis, line is downward sloping ] 


![title](figure5.png)

Now, we see how we can generate the payoff diagram for the portfolio of a stock and a put from above. The diagram for that portfolio is copied below.

[Figure Payoff Diagram - Value on Y-axis, price of underlying stock on X-axis, line is flat and then upward sloping ] 

![title](figure1.png)

Notice how we can generate this by adding the payoff diagrams of a stock and a put. We have shown that the payoff diagram of a portfolio can be represented as the sum of payoff diagrams of its components.

[Figure Payoff Diagram - Value on Y-axis, price of underlying stock on X-axis, there are lines for Put, Stock and Portfolio overlaid ] 

![title](figure6.png)

## Pricing Options

We have only been studying one side of an option contract: the holder of the option. Of course, in order to own a contract that gives you the right to buy or sell a stock at a certain price, someone has to be willing to guarantee you that right. Offering you that right comes with some risk, because whoever sells you the contract might be obligated to buy or sell a stock from you at a price that is not favorable. Because of this, they will ask for payment in return. Therefore, just like any other form of insurance, options are not free.

Let's think about what could contribute to an option's price. We know that someone is taking on risk by selling you an option, so whatever puts that person at an increased risk of losing money should make the option more expensive. For this lesson let's only think about the price of a call.

* **Strike price**. Suppose a stock is trading at \$100, and I am interested in a call with a strike of \$110. The person who sells me that call bears some risk, because the price of the stock might go above \$110 sometime in the future, in which case I would profit and the person who sold me the option would lose money. Now suppose I look at a call with a strike of \$120. The person selling me the \$120 call bears some risk, but not as much as the first person, because it is less likely that the stock's price exceeds \$120 sometime in the future compared to the chance that it exceeds \$110. So which of these two options should cost more, holding all else equal? Naturally, the call with the strike of \$110 should cost more, because it is more likely that I make money with this call as opposed to the \$120 call, and therefore more likely that the person selling it to me loses money. **This shows that for calls, the lower the strike price, the more expensive the call becomes**.
* **Time until expiration**. We briefly mentioned earlier in the definitions for calls and puts that options are only active for a certain time period. So if a call is only active for 1 week, while an otherwise identical call is active for 1 year, which should cost more? Using similar logic as above, the call lasting 1 year puts the option seller at a higher risk of losing money, since there is more time for the underlying stock price to move in such a way that is disadvantageous. The call lasting 1 week doesn't have much time at all to move in such a way that the option seller loses money. **This shows that for calls, the farther away the expiration, the more expensive the call becomes**.
* **Volatility**. Imagine a stock is trading at \$100, and historically this stock's price does not move very much at all. Now, imagine another stock that is also trading at \$100, but has a history of wild price swings. In this example, if there exist two calls with otherwise identical attributes on these two stocks, which one should cost more? We say that the stock with a history of price swings is more volatile than the more tame stock. The more volatile stock has a higher chance of jumping up in price to a point where you can make a profit compared to the more tame stock. **This shows that for calls, the higher the volatility of the underlying stock, the more expensive the call becomes**.
* **Underlying stock price**. This should be the most obvious factor that affects the prices of options. If someone is offering to sell a call with a strike of \$100 on some stock, and the stock's price is \$90, then the call option will have some price. But if the stock jumps in value to \$95, then clearly the call option will be worth more. After the stock's price increases to \$95, it becomes more likely that the stock' price can exceed \$100 sometime in the future. This exposes the person selling you the call to a higher risk of losing money, and therefore the person will charge more for the call. **This shows that for calls, the higher the price of the underlying stock, the more expensive the call becomes**.
* **The risk-free interest rate**. This one is a bit less intuitive and we will not be discussing this in depth, but the prevailing interest rate of risk-free deposits also affects the prices of options.

### Black-Scholes

So how would we calculate the fair price of an option? Intuitively, the fair price of an option should be the payoff of the option for every possible price of the stock, weighted by the probability of that stock being at that particular price, discounted into the present (since you receive some payment in the future). It turns out that this is a pretty complicated problem to solve, and we will not ask you derive the following formulae. Below is an expression for the price of a call and a put.

$$\begin{aligned}
C &= S \cdot N(d_1) - K \cdot e^{-r_{rf}T} \cdot N(d_2) \\
P &= K \cdot e^{-r_{rf}T} \cdot N(-d_2) - S \cdot N(-d_1)
\end{aligned}$$

where $S$ is the price of the stock, $K$ is the strike of the call, $r_{rf}$ is the risk-free interest rate, $T$ is time till expiration, $N()$ is the normal CDF, $d_1 = \frac{ln(S/K) - (r_{rf} + \sigma^2/2)T}{\sigma\sqrt{T}}$, and $d_2 = d_1 - \sigma\sqrt{T}$. This assumes that stock returns are normally distributed.

```{admonition} Disclaimer
:class: attention

These expressions are actually the prices **European options**. A European option is an option that only allows you to exercise (use the option) exactly on the expiration date, and not before. The definitions for options that we gave earlier correspond to **American options**, which allow you to exercise the option at any time, even before expiration. The reason we have the prices for European options and not American options is because the prices for American options are actually even more difficult to find. For example, American puts are priced quite differently from European puts. However, for a stock that does not pay dividends, the prices for American calls and European calls tend to be similar.
```

## Trading Options

Options can be traded on an exchange just like stocks. You can buy an option today, and if something happens that makes you not want to hold the option anymore, you can sell it tomorrow. Options therefore have some prevailing market price that is determined by buyers and sellers, but in a rational market the prices of options must follow the rules/trends defined above.

Importantly, you aren't restricted to just buying options. You can also assume the role of the person taking on risk by selling an option to someone else. Let's think about how this would work.

Suppose you want to buy a call option from me on some underlying stock currently trading at \$90. You want the option to have a strike of \$100, and you want it to expire in 1 month. You would have to pay me whatever the fair market price for this option is, but there's one more step that I would then have to take.

By owning a call, you have the right to buy the underlying stock from me at \$100, even if the price of the stock goes above \$100. From my perspective, that means that I might have the *obligation* to *sell* you stock at a price of \$100. In order to cover this obligation, I will have to put up some collateral to guarantee that I will be able to pay my obligation, should the time come. The specific rules for collateral vary with different brokers and other conditions.

Suppose I only needed to put up enough collateral to cover a \$20 increase in stock price. That means that I need to be able to afford buying stock at \$110, and selling it to you at \$100. That's a difference of \$10. But recall that option contracts actually deal with increments of 100 shares. So I will actually have to post \$10 x 100 = \$1000 in collateral. If the stock price does in fact jump to \$110, my broker may then ask me to put even more money down for collateral, in the event that it continues to go up.

If for some reason the rules specify that I need to completely cover all the risk of selling this call, no amount of money will be sufficient to cover the cost of buying shares at some unknown price in the future and selling at \$100. This is because there is no theoretical limit to how high the price of a stock can reach. In this situation, I would actually have to post 100 shares of the stock as collateral at the same time that I sell you the call. This way, I am guaranteed to have 100 shares of stock to sell to you in the event that you use your call.

Selling calls works similarly, but instead I need to have enough money down as collateral to afford to *buy* shares from you at the strike price.

## Returns 

This portion of the lecture is fairly simple. A return on security is essentially the money made or lost by investing in the security over a period of time.

The return on security has two components:
1. The change in price of the security 
2. Any cash flows associated with the security (dividends for stock, coupons for bonds)

If we write the change in price over a time interval τ as
 $∆x_t = x_{t+τ} − x_t = x(t + τ) − x(t)$,
then we can write the rate of return r as
$r(t) = \frac{x(t + τ) −x(t) + income − costs}{x(t)}$

### Calculating Returns Using an API

```python
#importing packages needed to access the API
!pip install yfinance
import yfinance as yf
import numpy as np
```

```python
data = yf.download("^GSPC", start="1993-01-01", end="2021-01-01")
# In code above, we input the ticker symbol for S&P500 and specify the start and end time intervals
```

```python
data
```

Let's plot the graph for the closing price of S&P500.'

[Figure returns for S&P 500 stock index - Value on Y-axis, Time on X-axis ]

```python
data['Close'].plot(color="purple",figsize=(10,8), title = 'S&P500 Returns');
```

The graph below shows the returns from S&P500 as a function of time over the previous time interval. 

[Figure Returns - Value of Returns on Y-axis, time on X-axis]

![title](figure9.png)


--- END 10-finance/options.md ---



--- START 10-finance/value-interest.md ---

---
title: value-interest
type: textbook
source_path: content/10-finance/value-interest.ipynb
chapter: 10
---

# Present Value, Future Value, and Interest Rates

## The Time Value of Money

An important concept that is the basis for most of finance is the *time value of money*: money now is worth more than money in the future. This makes sense; you would rather have \$100 now than later. But what if I owed you money and I really wanted to postpone the payment? In order to compensate you for your bias toward having money as soon as possible, I would need to pay more than I owed. Otherwise, you might not tolerate a delayed payment. This idea of an extra payment to address time concerns is called *interest*. There is also a dimension of risk as well. If you had reason to doubt my ability to repay you in the future, you might charge me more interest to compensate for this risk.

```{admonition} Definition
**Interest** is payment at a particular rate for the use of money or for delaying the repayment of money. Interest is typically set at a specific rate and that rate fluctuates based on the amount of time between borrowing and repayment, the risk of default, and other factors determined by the lender.
```

## Interest

Interest is at the heart of loans, fixed income securities (think bonds), and financial economics in general. We are familiar with the bank account: you give a certain amount of money to a financial institution, and they compensate you for allowing them to use your money to invest in other assets. What they pay you for keeping your money is interest, and is normally quantified as a percentage of what you deposit with them - an interest rate. Thus, for each \$1 deposited at the bank, you will receive $r$ dollars in interest, where $r$ is the interest rate quoted by the bank. Note that $r$ takes the form of a decimal value: 0.05, for instance, and not 5%.

Interest can be paid out in different time intervals - usually monthly, quarterly, yearly or continuously. Also, if you earn some interest in one year, in the next year you will not only earn interest on the initial amount you deposited, but also on the amount you earned the year before. This reflects the idea of *compounding interest*. We are able to determine how much \$1 will be worth in $t$ years, when compounding $n$ times per year at an interest rate of $r$.

$$\begin{aligned}
\text{Value of 1 dollar in } t \text{ years} &= 1 \times \left(1 + \dfrac{r}{n} \right) \times \left(1 + \dfrac{r}{n} \right) \times \cdots \times \left(1 + \dfrac{r}{n} \right) \\
&= 1 \left(1 + \dfrac{r}{n} \right)^{nt}
\end{aligned}$$

Take a look at the table below for an example of the effects of compounding.

|                        | Bank 1: 5\% annual compounding | Bank 2: 5\% semi-annual compounding                       |
|------------------------|--------------------------------|-----------------------------------------------------------|
| January 2016           | \$100                          | \$100                                                     |
| July 2016              |                                | $100 \left(1 + \dfrac{0.05}{2} \right) =$ \$102.50        |
| January 2017           | $100 (1 + 0.05) =$ \$105       | $102.50 \left(1 + \dfrac{0.05}{2} \right) =$ \$105.0625   |
| July 2017              |                                | $105.0625 \left(1 + \dfrac{0.05}{2} \right) =$ \$107.69   |
| January 2018           | $105 (1 + 0.05) =$ \$110.25    | $107.69 \left(1 + \dfrac{0.05}{2} \right) =$ \$110.38     |
| Total Percent Change   | 10.25%                         | 10.38%                                                    |

Notice that instead of a $5\% \cdot 2 = 10\%$ increase, you end up receiving a 10.25% or 10.38% increase depending on the rate of compounding. This is because the interest you received in the first compounding period (in bank 1’s case, a year, in bank 2’s case, half a year) is added onto your initial deposit, and this new deposit is used for calculating interest in the next period. Thus, even a small amount of money can grow quickly under interest rate compounding.

## Present Value, Future Value, and the Discount Factor

An important related concept is the idea of present and future value (which are effectively opposites). We have already discussed future value above. A \$100 deposit at bank 1 above has a future value of \$110.25 after 2 years. Conversely, an important question frequently asked in finance is the following:

> Given an amount of money in the future, what is its fair value today? 

In this example, what is the present value of \$110.25 at bank 1 two years in the future? Well, from the table above, \$100! This idea of present value is essential to the pricing of assets. In general, an asset's price is the present value of all expected future payments.

$$\begin{aligned}
\text{FV of 1 dollar} &= 1 \times \left(1 + \dfrac{r}{n} \right)^{nt} \\
\text{PV of 1 dollar} &= \dfrac{1}{\left(1 + \dfrac{r}{n} \right)^{nt}}
\end{aligned}$$

We call $\dfrac{1}{(1 + \frac{r}{n})^{nt}}$ a *discount factor*. It discounts the value of \$1 from the future into today. This ties in with the time value of money. Since a dollar today is worth more than a dollar tomorrow, in order for you to be indifferent between receiving money today or tomorrow, the money you would receive tomorrow has to be discounted into the present by some amount that depends on the interest rate.

$$
\text{PV} = \text{DF} \cdot \text{FV}
$$



--- END 10-finance/value-interest.md ---



--- START 11-econometrics/.ipynb_checkpoints/multivariable-checkpoint.md ---

---
title: multivariable-checkpoint
type: textbook
source_path: content/11-econometrics/.ipynb_checkpoints/multivariable-checkpoint.ipynb
chapter: 11
---

# Multivariable Regression and Bias

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import warnings
warnings.simplefilter("ignore")
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import patches
from datascience import *
%matplotlib inline
from sklearn.metrics import mean_squared_error
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets

nlsy = Table.read_table('nlsy_cleaned_small.csv')
nlsy = nlsy.with_column("college", nlsy.column("college").astype(float))
nlsy = nlsy.with_column("AFQT", nlsy.column("AFQT").astype(float))
```

Our procedure earlier showed that we expect to see roughly a 70% increase in earnings in people who went to college vs. people who did not go to college. Does this imply that your decision to go to college was worthwhile, and now you can expect to have roughly 70% higher earnings compared to the version of you who did not go to college?

Let's go back to our discussion of experiments from earlier. In an ideal experiment, we would want a good sample of people who are about to graduate high school, and then randomly assign them to either a treatment group that gets sent to college, and a control group that does not. If you are in the treatment group you *must* go to college, and if you are in the control group you *cannot* go to college. Since the decision to go to college in this case is completely random, we can safely assume that the treatment and control groups are on average identical in attributes, apart from college attendance. We can therefore compare their log earnings in the future to see the effect of going to college.

Clearly this experiment is impossible to perform. We cannot randomly assign people to go to college. What's different between this ideal experiment and our regression from earlier? What's the issue with comparing the differences in log earnings for people in our sample who happened to go to college and those who did not?

In our sample, the treatment (went to college) and control (did not go to college) groups are not identical in every way except for college! People aren't randomly assigned college, they *choose* to go to college. The factors that cause someone to go to college are complex and lead to differences between people who chose to go to college and those who did not. When we perform regression on the variable `college`, since the groups in the sample are different, not only are we capturing the effect of going to college, but we are also capturing the effects of everything else that is different about the two groups that also affects earnings.

Imagine another variable that captures the wealth of your family. College can be very expensive, so it might be the case that the wealthier your family is, the more likely you are to go to college. Also, it's easy to imagine that the wealthier your family is, the wealthier you are likely to be in the future. This implies that the group of people in the sample who went to college will tend to be wealthier than the group that did not. Also, the group of people who went to college is expected to earn more not necessarily because they went to college, but simply because they are wealthier.

Therefore, when we do regression to measure the differences in earnings between people who went to college and those who did not, we also capture differences in earnings between people who grew up wealthier and those who did not. Because of this, we *over-estimate* the effect of going to college. $\hat{\beta}$ captures the average observed benefit of going to college *and* being wealthier, but we're only interested in college. This is called *omitted variable bias*. Family wealth is an omitted variable in this regression and it's causing our results to be biased.

Let's think of another example of omitted variable bias. In the NLSY dataset, there is a variable called `AFQT`. AFQT is a score on a particular standardized test that all people in the sample took. Let's use AFQT as a proxy measurement for the abstract idea of academic capability. While a standardized test is by no means the sole indication of someone's ability or intelligence, let's ignore that very complicated issue for now and assume that AFQT does an O.K. job at capturing this ability variable that is otherwise very difficult to measure.

Is there omitted variable bias from AFQT? Almost certainly. It seems fair to believe that people who choose to go to college are on average more academically-capable, and it also seems fair to say that on average we expect more capable people to earn more. Therefore, $\hat{\beta}$ above might be capturing the effects of being more capable, along with the effects of going to college.

How can we fix this issue? Multivariable regression.

## Multivariable Regression

So far we have only been regressing outcome variable $y$ onto one explanatory variable $x$. To find the regression line, we choose $\hat{\alpha}$ and $\hat{\beta}$ that minimize the mean squared error. But what if we believe that $y$ is actually determined by two variables, $x_1$ and $x_2$? Specifically, what if the "true" model is

$$
y = \alpha + \beta_1 x_{1} + \beta_2 x_{2} + \epsilon
$$

and we would like to estimate

$$
\hat{y} = \hat{\alpha} + \hat{\beta}_1 x_{1} + \hat{\beta}_2 x_{2}
$$

Now our challenge is choosing $\hat{\alpha}$, $\hat{\beta}_1$, *and* $\hat{\beta}_2$ that minimize the mean squared error. To this end, we will use the `minimize` function to minimize a function `to_minimize` that takes in model parameters and returns the model's RMSE.

```python
def rmse(target, pred):
    return np.sqrt(mean_squared_error(target, pred)) 

def to_minimize(intercept, beta_1, beta_2):
    predictions = intercept + beta_1 * nlsy.column('college') + beta_2 * nlsy.column("AFQT")
    actual = nlsy.column('log_earn_1999')
    return rmse(predictions, actual)

minimize(to_minimize)
```

We see that `minimize` return an $\hat{\alpha}$ of 9.956, a $\hat{\beta}_1$ of 0.430, and a $\hat{\beta}_2$ of 0.008. Let's perform the regression using `statsmodels` and see what we get.

```python
y = nlsy.select('log_earn_1999').values
X = nlsy.select('college', 'AFQT').values

model = sm.OLS(y, sm.add_constant(X)).fit()
model.summary()
```

Here $\hat{\beta}_1$ is 0.43, compared to 0.72 from the earlier biased (single variable) regression. That's huge! This implies that when we control for a person's ability (i.e. we get rid of that source of bias), we only see that on average going to college is associated with a 43% increase in earnings instead of 72%. Furthermore, looking at the 95% confidence interval for $\hat{\beta}_2$, we see that it does not contain 0, which would imply that AFQT score has a strong non-zero association with earnings.

These observations validate our claim that AFQT was probably an omitted variable causing $\hat{\beta}_1$ to be biased. When interpreting $\hat{\beta}$ coefficients, you should always be mindful of potential sources of bias that could make your coefficients misleading and not useful from an econometric context.

**Note:** A linear regression model makes predictions for $y$ which we've been calling $\hat{y}$. If you imagine that you are the model and are tasked with predicting people's earnings, you will almost certainly want more than just their years of schooling to make an accurate prediction. The more relevant variables you are given, the better predictions you are likely to make compared to just using one variable. The Variable needs to be relevant though; the day of the week a person was born on is probably not useful in predicting earnings. This is just another way of thinking about the usefulness of multivariable regression.

### Visualizing Multivariable Regression

The 3D plots below show us our variables of interest and the regression plane from two different angles.

```python
ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
ax.scatter(nlsy.column("AFQT"), 
           nlsy.column("college"), 
           nlsy.column('log_earn_1999'))
plt.xlabel("AFQT")
plt.ylabel("college")
ax.set_zlabel("log_earn_1999")
plt.title("Data Points", pad=30, size=15);
```

```python
X,Y = np.meshgrid(np.arange(0,100,1), np.arange(0,1,0.01))
Z = 0.0084 * X + 0.4301 * Y + 9.9563
ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
ax.scatter(nlsy.column("AFQT"), 
           nlsy.column("college"), 
           nlsy.column('log_earn_1999'))
ax.plot_surface(X, Y, Z, alpha=0.5)
plt.xlabel("AFQT")
plt.ylabel("college")
ax.set_zlabel("log_earn_1999")
plt.title("Data Points + Regression Plane", pad=30, size=15);
```

```python
X,Y = np.meshgrid(np.arange(0,1,0.01), np.arange(0,100,1))
Z = 0.4301 * X + 0.0084 * Y + 9.9563
ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
ax.scatter(nlsy.column("college"), 
           nlsy.column("AFQT"), 
           nlsy.column('log_earn_1999'))
ax.plot_surface(X, Y, Z, alpha=0.1)
plt.ylabel("AFQT")
plt.xlabel("college")
ax.set_zlabel("log_earn_1999")
plt.title("Data Points + Regression Plane", pad=30, size=15);
```

The regression plane, instead of the regression line, represents the values for log earnings that the regression model would predict for any given college and AFQT input. It's a plane now because there are two possible inputs, as opposed to one.

## Colinearity and Dummy Variables

When we do regression onto the variable `college`, why don't we also include a variable that measures not going to college? In other words, why don't we regress on college and the opposite of college so that way we can get an estimate of the average earnings of college-goers and non-college-goers. Why do we do this roundabout way of using the intercept term and a difference in means?

Imagine we have a dataset with a variable for college, a variable for not going to college, and the intercept term. The issue with this is that there is now redundant information.

Let's look at just one element of the sample. Let's say this person went to college, so this person's features are the following:
* College = 1
* Not College = 0
* Intercept term = 1

Clearly there is a redundancy here; you can guess one of the variables from the others. More specifically, by redundancy we mean that *one variable can be written as a linear combination of the other variables*. In fact, there are three different combinations:
* Intercept = College + Not College
* Not College = Intercept - College
* College = Intercept - Not College

These equalities aren't just true for this one person; they actually hold true for any possible person in the sample. This is because of the way we defined "college" and "not college". You can't simultaneously be in both, and so adding them together you get 1, which is just the intercept term.

In general, we have redundancy whenever we have *mutually exclusive* and *exhaustive* dummy variables in combination with an intercept term.
* Mutually exclusive: You cannot be in more than one dummy variable.
* Exhaustive: You must be in at least one dummy variable.

You can see that "college" and "not college" satisfy these conditions. So why is this redundancy an issue? It becomes ambiguous what the values for $\hat{\alpha}$, $\hat{\beta}_1$, and $\hat{\beta}_2$ should be in the model where we include all three terms:

$$
\text{log earnings} = \hat{\alpha} + \hat{\beta}_1 \text{college} + \hat{\beta}_2 \text{not college}
$$

Consider a case where we expect people who went to college to have log earnings of 10 and those who did not go to college to have log earnings of 8. What values for $\hat{\beta}$ and $\hat{\alpha}$ make sense?

* $\hat{\beta}_1 = 10$
* $\hat{\beta}_2 = 8$
* $\hat{\alpha} = 0$

make sense. These are valid values for $\hat{\beta}$ and $\hat{\alpha}$ that satisfy the condition above. To see why, consider a person with college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 1 + \hat{\beta}_2 \cdot 0 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \\
\text{log earnings} &= 0 + 10 = 10
\end{aligned}$$

and a person without college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 0 + \hat{\beta}_2 \cdot 1 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_2 \\
\text{log earnings} &= 0 + 8 = 8
\end{aligned}$$

* $\hat{\beta}_1 = 2$
* $\hat{\beta}_2 = 0$
* $\hat{\alpha} = 8$

also make sense. To see why, consider a person with college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 1 + \hat{\beta}_2 \cdot 0 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \\
\text{log earnings} &= 8 + 2 = 10
\end{aligned}$$

and a person without college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 0 + \hat{\beta}_2 \cdot 1 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_2 \\
\text{log earnings} &= 8 + 0 = 8
\end{aligned}$$

It turns out, there are actually infinitely many solutions for $\hat{\beta}$ that satisfy the condition where people who went to college have mean log earnings of 10 and people who did not go to college have mean log earnings of 8. This holds true for all situations where you regress on a constant and a set of mutually exclusive and exhaustive dummies. There is no unique solution for $\hat{\beta}$, which is a problem for econometricians who want unique and interpretable coefficients.

In fact, there is mathematical justification for this as well. At some point in the math involved in performing regression, having redundant variables causes a division by 0. This is particularly upsetting for your computer, and it will complain.

So how do we avoid this problem? We deliberately exclude one of the variables. It can technically either be one of the dummy variables or the intercept term, but we usually really want to have an intercept term present in our regression for other reasons. So we usually get rid of one of the dummy variables. Notice that we implicitly did this earlier. We did not include "not college" in our first regression.



--- END 11-econometrics/.ipynb_checkpoints/multivariable-checkpoint.md ---



--- START 11-econometrics/.ipynb_checkpoints/reading-econ-papers-checkpoint.md ---

---
title: reading-econ-papers-checkpoint
type: textbook
source_path: content/11-econometrics/.ipynb_checkpoints/reading-econ-papers-checkpoint.ipynb
chapter: 11
---

# Reading Economics Papers

In upper division economics courses, you'll often read economics papers that utilize ordinary least squares to conduct regression. Now that we have familiarized ourselves with multi-variate regression, let's familiarize ourselves with reading the results of economics papers!

Let's consider an existing empirical study conducted by David Card {cite}`11reading-econ-papers`, a Nobel Prize winning professor at UC Berkeley, that regresses income on education:

![](https://i.imgur.com/FPLII4s.png)

Every column here is from a different regression: the first column predicts the log hourly earnings from years of education, the fifth column predicts the log annual earnings from years of education, and so on. For now, let's focus on the first column, which states the linear regression as follows: 

$$
\ln{(\text{hourly earnings})_i} = \alpha + \beta \cdot (\text{years of schooling})_i + \varepsilon_i
$$

From the table, the education coefficient is 0.100, with a (0.001) underneath it. This means that our $\beta$ value is equal to 0.100. What does the (0.001) mean? It is the standard error, which is essentially a measure of our uncertainty. From Data 8, the standard error is most similar to the standard deviation of sample means, which is a measure of the spread in the population mean. Similarly, the the standard error here is a measure of the spread in the population coefficient. We can use the standard error to construct a confidence interval of the actual coefficient: a 95% confidence interval is between 2 standard errors above and below the reported value.

The effects of schooling on income is captured by the education coefficient term: 0.100. This means that an increase in 1 unit (year) of education is correlated with a log hourly earnings by 0.1. This approximately corresponds to a 10% increase in wages per year of schooling.



--- END 11-econometrics/.ipynb_checkpoints/reading-econ-papers-checkpoint.md ---



--- START 11-econometrics/.ipynb_checkpoints/single-variable-checkpoint.md ---

---
title: single-variable-checkpoint
type: textbook
source_path: content/11-econometrics/.ipynb_checkpoints/single-variable-checkpoint.ipynb
chapter: 11
---

# Single Variable Regression

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import warnings
warnings.simplefilter("ignore")
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import patches
from datascience import *
%matplotlib inline
from sklearn.metrics import mean_squared_error
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets
```

Suppose we have some data that look like this:

![title](figure1.png)

In Data 8 we learned that $x$ and $y$ above have some **correlation coefficient** $r$, which is a measure of the strength of the linear relationship between the two variables.

It looks like there is some positive linear association between $x$ and $y$ such that larger values of $x$ correspond to larger values of $y$. We therefore expect $r$ to be some positive number between 0 and 1, but not exactly 0 or exactly 1.

First, let's convert the data (stored in the arrays `x` and `y`) to standard units. To convert a set of data points to standard units, we subtract out the mean of the data and scale by the standard deviation. This has the effect of changing the data so that the data in standard units have mean 0 and  standard deviation 1. Below we construct a function that does this.

$$
x_{su} = \dfrac{x - \mu_x}{\sigma_x}
$$

```python
def standard_units(array):
    return (array - np.mean(array)) / np.std(array)
```

```python
# Hide this.
np.random.seed(42)
x = np.random.uniform(0, 10, 100)
noise = np.random.randn(100) * 4
y = 1.5 * x + noise
```

```python
x_standard = standard_units(x)
y_standard = standard_units(y)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(x_standard, y_standard)
plt.xlabel('$x$_standard')
plt.ylabel('$y$_standard');
```

The plot looks the same as before, except now the axes are scaled such that we measure $x$ and $y$ in standard units. Now recall that $r$ is calculated as the average of the product of two variables, when the variables are measured in standard units. Below we define a function that calculates $r$, assuming that the inputs have already been converted to standard units.

```python
def correlation(array1, array2):
    return np.mean(array1 * array2)
```

What is the correlation between these two variables?

```python
correlation(x_standard, y_standard)
```

Recall from Data 8 that we use $r$ to form a line called the *regression line*, which makes predictions for $y$ given some $x$. Our prediction for $y$ in standard units is $r \cdot x$. If we want to fit this regression line in the original units, recall that the slope of this line is given by

$$
\text{slope} = r \cdot \dfrac{\hat{\sigma}_y}{\hat{\sigma}_x}
$$

and the intercept is given by

$$
\text{intercept} = \hat{\mu}_y - \text{slope} \cdot \hat{\mu}_x
$$

where $\hat{\sigma}_x$ is the observed standard deviation of a variable $x$ and $\hat{\mu}_x$ is the observed mean. Our regression line will have the form

$$
y = \hat{\alpha} + \hat{\beta} x
$$

where $\hat{\alpha}$ is the intercept from above and $\hat{\beta}$ the slope.

Below we plot this line.

```python
r = correlation(x_standard, y_standard)
slope = r * np.std(y) / np.std(x)
intercept = np.mean(y) - slope * np.mean(x)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(x, y)
plt.plot(np.linspace(0, 10), slope * np.linspace(0, 10) + intercept, color='tab:orange')
plt.xlabel('$x$')
plt.ylabel('$y$');
```

Let's take a closer look at the slope we found.

```python
print('Slope: ', slope)
```

To generate the data above, we started with some range of $x$ values, and generated $y$ as a linear function of $x$ with some random noise added in. Take a look:

```python
np.random.seed(42)
x = np.random.uniform(0, 10, 100)
noise = np.random.randn(100) * 4
y = 1.5 * x + noise
```

Notice how I defined $y$:

$$
y = 1.5 \cdot x + u
$$

where $u$ is some normally-distributed random noise whose average is 0. So, while there is some randomness to the data, on average the "true" slope of the relationship is 1.5. Yet we predicted it to be roughly 1.3!

This highlights the following fact: Suppose we have some random data that we believe has a linear relationship. The least-squares slope we generate from the data is an *estimate* of the "true" slope of that data. Because of this, the estimated slope is a random variable that depends on the data we happen to have.

To highlight this fact, let's repeat the procedure above but with a different [random seed](https://en.wikipedia.org/wiki/Random_seed), in order to get data with the same underlying relationship but different values.

```python
np.random.seed(189)
x = np.random.uniform(0, 10, 100)
noise = np.random.randn(100) * 4
y = 1.5 * x + noise

r = correlation(x_standard, y_standard)
slope = r * np.std(y) / np.std(x)
intercept = np.mean(y) - slope * np.mean(x)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(x, y)
plt.plot(np.linspace(0, 10), slope * np.linspace(0, 10) + intercept, color='tab:orange')
plt.xlabel('$x$')
plt.ylabel('$y$');
print('Slope: ', slope)
```

Now the estimated slope is roughly 1.6, even though the underlying data was still generated using a slope of 1.5. This is a very important concept that we will revisit soon.

Keep in mind, however, that correlation in data *does not* imply causation. In this example we know the true causal relationship between $x$ and $y$ because we defined it ourselves. However, when using real data you do not see the "true" relation and thus cannot conclude causality from correlation. It could simply be that both your variables depend on an unseen third variable and have no causal effect on one another. Or even worse, while unlikely it could be the case that slight linear trends in two variables is a complete coincidence.

## Root-Mean-Squared Error

While we can arbitrarily pick $\hat{\alpha}$ and $\hat{\beta}$ values, we do want to pick the values that help predict $\hat{y}$ that are closest to actual $y$ values. To achieve this, we want to minimize a **loss function** that quantifies how far off our prediction $\hat{y}$ is from $y$ for some known data points. One of the most common loss functions is called the **root-mean-squared error**, and is defined as

$$
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left ( y_i - \hat{y}_i \right ) ^2 }
$$

where $n$ is the number of observations. The effect of this is to take the mean of the distance of each value of $\hat{y}$ from its corresponding value in $y$; squaring these values keeps them positive, and then we take the square root to correct the units of the error.

Plugging in the formula $\hat{y}$ in RMSE formula, we get,

$$
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left ( y_i - (\hat{\alpha} + \hat{\beta}x_i) \right ) ^2 }
$$

By doing a bit of calculus, we get the following formulas for $\hat{\alpha}$ and $\hat{\beta}$

$$\Large
\hat{\beta} = r\frac {\hat{\sigma}_y} {\hat{\sigma}_x} \qquad \qquad
\hat{\alpha} = \hat{\mu}_y - \hat{\beta}\hat{\mu}_x
$$

where $r$ is the **correlation** between $x$ and $y$, $\hat{\sigma}_y$ is the standard deviation of $y$, $\hat{\sigma}_x$ is the standard deviation of $x$, $\hat{\mu}_y$ is the average of all our $y$ values, and $\hat{\mu}_x$ is the average of all our $x$ values. (As an aside, note the hats on our $\sigma$'s and $\mu$'s; this is because these are _empirical estimates_ of the parameters of these distributions, rather than the true values.) These are the same values we had above!

Note that our formula for $\hat{\beta}$ involves the **correlation coefficient** $r$ of $x$ and $y$. The correlation coefficient of two variables is a measure of the strength of a linear relationship between them. $r$ goes from -1 to 1, where $|r|=1$ is a perfect linear relationship and $r=0$ is no linear relationship. The formula for $r$ is

$$
r = \frac{1}{n}\sum^n_{i=1} \left ( \frac{x_i - \hat{\mu}_x}{\hat{\sigma}_x} \right ) \left ( \frac{y_i - \hat{\mu}_y}{\hat{\sigma}_y} \right )
$$

(Note: the form $\frac{x_i - \hat{\mu}_x}{\hat{\sigma}_x}$ of a variable $x$ is it's representation in standard units, as mentioned above.)

To calculate the RMSE, we will write an `rmse` function that makes use of sklearn's `mean_squared_error` function.

```python
def rmse(target, pred):
    return np.sqrt(mean_squared_error(target, pred))
```

To get a better idea of what the RMSE represents, the figures below show a small dataset, a proposed regression line, and the squared error that we are summing in the RMSE. The data points are

| $x$ | $y$ |
|-----|-----|
| 0 | 1 |
| 1 | .5 |
| 2 | -1 |
| 3 | 2 |
| 4 | -3 |

Here are the proposed regression lines and their errors:

![se_slope_1.png](se_slope_1.png)

![se_slope_-1.png](se_slope_-1.png)

```python
d = Table().with_columns(
    'x', make_array(0,  1,  2,  3,  4),
    'y', make_array(1, .5, -1,  2, -3))

def plot_line_and_errors(slope, intercept):
    print("RMSE:", rmse(slope * d.column('x') + intercept, d.column('y')))
    plt.figure(figsize=(5,5))
    points = make_array(-2, 7)
    p = plt.plot(points, slope*points + intercept, color='orange', label='Proposed line')
    ax = p[0].axes
    
    predicted_ys = slope*d.column('x') + intercept
    diffs = predicted_ys - d.column('y')
    for i in np.arange(d.num_rows):
        x = d.column('x').item(i)
        y = d.column('y').item(i)
        diff = diffs.item(i)
        
        if diff > 0:
            bottom_left_x = x
            bottom_left_y = y
        else:
            bottom_left_x = x + diff
            bottom_left_y = y + diff
        
        ax.add_patch(patches.Rectangle(make_array(bottom_left_x, bottom_left_y), abs(diff), abs(diff), color='red', alpha=.3, label=('Squared error' if i == 0 else None)))
        plt.plot(make_array(x, x), make_array(y, y + diff), color='red', alpha=.6, label=('Error' if i == 0 else None))
    
    plt.scatter(d.column('x'), d.column('y'), color='blue', label='Points')
    
    plt.xlim(-4, 8)
    plt.ylim(-6, 6)
    plt.gca().set_aspect('equal', adjustable='box')
    
    plt.legend(bbox_to_anchor=(1.8, .8))
    plt.show()

interact(plot_line_and_errors, slope=widgets.FloatSlider(min=-4, max=4, step=.1), intercept=widgets.FloatSlider(min=-4, max=4, step=.1));
```

## Econometric Single Variable Regression

The regression line can have two purposes:

* Of particular interest to data scientists is the line's ability to predict values of $y$ for new values of $x$ that we didn't see before.

* Of particular interest to economists is the line's ability to estimate the "true" underlying slope of the data via its slope.

This is why regression is such a powerful tool and forms the backbone of econometrics. If we believe that our data satisfy certain assumptions (which we won't explore too much this lecture), then we can use the slope of the regression line to estimate the "true" relation between the variables in question and learn more about the world we live in.

In econometrics, we usually write the "true" underlying linear relationship as follows:

$$
y = \alpha + \beta \cdot x + \varepsilon
$$

where $y$ and $x$ are values for any arbitrary point, $\alpha$ is the intercept, $\beta$ is the slope, and $\varepsilon$ is some noise. This is entirely analogous to the code from earlier that determined the true linear relationship between the data:

```python
y = 1.5 * x + noise
```

Here, $\beta = 1.5$, $\alpha = 0$, and $\varepsilon = \text{noise}$.

When we fit a regression line onto the data, we express the line as:

$$
\hat{y} = \hat{\alpha} + \hat{\beta} \cdot x
$$

Here, we put hats over the slope and intercept terms because they are *estimates* of the true slope and intercept terms. Similarly, we put a hat over $y$ because this is the $y$ value that the regression line predicts.

Notice how the noise term $\varepsilon$ does not appear in the expression for the regression line. This is because the noise term is a random variable that has no relation with $x$, and is thus impossible to predict from the data. Furthermore, the noise term has a mean value of 0, so on average we actually don't expect the noise term to have any impact on the underlying trends of the data.

For the Data 8 demonstration above, we forced these conditions to be true. However, with real data these are assumptions that we have to make, and is something that econometricians spend a lot of time thinking about.

### Years of Schooling and Earnings

Consider a case where we want to study how years of schooling relate to a person's earnings. This should be of particular interest to college students. Below we import a dataset that has the hourly wage, years of schooling, and other information on thousands of people sampled in the March 2012 Current Population Survey.

```python
cps = Table.read_table('cps.csv')
cps
```

We want to consider a person's wage and years of schooling. But first, we will convert wage to log-wage. Wage is a variable that we would expect to increase proportionally (or, exponentially) with changes in years of schooling. And as such, we usually take the natural log of wage instead. Below we plot log wage and years of schooling for the CPS data.

```python
educ = cps.column('educ')
logwage = cps.column('logwage')

plt.figure(figsize=(8,6))
plt.scatter(educ, logwage)
plt.xlabel('Years of Education')
plt.ylabel('Log Wage')
plt.title('Log Wage vs. Years of Education');
```

Now let's fit a least-squares regression line onto this data. First, we'll do it manually in the Data 8 style above.

```python
educ_standard = standard_units(educ)
logwage_standard = standard_units(logwage)

r = correlation(logwage_standard, educ_standard)
slope = r * np.std(logwage) / np.std(educ)
intercept = np.mean(logwage) - slope * np.mean(educ)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(educ, logwage)
plt.plot(np.linspace(0, 20), slope * np.linspace(0, 20) + intercept, color='tab:orange')
plt.xlabel('Years of Education')
plt.ylabel('Log Wage')
plt.title('Log Wage vs. Years of Education');
print('Slope: ', slope)
print('Intercept: ', intercept)
```

So from the very simple and straight-forward model above, it seems that we estimate a slope of roughly 0.1, meaning we might expect that a one-year increase in schooling is associated with a 10% increase in wage, on average.

We can also see that we have a non-zero intercept term. We should be careful how we interpret this term; from a strictly mathematical point of view, the intercept represents the expected value of $y$ (in this case log wage) when $x = 0$. However, in economics sometimes it makes no sense for $x$ to be 0, and so we cannot use the above interpretation. We won't go into detail this lecture, but regardless of whether the intercept is interpretable, we almost always want to include it.

## Uncertainty in $\hat{\beta}$

We mentioned earlier that the slope we estimate from regression is exactly that: an estimate of the "true" underlying slope. Because of this, the estimate $\hat{\beta}$ is a random variable that depends on the underlying data.

Let's assume there is the following true linear relation between log wage and years of schooling,

$$
\text{logwage} = \alpha + \beta \cdot \text{years of schooling} + \varepsilon
$$

and we try to estimate $\alpha$ and $\beta$.

If our data are "well-behaved", then even though there is uncertainty in our estimate $\hat{\beta}$, on average $\hat{\beta}$ will be $\beta$; that is to say that the expectation of $\hat{\beta}$ is $\beta$. Additionally, if our data are "well-behaved", then $\hat{\beta}$ has some normal distribution with mean $\beta$. We won't worry too much about what assumptions need to be satisfied to make the data "well-behaved".

You can think of each person as an observation of these variables, and using a sample of people we can estimate the relationship between the two variables. However, due to the noise term and the fact that we only have a finite sample of people, the true relationship is always hidden from us, and we can only hope to get better estimates by designing better experiments and sampling more people.

Let's try to get an idea of how "certain" we can be of our estimate $\hat{\beta}$. We'll do this in classic Data 8 style: bootstrapping. Using our existing sample data, we'll create new samples by bootstrapping from the existing data. Then, for each sample, we'll fit a line, and keep the slope of that line in a list with all of the other slopes. Then, we'll find the standard deviation of that list of slopes.

```python
slopes = make_array()
educ_logwage = cps.select("educ", "logwage")

np.random.seed(42)
for i in np.arange(200):
    educ_logwage_sample = educ_logwage.sample()
    y = educ_logwage_sample.column("logwage")
    X = educ_logwage_sample.column("educ")
    model = sm.OLS(y, sm.add_constant(X)).fit()
    slopes  = np.append(model.params[1], slopes)
    
Table().with_columns("Slopes", slopes).hist()    
print('Standard dev. of bootstrapped slopes: ', np.std(slopes))
```

Our bootstrapped approximation standard error of 0.00159 is pretty close to the true standard error of 0.00144. `statsmodels`, the package we will be using to perform regressions, actually uses a precise mathematical formula for finding the standard error whereas we tried to find this value through simulation, but the idea behind the standard error is the same.

Armed with a standard error, we can now form a 95% confidence interval and perform a test of significance to see if $\hat{\beta}$ is significantly different from 0.

```python
# Using our resampled slopes
lower_bound = percentile(2.5, slopes)
upper_bound = percentile(97.5, slopes)
print('95% confidence interval: [{}, {}]'.format(lower_bound, upper_bound))
```

The 95% confidence interval does not contain 0, and so $\beta$ is unlikely to be 0.

## Regression with a Binary Variable

A binary variable is a variable that takes on the value of 1 if some condition is true, and 0 otherwise. These are also called dummy variables or indicator variables. It might sound strange at first, but you can actually perform regression of a variable like log earnings onto a binary variable.

Let's import a different dataset that has the following features. Some will be useful to us later.

```python
nlsy = Table.read_table('nlsy_cleaned_small.csv')
nlsy
```

Now let's visualize log earnings vs. the binary variable corresponding to whether or not an observation went to college.

```python
coll = nlsy.column('college')
logearn = nlsy.column('log_earn_1999')

plt.figure(figsize=(8,6))
plt.scatter(coll, logearn)
plt.xlabel('College')
plt.ylabel('Log Earnings')
plt.title('Log Earnings vs. College Completion')
plt.xticks([0,1]);
```

```python
no_college = nlsy.where('college', 0).column("log_earn_1999")
has_college = nlsy.where('college', 1).column("log_earn_1999")

plt.figure(figsize=(8,6))
plt.xlabel('College')
plt.ylabel('Log Earnings')
plt.title('Log Earnings vs. College Completion')
plt.violinplot(no_college, positions = [0], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False)
plt.violinplot(has_college, positions = [1], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False);
```

Now let's fit a regression model:

```python
coll_standard = standard_units(coll)
logearn_standard = standard_units(logearn)

r = correlation(logearn_standard, coll_standard)
slope = r * np.std(logearn) / np.std(coll)
intercept = np.mean(logearn) - slope * np.mean(coll)

print("y = {:.5f} * x + {:.5f}".format(slope, intercept))
```

Wow! This regression would imply that we expect, on average, observations who went to college to have 70% higher earnings than those who did not go to college. Let's now plot this line on the data:

```python
plt.figure(figsize=(8,6))
plt.scatter(coll, logearn)
plt.plot(np.linspace(0, 1), slope * np.linspace(0, 1) + intercept, color='tab:orange')
plt.xlabel('College')
plt.ylabel('Log Earnings')
plt.title('Log Earnings vs. College Completion')
plt.xticks([0,1]);
```

```python
no_college = nlsy.where('college', 0).column("log_earn_1999")
has_college = nlsy.where('college', 1).column("log_earn_1999")

plt.figure(figsize=(8,6))
plt.plot(np.linspace(0, 1), slope * np.linspace(0, 1) + intercept, color='tab:green')
plt.xlabel('College')
plt.ylabel('Log Earnings')
plt.title('Log Earnings vs. College Completion')
plt.violinplot(no_college, positions = [0], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False)
plt.violinplot(has_college, positions = [1], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False);
```

When we perform a simple regression onto just a dummy variable, it is an important fact that $\hat{\alpha}$ is the mean value of $y$ for all observations in the sample where $x = 0$, and $\hat{\beta}$ is the difference between the mean value of $y$ for observations in the sample where $x = 1$ and observations where $x = 0$. Proving this claim is beyond our scope this week, but let's verify it with our data:

```python
avg_logearn_coll = np.mean(logearn[coll == 1])
avg_logearn_nocoll = np.mean(logearn[coll == 0])

print('Avg logearn for coll = 1: ', avg_logearn_coll)
print('Avg logearn for coll = 0: ', avg_logearn_nocoll)
print('Difference between the two: ', avg_logearn_coll - avg_logearn_nocoll)
```

```python
print('Intercept: ', intercept)
print('Slope: ', slope)
```



--- END 11-econometrics/.ipynb_checkpoints/single-variable-checkpoint.md ---



--- START 11-econometrics/.ipynb_checkpoints/statsmodels-checkpoint.md ---

---
title: statsmodels-checkpoint
type: textbook
source_path: content/11-econometrics/.ipynb_checkpoints/statsmodels-checkpoint.ipynb
chapter: 11
---

# Using `statsmodels` for Regression

```python
from datascience import *
import numpy as np
import statsmodels.api as sm

cps = Table.read_table('cps.csv')
```

In the previous section, we used functions in NumPy and concepts taught in Data 8 to perform single variable regressions. It turns out that there are (several) Python packages that can perform these regressions for us and which extend nicely into the types of regressions we will cover in the next few sections. In this section, we introduce `statsmodels` for performing single variable regressions, a foundation upon which we will build our discussion of multivariable regression.

`statsmodels` is a popular Python package used to create and analyze various statistical models. To create a linear regression model in `statsmodels`, which is generally import as `sm`, we use the following skeleton code:

```python
x = data.select(features).values            # Separate features (independent variables) 
y = data.select(target).values              # Separate target (outcome variable)
model = sm.OLS(y, sm.add_constant(x))       # Initialize the OLS regression model
result = model.fit()                        # Fit the regression model and save it to a variable
result.summary()                            # Display a summary of results
```

*You must manually add a constant column of all 1's to your independent features. `statsmodels` will not do this for you and if you fail to do this you will perform a regression without an intercept $\alpha$ term. This is performed in the third line by calling `sm.add_constant` on `x`.* Also note that we call `.values` after we select the columns in `x` and `y`; this gives us `NumPy` arrays containing the corresponding values, since `statsmodels` can't process `Table`s.

Recall the `cps` dataset we used in the previous section:

```python
cps
```

Let's use `statsmodels` to perform our regression of `logwage` on `educ` again.

```python
x = cps.select("educ").values
y = cps.select("logwage").values

model = sm.OLS(y, sm.add_constant(x))
results = model.fit()
results.summary()
```

The summary above provides us with a lot of information. Let's start with the most important pieces: the values of $\hat{\alpha}$ and $\hat{\beta}$. The middle table contains these values for us as `const` and `x1`'s `coef` values: we have $\hat{\alpha}$ is 1.4723 and $\hat{\beta}$ is 0.1078.

Recall also our discussion of uncertainty in $\hat{\beta}$. `statsmodels` provides us with our calculated standard error in the `std err` column, and we see that the standard error of $\hat{\beta}$ is 0.001, matching our empirical estimate via bootstrapping from the last section. We can also see the 95% confidence interval that we calculated in the two rightmost columns.

![](statsmodels-coeffs.png)

Earlier we said that $\hat{\beta}$ has some normal distribution with mean $\beta$ if certain assumptions are satisfied. We now can see that the standard deviation of that normal distribution is the standard error of $\hat{\beta}$. We can also use this to test a null hypothesis that $\beta = 0$. To do so, construct a [t-statistic](https://en.wikipedia.org/wiki/T-statistic) (which `statsmodels` does for you) that indicates how many standard deviations away $\hat{\beta}$ is from 0, assuming that the distribution of $\hat{\beta}$ is in fact centered at 0.

We can see that $\hat{\beta}$ is 74 standard deviations away from the null hypothesis mean of 0, which is an enormous number. How likely do you think it is to draw a random number roughly 74 standard deviations away from the mean, assuming a standard normal distribution? Essentially 0. This is strong evidence that the mean of the distribution (the mean of $\hat{\beta}$ is the true value $\beta$) is not 0. Accompanying the t-statistic is a p-value that indicates the statistical significance.



--- END 11-econometrics/.ipynb_checkpoints/statsmodels-checkpoint.md ---



--- START 11-econometrics/index.md ---

---
title: index
type: textbook
source_path: content/11-econometrics/index.md
chapter: 11
---

# Econometrics

One way to think about Econometrics is that it is the closest economists can usually get to doing experiments in the same way experiments are done in medicine and other fields involving people. The idealized experiment is one where people are randomly assigned to either the treatment or control group such that the participants *and* the researchers don't know which group each person is in.

By randomly assigning a large sample of participants to two different groups, we can be fairly confident that on average the two groups are identical in their attributes, apart from the treatment that only one group receives. By having a double-blind experiment, we can be confident that the participants and researchers aren't conciously or subconciously (placebo effect) affecting the results of the treatment group. Because of this setup, we can look at the difference in outcomes of the two groups and be fairly confident that any differences are due to the treatment and nothing else.

But it's usually impossible or unethical to perform an ideal experiment in economics. Imagine trying to examine the effect of years of schooling on future earnings. Can you force people into a treatment group with more years of schooling and a control group with less? Is it sufficient to just collect a sample of people and compare the earnings of people with high schooling to those with low schooling?

How can we answer the above question and others like it? Econometrics. Econometrics is a vast field and today we will just focus on the basics of something called regression.


--- END 11-econometrics/index.md ---



--- START 11-econometrics/multivariable.md ---

---
title: multivariable
type: textbook
source_path: content/11-econometrics/multivariable.ipynb
chapter: 11
---

# Multivariable Regression and Bias

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import warnings
warnings.simplefilter("ignore")
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import patches
from datascience import *
%matplotlib inline
from sklearn.metrics import mean_squared_error
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets

nlsy = Table.read_table('nlsy_cleaned_small.csv')
nlsy = nlsy.with_column("college", nlsy.column("college").astype(float))
nlsy = nlsy.with_column("AFQT", nlsy.column("AFQT").astype(float))
```

Our procedure earlier showed that we expect to see roughly a 70% increase in earnings in people who went to college vs. people who did not go to college. Does this imply that your decision to go to college was worthwhile, and now you can expect to have roughly 70% higher earnings compared to the version of you who did not go to college?

Let's go back to our discussion of experiments from earlier. In an ideal experiment, we would want a good sample of people who are about to graduate high school, and then randomly assign them to either a treatment group that gets sent to college, and a control group that does not. If you are in the treatment group you *must* go to college, and if you are in the control group you *cannot* go to college. Since the decision to go to college in this case is completely random, we can safely assume that the treatment and control groups are on average identical in attributes, apart from college attendance. We can therefore compare their log earnings in the future to see the effect of going to college.

Clearly this experiment is impossible to perform. We cannot randomly assign people to go to college. What's different between this ideal experiment and our regression from earlier? What's the issue with comparing the differences in log earnings for people in our sample who happened to go to college and those who did not?

In our sample, the treatment (went to college) and control (did not go to college) groups are not identical in every way except for college! People aren't randomly assigned college, they *choose* to go to college. The factors that cause someone to go to college are complex and lead to differences between people who chose to go to college and those who did not. When we perform regression on the variable `college`, since the groups in the sample are different, not only are we capturing the effect of going to college, but we are also capturing the effects of everything else that is different about the two groups that also affects earnings.

Imagine another variable that captures the wealth of your family. College can be very expensive, so it might be the case that the wealthier your family is, the more likely you are to go to college. Also, it's easy to imagine that the wealthier your family is, the wealthier you are likely to be in the future. This implies that the group of people in the sample who went to college will tend to be wealthier than the group that did not. Also, the group of people who went to college is expected to earn more not necessarily because they went to college, but simply because they are wealthier.

Therefore, when we do regression to measure the differences in earnings between people who went to college and those who did not, we also capture differences in earnings between people who grew up wealthier and those who did not. Because of this, we *over-estimate* the effect of going to college. $\hat{\beta}$ captures the average observed benefit of going to college *and* being wealthier, but we're only interested in college. This is called *omitted variable bias*. Family wealth is an omitted variable in this regression and it's causing our results to be biased.

Let's think of another example of omitted variable bias. In the NLSY dataset, there is a variable called `AFQT`. AFQT is a score on a particular standardized test that all people in the sample took. Let's use AFQT as a proxy measurement for the abstract idea of academic capability. While a standardized test is by no means the sole indication of someone's ability or intelligence, let's ignore that very complicated issue for now and assume that AFQT does an O.K. job at capturing this ability variable that is otherwise very difficult to measure.

Is there omitted variable bias from AFQT? Almost certainly. It seems fair to believe that people who choose to go to college are on average more academically-capable, and it also seems fair to say that on average we expect more capable people to earn more. Therefore, $\hat{\beta}$ above might be capturing the effects of being more capable, along with the effects of going to college.

How can we fix this issue? Multivariable regression.

## Multivariable Regression

So far we have only been regressing outcome variable $y$ onto one explanatory variable $x$. To find the regression line, we choose $\hat{\alpha}$ and $\hat{\beta}$ that minimize the mean squared error. But what if we believe that $y$ is actually determined by two variables, $x_1$ and $x_2$? Specifically, what if the "true" model is

$$
y = \alpha + \beta_1 x_{1} + \beta_2 x_{2} + \epsilon
$$

and we would like to estimate

$$
\hat{y} = \hat{\alpha} + \hat{\beta}_1 x_{1} + \hat{\beta}_2 x_{2}
$$

Now our challenge is choosing $\hat{\alpha}$, $\hat{\beta}_1$, *and* $\hat{\beta}_2$ that minimize the mean squared error. To this end, we will use the `minimize` function to minimize a function `to_minimize` that takes in model parameters and returns the model's RMSE.

```python
def rmse(target, pred):
    return np.sqrt(mean_squared_error(target, pred)) 

def to_minimize(intercept, beta_1, beta_2):
    predictions = intercept + beta_1 * nlsy.column('college') + beta_2 * nlsy.column("AFQT")
    actual = nlsy.column('log_earn_1999')
    return rmse(predictions, actual)

minimize(to_minimize)
```

We see that `minimize` return an $\hat{\alpha}$ of 9.956, a $\hat{\beta}_1$ of 0.430, and a $\hat{\beta}_2$ of 0.008. Let's perform the regression using `statsmodels` and see what we get.

```python
y = nlsy.select('log_earn_1999').values
X = nlsy.select('college', 'AFQT').values

model = sm.OLS(y, sm.add_constant(X)).fit()
model.summary()
```

Here $\hat{\beta}_1$ is 0.43, compared to 0.72 from the earlier biased (single variable) regression. That's huge! This implies that when we control for a person's ability (i.e. we get rid of that source of bias), we only see that on average going to college is associated with a 43% increase in earnings instead of 72%. Furthermore, looking at the 95% confidence interval for $\hat{\beta}_2$, we see that it does not contain 0, which would imply that AFQT score has a strong non-zero association with earnings.

These observations validate our claim that AFQT was probably an omitted variable causing $\hat{\beta}_1$ to be biased. When interpreting $\hat{\beta}$ coefficients, you should always be mindful of potential sources of bias that could make your coefficients misleading and not useful from an econometric context.

**Note:** A linear regression model makes predictions for $y$ which we've been calling $\hat{y}$. If you imagine that you are the model and are tasked with predicting people's earnings, you will almost certainly want more than just their years of schooling to make an accurate prediction. The more relevant variables you are given, the better predictions you are likely to make compared to just using one variable. The Variable needs to be relevant though; the day of the week a person was born on is probably not useful in predicting earnings. This is just another way of thinking about the usefulness of multivariable regression.

### Visualizing Multivariable Regression

The 3D plots below show us our variables of interest and the regression plane from two different angles.

```python
ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
ax.scatter(nlsy.column("AFQT"), 
           nlsy.column("college"), 
           nlsy.column('log_earn_1999'))
plt.xlabel("AFQT", fontsize=14)
plt.ylabel("college", fontsize=14)
ax.set_zlabel("log_earn_1999", fontsize=14)
plt.title("Data Points", pad=30, size=20);
```

Scatter plot in 3D showing data points related to AFQT, college, and log_earn_1999.

```python
X,Y = np.meshgrid(np.arange(0,100,1), np.arange(0,1,0.01))
Z = 0.0084 * X + 0.4301 * Y + 9.9563
ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
ax.scatter(nlsy.column("AFQT"), 
           nlsy.column("college"), 
           nlsy.column('log_earn_1999'))
ax.plot_surface(X, Y, Z, alpha=0.5)
plt.xlabel("AFQT", fontsize=14)
plt.ylabel("college", fontsize=14)
ax.set_zlabel("log_earn_1999", fontsize=14)
plt.title("Data Points + Regression Plane", pad=30, size=20);
```

Scatter plot in 3D with a visible regression plane. Data points are related to AFQT, college, and log_earn_1999. Regression plane represented by a surface.

```python
X,Y = np.meshgrid(np.arange(0,1,0.01), np.arange(0,100,1))
Z = 0.4301 * X + 0.0084 * Y + 9.9563
ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
ax.scatter(nlsy.column("college"), 
           nlsy.column("AFQT"), 
           nlsy.column('log_earn_1999'))
ax.plot_surface(X, Y, Z, alpha=0.1)
plt.ylabel("AFQT", fontsize=14)
plt.xlabel("college", fontsize=14)
ax.set_zlabel("log_earn_1999", fontsize=14)
plt.title("Data Points + Regression Plane", pad=30, size=20);
```

Scatter plot in 3D with a rotated view of data points related to college, AFQT, and log_earn_1999. Regression plane represented by a semi-transparent surface.

The regression plane, instead of the regression line, represents the values for log earnings that the regression model would predict for any given college and AFQT input. It's a plane now because there are two possible inputs, as opposed to one.

## Colinearity and Dummy Variables

When we do regression onto the variable `college`, why don't we also include a variable that measures not going to college? In other words, why don't we regress on college and the opposite of college so that way we can get an estimate of the average earnings of college-goers and non-college-goers. Why do we do this roundabout way of using the intercept term and a difference in means?

Imagine we have a dataset with a variable for college, a variable for not going to college, and the intercept term. The issue with this is that there is now redundant information.

Let's look at just one element of the sample. Let's say this person went to college, so this person's features are the following:
* College = 1
* Not College = 0
* Intercept term = 1

Clearly there is a redundancy here; you can guess one of the variables from the others. More specifically, by redundancy we mean that *one variable can be written as a linear combination of the other variables*. In fact, there are three different combinations:
* Intercept = College + Not College
* Not College = Intercept - College
* College = Intercept - Not College

These equalities aren't just true for this one person; they actually hold true for any possible person in the sample. This is because of the way we defined "college" and "not college". You can't simultaneously be in both, and so adding them together you get 1, which is just the intercept term.

In general, we have redundancy whenever we have *mutually exclusive* and *exhaustive* dummy variables in combination with an intercept term.
* Mutually exclusive: You cannot be in more than one dummy variable.
* Exhaustive: You must be in at least one dummy variable.

You can see that "college" and "not college" satisfy these conditions. So why is this redundancy an issue? It becomes ambiguous what the values for $\hat{\alpha}$, $\hat{\beta}_1$, and $\hat{\beta}_2$ should be in the model where we include all three terms:

$$
\text{log earnings} = \hat{\alpha} + \hat{\beta}_1 \text{college} + \hat{\beta}_2 \text{not college}
$$

Consider a case where we expect people who went to college to have log earnings of 10 and those who did not go to college to have log earnings of 8. What values for $\hat{\beta}$ and $\hat{\alpha}$ make sense?

* $\hat{\beta}_1 = 10$
* $\hat{\beta}_2 = 8$
* $\hat{\alpha} = 0$

make sense. These are valid values for $\hat{\beta}$ and $\hat{\alpha}$ that satisfy the condition above. To see why, consider a person with college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 1 + \hat{\beta}_2 \cdot 0 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \\
\text{log earnings} &= 0 + 10 = 10
\end{aligned}$$

and a person without college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 0 + \hat{\beta}_2 \cdot 1 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_2 \\
\text{log earnings} &= 0 + 8 = 8
\end{aligned}$$

* $\hat{\beta}_1 = 2$
* $\hat{\beta}_2 = 0$
* $\hat{\alpha} = 8$

also make sense. To see why, consider a person with college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 1 + \hat{\beta}_2 \cdot 0 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \\
\text{log earnings} &= 8 + 2 = 10
\end{aligned}$$

and a person without college:

$$\begin{aligned}
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_1 \cdot 0 + \hat{\beta}_2 \cdot 1 \\
\text{log earnings} &= \hat{\alpha} + \hat{\beta}_2 \\
\text{log earnings} &= 8 + 0 = 8
\end{aligned}$$

It turns out, there are actually infinitely many solutions for $\hat{\beta}$ that satisfy the condition where people who went to college have mean log earnings of 10 and people who did not go to college have mean log earnings of 8. This holds true for all situations where you regress on a constant and a set of mutually exclusive and exhaustive dummies. There is no unique solution for $\hat{\beta}$, which is a problem for econometricians who want unique and interpretable coefficients.

In fact, there is mathematical justification for this as well. At some point in the math involved in performing regression, having redundant variables causes a division by 0. This is particularly upsetting for your computer, and it will complain.

So how do we avoid this problem? We deliberately exclude one of the variables. It can technically either be one of the dummy variables or the intercept term, but we usually really want to have an intercept term present in our regression for other reasons. So we usually get rid of one of the dummy variables. Notice that we implicitly did this earlier. We did not include "not college" in our first regression.



--- END 11-econometrics/multivariable.md ---



--- START 11-econometrics/reading-econ-papers.md ---

---
title: reading-econ-papers
type: textbook
source_path: content/11-econometrics/reading-econ-papers.ipynb
chapter: 11
---

# Reading Economics Papers

In upper division economics courses, you'll often read economics papers that utilize ordinary least squares to conduct regression. Now that we have familiarized ourselves with multi-variate regression, let's familiarize ourselves with reading the results of economics papers!

Let's consider an existing empirical study conducted by David Card {cite}`11reading-econ-papers`, a Nobel Prize winning professor at UC Berkeley, that regresses income on education:

![](https://i.imgur.com/FPLII4s.png)

Every column here is from a different regression: the first column predicts the log hourly earnings from years of education, the fifth column predicts the log annual earnings from years of education, and so on. For now, let's focus on the first column, which states the linear regression as follows: 

$$
\ln{(\text{hourly earnings})_i} = \alpha + \beta \cdot (\text{years of schooling})_i + \varepsilon_i
$$

From the table, the education coefficient is 0.100, with a (0.001) underneath it. This means that our $\beta$ value is equal to 0.100. What does the (0.001) mean? It is the standard error, which is essentially a measure of our uncertainty. From Data 8, the standard error is most similar to the standard deviation of sample means, which is a measure of the spread in the population mean. Similarly, the the standard error here is a measure of the spread in the population coefficient. We can use the standard error to construct a confidence interval of the actual coefficient: a 95% confidence interval is between 2 standard errors above and below the reported value.

The effects of schooling on income is captured by the education coefficient term: 0.100. This means that an increase in 1 unit (year) of education is correlated with a log hourly earnings by 0.1. This approximately corresponds to a 10% increase in wages per year of schooling.



--- END 11-econometrics/reading-econ-papers.md ---



--- START 11-econometrics/single-variable.md ---

---
title: single-variable
type: textbook
source_path: content/11-econometrics/single-variable.ipynb
chapter: 11
---

# Single Variable Regression

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import warnings
warnings.simplefilter("ignore")
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import patches
from datascience import *
%matplotlib inline
from sklearn.metrics import mean_squared_error
from ipywidgets import interact, interactive, fixed
import ipywidgets as widgets
```

Suppose we have some data that looks like this on a scatter plot:

![title](figure1.png)

In Data 8 we learned that $x$ and $y$ above have some **correlation coefficient** $r$, which is a measure of the strength of the linear relationship between the two variables.

It looks like there is some positive linear association between $x$ and $y$ such that larger values of $x$ correspond to larger values of $y$. We therefore expect $r$ to be some positive number between 0 and 1, but not exactly 0 or exactly 1.

First, let's convert the data (stored in the arrays `x` and `y`) to standard units. To convert a set of data points to standard units, we subtract out the mean of the data and scale by the standard deviation. This has the effect of changing the data so that the data in standard units have mean 0 and  standard deviation 1. Below we construct a function that does this.

$$
x_{su} = \dfrac{x - \mu_x}{\sigma_x}
$$

```python
def standard_units(array):
    return (array - np.mean(array)) / np.std(array)
```

```python
# Hide this.
np.random.seed(42)
x = np.random.uniform(0, 10, 100)
noise = np.random.randn(100) * 4
y = 1.5 * x + noise
```

```python
x_standard = standard_units(x)
y_standard = standard_units(y)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(x_standard, y_standard)
plt.xlabel('$x$_standard', fontsize = 14)
plt.ylabel('$y$_standard', fontsize = 14);
```

The scatter plot looks the same as before, except now the axes are scaled such that we measure $x$ and $y$ in standard units. Now recall that $r$ is calculated as the average of the product of two variables, when the variables are measured in standard units. Below we define a function that calculates $r$, assuming that the inputs have already been converted to standard units.

```python
def correlation(array1, array2):
    return np.mean(array1 * array2)
```

What is the correlation between these two variables?

```python
correlation(x_standard, y_standard)
```

Recall from Data 8 that we use $r$ to form a line called the *regression line*, which makes predictions for $y$ given some $x$. Our prediction for $y$ in standard units is $r \cdot x$. If we want to fit this regression line in the original units, recall that the slope of this line is given by

$$
\text{slope} = r \cdot \dfrac{\hat{\sigma}_y}{\hat{\sigma}_x}
$$

and the intercept is given by

$$
\text{intercept} = \hat{\mu}_y - \text{slope} \cdot \hat{\mu}_x
$$

where $\hat{\sigma}_x$ is the observed standard deviation of a variable $x$ and $\hat{\mu}_x$ is the observed mean. Our regression line will have the form

$$
y = \hat{\alpha} + \hat{\beta} x
$$

where $\hat{\alpha}$ is the intercept from above and $\hat{\beta}$ the slope.

Below we plot this line on the same scatter plot from before.

```python
r = correlation(x_standard, y_standard)
slope = r * np.std(y) / np.std(x)
intercept = np.mean(y) - slope * np.mean(x)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(x, y)
plt.plot(np.linspace(0, 10), slope * np.linspace(0, 10) + intercept, color='tab:orange')
plt.xlabel('$x$', fontsize = 14)
plt.ylabel('$y$', fontsize = 14);
```

Let's take a closer look at the slope we found.

```python
print('Slope: ', slope)
```

To generate the data above, we started with some range of $x$ values, and generated $y$ as a linear function of $x$ with some random noise added in. Take a look:

```python
np.random.seed(42)
x = np.random.uniform(0, 10, 100)
noise = np.random.randn(100) * 4
y = 1.5 * x + noise
```

Notice how I defined $y$:

$$
y = 1.5 \cdot x + u
$$

where $u$ is some normally-distributed random noise whose average is 0. So, while there is some randomness to the data, on average the "true" slope of the relationship is 1.5. Yet we predicted it to be roughly 1.3!

This highlights the following fact: Suppose we have some random data that we believe has a linear relationship. The least-squares slope we generate from the data is an *estimate* of the "true" slope of that data. Because of this, the estimated slope is a random variable that depends on the data we happen to have.

To highlight this fact, let's repeat the procedure above but with a different [random seed](https://en.wikipedia.org/wiki/Random_seed), in order to get data with the same underlying relationship but different values.

```python
np.random.seed(189)
x = np.random.uniform(0, 10, 100)
noise = np.random.randn(100) * 4
y = 1.5 * x + noise

r = correlation(x_standard, y_standard)
slope = r * np.std(y) / np.std(x)
intercept = np.mean(y) - slope * np.mean(x)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(x, y)
plt.plot(np.linspace(0, 10), slope * np.linspace(0, 10) + intercept, color='tab:orange')
plt.xlabel('$x$', fontsize = 14)
plt.ylabel('$y$', fontsize = 14);
print('Slope: ', slope)
```

Now the estimated slope is roughly 1.6, even though the underlying data was still generated using a slope of 1.5. This is a very important concept that we will revisit soon.

Keep in mind, however, that correlation in data *does not* imply causation. In this example we know the true causal relationship between $x$ and $y$ because we defined it ourselves. However, when using real data you do not see the "true" relation and thus cannot conclude causality from correlation. It could simply be that both your variables depend on an unseen third variable and have no causal effect on one another. Or even worse, while unlikely it could be the case that slight linear trends in two variables is a complete coincidence.

## Root-Mean-Squared Error

While we can arbitrarily pick $\hat{\alpha}$ and $\hat{\beta}$ values, we do want to pick the values that help predict $\hat{y}$ that are closest to actual $y$ values. To achieve this, we want to minimize a **loss function** that quantifies how far off our prediction $\hat{y}$ is from $y$ for some known data points. One of the most common loss functions is called the **root-mean-squared error**, and is defined as

$$
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left ( y_i - \hat{y}_i \right ) ^2 }
$$

where $n$ is the number of observations. The effect of this is to take the mean of the distance of each value of $\hat{y}$ from its corresponding value in $y$; squaring these values keeps them positive, and then we take the square root to correct the units of the error.

Plugging in the formula $\hat{y}$ in RMSE formula, we get,

$$
\text{RMSE} = \sqrt{ \frac{1}{n} \sum_{i=1}^n \left ( y_i - (\hat{\alpha} + \hat{\beta}x_i) \right ) ^2 }
$$

By doing a bit of calculus, we get the following formulas for $\hat{\alpha}$ and $\hat{\beta}$

$$\Large
\hat{\beta} = r\frac {\hat{\sigma}_y} {\hat{\sigma}_x} \qquad \qquad
\hat{\alpha} = \hat{\mu}_y - \hat{\beta}\hat{\mu}_x
$$

where $r$ is the **correlation** between $x$ and $y$, $\hat{\sigma}_y$ is the standard deviation of $y$, $\hat{\sigma}_x$ is the standard deviation of $x$, $\hat{\mu}_y$ is the average of all our $y$ values, and $\hat{\mu}_x$ is the average of all our $x$ values. (As an aside, note the hats on our $\sigma$'s and $\mu$'s; this is because these are _empirical estimates_ of the parameters of these distributions, rather than the true values.) These are the same values we had above!

Note that our formula for $\hat{\beta}$ involves the **correlation coefficient** $r$ of $x$ and $y$. The correlation coefficient of two variables is a measure of the strength of a linear relationship between them. $r$ goes from -1 to 1, where $|r|=1$ is a perfect linear relationship and $r=0$ is no linear relationship. The formula for $r$ is

$$
r = \frac{1}{n}\sum^n_{i=1} \left ( \frac{x_i - \hat{\mu}_x}{\hat{\sigma}_x} \right ) \left ( \frac{y_i - \hat{\mu}_y}{\hat{\sigma}_y} \right )
$$

(Note: the form $\frac{x_i - \hat{\mu}_x}{\hat{\sigma}_x}$ of a variable $x$ is it's representation in standard units, as mentioned above.)

To calculate the RMSE, we will write an `rmse` function that makes use of sklearn's `mean_squared_error` function.

```python
def rmse(target, pred):
    return np.sqrt(mean_squared_error(target, pred))
```

To get a better idea of what the RMSE represents, the figures below show a small dataset, a proposed regression line, and the squared error that we are summing in the RMSE. The data points are

| $x$ | $y$ |
|-----|-----|
| 0 | 1 |
| 1 | .5 |
| 2 | -1 |
| 3 | 2 |
| 4 | -3 |

Here are the proposed regression lines and their errors:

![se_slope_1.png](se_slope_1.png)

![se_slope_-1.png](se_slope_-1.png)

```python
d = Table().with_columns(
    'x', make_array(0,  1,  2,  3,  4),
    'y', make_array(1, .5, -1,  2, -3))

def plot_line_and_errors(slope, intercept):
    print("RMSE:", rmse(slope * d.column('x') + intercept, d.column('y')))
    plt.figure(figsize=(5,5))
    points = make_array(-2, 7)
    p = plt.plot(points, slope*points + intercept, color='orange', label='Proposed line')
    ax = p[0].axes
    
    predicted_ys = slope*d.column('x') + intercept
    diffs = predicted_ys - d.column('y')
    for i in np.arange(d.num_rows):
        x = d.column('x').item(i)
        y = d.column('y').item(i)
        diff = diffs.item(i)
        
        if diff > 0:
            bottom_left_x = x
            bottom_left_y = y
        else:
            bottom_left_x = x + diff
            bottom_left_y = y + diff
        
        ax.add_patch(patches.Rectangle(make_array(bottom_left_x, bottom_left_y), abs(diff), abs(diff), color='red', alpha=.3, label=('Squared error' if i == 0 else None)))
        plt.plot(make_array(x, x), make_array(y, y + diff), color='red', alpha=.6, label=('Error' if i == 0 else None))
    
    plt.scatter(d.column('x'), d.column('y'), color='blue', label='Points')
    
    plt.xlim(-4, 8)
    plt.ylim(-6, 6)
    plt.gca().set_aspect('equal', adjustable='box')
    
    plt.legend(bbox_to_anchor=(1.8, .8))
    plt.show()

interact(plot_line_and_errors, slope=widgets.FloatSlider(min=-4, max=4, step=.1), intercept=widgets.FloatSlider(min=-4, max=4, step=.1));
```

## Econometric Single Variable Regression

The regression line can have two purposes:

* Of particular interest to data scientists is the line's ability to predict values of $y$ for new values of $x$ that we didn't see before.

* Of particular interest to economists is the line's ability to estimate the "true" underlying slope of the data via its slope.

This is why regression is such a powerful tool and forms the backbone of econometrics. If we believe that our data satisfy certain assumptions (which we won't explore too much this lecture), then we can use the slope of the regression line to estimate the "true" relation between the variables in question and learn more about the world we live in.

In econometrics, we usually write the "true" underlying linear relationship as follows:

$$
y = \alpha + \beta \cdot x + \varepsilon
$$

where $y$ and $x$ are values for any arbitrary point, $\alpha$ is the intercept, $\beta$ is the slope, and $\varepsilon$ is some noise. This is entirely analogous to the code from earlier that determined the true linear relationship between the data:

```python
y = 1.5 * x + noise
```

Here, $\beta = 1.5$, $\alpha = 0$, and $\varepsilon = \text{noise}$.

When we fit a regression line onto the data, we express the line as:

$$
\hat{y} = \hat{\alpha} + \hat{\beta} \cdot x
$$

Here, we put hats over the slope and intercept terms because they are *estimates* of the true slope and intercept terms. Similarly, we put a hat over $y$ because this is the $y$ value that the regression line predicts.

Notice how the noise term $\varepsilon$ does not appear in the expression for the regression line. This is because the noise term is a random variable that has no relation with $x$, and is thus impossible to predict from the data. Furthermore, the noise term has a mean value of 0, so on average we actually don't expect the noise term to have any impact on the underlying trends of the data.

For the Data 8 demonstration above, we forced these conditions to be true. However, with real data these are assumptions that we have to make, and is something that econometricians spend a lot of time thinking about.

### Years of Schooling and Earnings

Consider a case where we want to study how years of schooling relate to a person's earnings. This should be of particular interest to college students. Below we import a dataset that has the hourly wage, years of schooling, and other information on thousands of people sampled in the March 2012 Current Population Survey.

```python
cps = Table.read_table('cps.csv')
cps
```

We want to consider a person's wage and years of schooling. But first, we will convert wage to log-wage. Wage is a variable that we would expect to increase proportionally (or, exponentially) with changes in years of schooling. And as such, we usually take the natural log of wage instead. Below we plot log wage and years of schooling for the CPS data.

```python
educ = cps.column('educ')
logwage = cps.column('logwage')

plt.figure(figsize=(8,6))
plt.scatter(educ, logwage)
plt.xlabel('Years of Education', fontsize = 14)
plt.ylabel('Log Wage', fontsize = 14)
plt.title('Log Wage vs. Years of Education', size = 20);
```

Now let's fit a least-squares regression line onto this data. First, we'll do it manually in the Data 8 style above.

```python
educ_standard = standard_units(educ)
logwage_standard = standard_units(logwage)

r = correlation(logwage_standard, educ_standard)
slope = r * np.std(logwage) / np.std(educ)
intercept = np.mean(logwage) - slope * np.mean(educ)
```

```python
plt.figure(figsize=(8,6))
plt.scatter(educ, logwage)
plt.plot(np.linspace(0, 20), slope * np.linspace(0, 20) + intercept, color='tab:orange')
plt.xlabel('Years of Education', fontsize = 14)
plt.ylabel('Log Wage', fontsize = 14)
plt.title('Log Wage vs. Years of Education', size = 20);
print('Slope: ', slope)
print('Intercept: ', intercept)
```

So from the very simple and straight-forward model above, it seems that we estimate a slope of roughly 0.1, meaning we might expect that a one-year increase in schooling is associated with a 10% increase in wage, on average.

We can also see that we have a non-zero intercept term. We should be careful how we interpret this term; from a strictly mathematical point of view, the intercept represents the expected value of $y$ (in this case log wage) when $x = 0$. However, in economics sometimes it makes no sense for $x$ to be 0, and so we cannot use the above interpretation. We won't go into detail this lecture, but regardless of whether the intercept is interpretable, we almost always want to include it.

## Uncertainty in $\hat{\beta}$

We mentioned earlier that the slope we estimate from regression is exactly that: an estimate of the "true" underlying slope. Because of this, the estimate $\hat{\beta}$ is a random variable that depends on the underlying data.

Let's assume there is the following true linear relation between log wage and years of schooling,

$$
\text{logwage} = \alpha + \beta \cdot \text{years of schooling} + \varepsilon
$$

and we try to estimate $\alpha$ and $\beta$.

If our data are "well-behaved", then even though there is uncertainty in our estimate $\hat{\beta}$, on average $\hat{\beta}$ will be $\beta$; that is to say that the expectation of $\hat{\beta}$ is $\beta$. Additionally, if our data are "well-behaved", then $\hat{\beta}$ has some normal distribution with mean $\beta$. We won't worry too much about what assumptions need to be satisfied to make the data "well-behaved".

You can think of each person as an observation of these variables, and using a sample of people we can estimate the relationship between the two variables. However, due to the noise term and the fact that we only have a finite sample of people, the true relationship is always hidden from us, and we can only hope to get better estimates by designing better experiments and sampling more people.

Let's try to get an idea of how "certain" we can be of our estimate $\hat{\beta}$. We'll do this in classic Data 8 style: bootstrapping. Using our existing sample data, we'll create new samples by bootstrapping from the existing data. Then, for each sample, we'll fit a line, and keep the slope of that line in a list with all of the other slopes. Then, we'll find the standard deviation of that list of slopes.

```python
slopes = make_array()
educ_logwage = cps.select("educ", "logwage")

np.random.seed(42)
for i in np.arange(200):
    educ_logwage_sample = educ_logwage.sample()
    y = educ_logwage_sample.column("logwage")
    X = educ_logwage_sample.column("educ")
    model = sm.OLS(y, sm.add_constant(X)).fit()
    slopes  = np.append(model.params[1], slopes)
    
Table().with_columns("Slopes", slopes).hist()    
print('Standard dev. of bootstrapped slopes: ', np.std(slopes))
```

Our bootstrapped approximation standard error of 0.00159 is pretty close to the true standard error of 0.00144. `statsmodels`, the package we will be using to perform regressions, actually uses a precise mathematical formula for finding the standard error whereas we tried to find this value through simulation, but the idea behind the standard error is the same.

Armed with a standard error, we can now form a 95% confidence interval and perform a test of significance to see if $\hat{\beta}$ is significantly different from 0.

```python
# Using our resampled slopes
lower_bound = percentile(2.5, slopes)
upper_bound = percentile(97.5, slopes)
print('95% confidence interval: [{}, {}]'.format(lower_bound, upper_bound))
```

The 95% confidence interval does not contain 0, and so $\beta$ is unlikely to be 0.

## Regression with a Binary Variable

A binary variable is a variable that takes on the value of 1 if some condition is true, and 0 otherwise. These are also called dummy variables or indicator variables. It might sound strange at first, but you can actually perform regression of a variable like log earnings onto a binary variable.

Let's import a different dataset that has the following features. Some will be useful to us later.

```python
nlsy = Table.read_table('nlsy_cleaned_small.csv')
nlsy
```

Now let's visualize log earnings vs. the binary variable corresponding to whether or not an observation went to college.

```python
coll = nlsy.column('college')
logearn = nlsy.column('log_earn_1999')

plt.figure(figsize=(8,6))
plt.scatter(coll, logearn)
plt.xlabel('College', fontsize = 14)
plt.ylabel('Log Earnings', fontsize = 14)
plt.title('Log Earnings vs. College Completion', size = 20)
plt.xticks([0,1]);
```

```python
no_college = nlsy.where('college', 0).column("log_earn_1999")
has_college = nlsy.where('college', 1).column("log_earn_1999")

plt.figure(figsize=(8,6))
plt.xlabel('College', fontsize = 14)
plt.ylabel('Log Earnings', fontsize = 14)
plt.title('Log Earnings vs. College Completion', size = 20)
plt.violinplot(no_college, positions = [0], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False)
plt.violinplot(has_college, positions = [1], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False);
```

Now let's fit a regression model:

```python
coll_standard = standard_units(coll)
logearn_standard = standard_units(logearn)

r = correlation(logearn_standard, coll_standard)
slope = r * np.std(logearn) / np.std(coll)
intercept = np.mean(logearn) - slope * np.mean(coll)

print("y = {:.5f} * x + {:.5f}".format(slope, intercept))
```

Wow! This regression would imply that we expect, on average, observations who went to college to have 70% higher earnings than those who did not go to college. Let's now plot this line on the data:

```python
plt.figure(figsize=(8,6))
plt.scatter(coll, logearn)
plt.plot(np.linspace(0, 1), slope * np.linspace(0, 1) + intercept, color='tab:orange')
plt.xlabel('College', fontsize = 14)
plt.ylabel('Log Earnings', fontsize = 14)
plt.title('Log Earnings vs. College Completion', size = 20)
plt.xticks([0,1]);
```

```python
no_college = nlsy.where('college', 0).column("log_earn_1999")
has_college = nlsy.where('college', 1).column("log_earn_1999")

plt.figure(figsize=(8,6))
plt.plot(np.linspace(0, 1), slope * np.linspace(0, 1) + intercept, color='tab:green')
plt.xlabel('College', fontsize = 14)
plt.ylabel('Log Earnings', fontsize = 14)
plt.title('Log Earnings vs. College Completion', size = 20)
plt.violinplot(no_college, positions = [0], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False)
plt.violinplot(has_college, positions = [1], points=20, widths=0.3, showmeans=True, showextrema=True, showmedians=False);
```

When we perform a simple regression onto just a dummy variable, it is an important fact that $\hat{\alpha}$ is the mean value of $y$ for all observations in the sample where $x = 0$, and $\hat{\beta}$ is the difference between the mean value of $y$ for observations in the sample where $x = 1$ and observations where $x = 0$. Proving this claim is beyond our scope this week, but let's verify it with our data:

```python
avg_logearn_coll = np.mean(logearn[coll == 1])
avg_logearn_nocoll = np.mean(logearn[coll == 0])

print('Avg logearn for coll = 1: ', avg_logearn_coll)
print('Avg logearn for coll = 0: ', avg_logearn_nocoll)
print('Difference between the two: ', avg_logearn_coll - avg_logearn_nocoll)
```

```python
print('Intercept: ', intercept)
print('Slope: ', slope)
```



--- END 11-econometrics/single-variable.md ---



--- START 11-econometrics/statsmodels.md ---

---
title: statsmodels
type: textbook
source_path: content/11-econometrics/statsmodels.ipynb
chapter: 11
---

# Using `statsmodels` for Regression

```python
from datascience import *
import numpy as np
import statsmodels.api as sm

cps = Table.read_table('cps.csv')
```

In the previous section, we used functions in NumPy and concepts taught in Data 8 to perform single variable regressions. It turns out that there are (several) Python packages that can perform these regressions for us and which extend nicely into the types of regressions we will cover in the next few sections. In this section, we introduce `statsmodels` for performing single variable regressions, a foundation upon which we will build our discussion of multivariable regression.

`statsmodels` is a popular Python package used to create and analyze various statistical models. To create a linear regression model in `statsmodels`, which is generally import as `sm`, we use the following skeleton code:

```python
x = data.select(features).values            # Separate features (independent variables) 
y = data.select(target).values              # Separate target (outcome variable)
model = sm.OLS(y, sm.add_constant(x))       # Initialize the OLS regression model
result = model.fit()                        # Fit the regression model and save it to a variable
result.summary()                            # Display a summary of results
```

*You must manually add a constant column of all 1's to your independent features. `statsmodels` will not do this for you and if you fail to do this you will perform a regression without an intercept $\alpha$ term. This is performed in the third line by calling `sm.add_constant` on `x`.* Also note that we call `.values` after we select the columns in `x` and `y`; this gives us `NumPy` arrays containing the corresponding values, since `statsmodels` can't process `Table`s.

Recall the `cps` dataset we used in the previous section:

```python
cps
```

Let's use `statsmodels` to perform our regression of `logwage` on `educ` again.

```python
x = cps.select("educ").values
y = cps.select("logwage").values

model = sm.OLS(y, sm.add_constant(x))
results = model.fit()
results.summary()
```

The summary above provides us with a lot of information. Let's start with the most important pieces: the values of $\hat{\alpha}$ and $\hat{\beta}$. The middle table contains these values for us as `const` and `x1`'s `coef` values: we have $\hat{\alpha}$ is 1.4723 and $\hat{\beta}$ is 0.1078.

Recall also our discussion of uncertainty in $\hat{\beta}$. `statsmodels` provides us with our calculated standard error in the `std err` column, and we see that the standard error of $\hat{\beta}$ is 0.001, matching our empirical estimate via bootstrapping from the last section. We can also see the 95% confidence interval that we calculated in the two rightmost columns.

![](statsmodels-coeffs.png)

Earlier we said that $\hat{\beta}$ has some normal distribution with mean $\beta$ if certain assumptions are satisfied. We now can see that the standard deviation of that normal distribution is the standard error of $\hat{\beta}$. We can also use this to test a null hypothesis that $\beta = 0$. To do so, construct a [t-statistic](https://en.wikipedia.org/wiki/T-statistic) (which `statsmodels` does for you) that indicates how many standard deviations away $\hat{\beta}$ is from 0, assuming that the distribution of $\hat{\beta}$ is in fact centered at 0.

We can see that $\hat{\beta}$ is 74 standard deviations away from the null hypothesis mean of 0, which is an enormous number. How likely do you think it is to draw a random number roughly 74 standard deviations away from the mean, assuming a standard normal distribution? Essentially 0. This is strong evidence that the mean of the distribution (the mean of $\hat{\beta}$ is the true value $\beta$) is not 0. Accompanying the t-statistic is a p-value that indicates the statistical significance.



--- END 11-econometrics/statsmodels.md ---



--- START 12-environmental/.ipynb_checkpoints/KuznetsHypothesis-Copy1-checkpoint.md ---

---
title: KuznetsHypothesis-Copy1-checkpoint
type: textbook
source_path: content/12-environmental/.ipynb_checkpoints/KuznetsHypothesis-Copy1-checkpoint.ipynb
chapter: 12
---

```python
# HIDDEN
#Importing packages
from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import patches
%matplotlib inline
```

# Environmental Kuznets Curve Hypothesis

The Environmental Kuznets curve hypothesis that the economic development of a nation is associated with a downward-facing U-shape.  The Y-axis is in terms of the level of environmental degradation (e.g pollution, water quality, deforestation.  The X-axis would be the GDP/capita.  The idea is that the environmental degradation worsens, until a certain level of income, and after which it gets better. In the US this could be seen in terms of air or water quality, where the skies or rivers were very polluted in the 1960s, until the Clean Air Act and Clean Water Act were passed and Air Quality and Water Quality improved.  Another motivation for the downward slope would be the idea that at some point a wealthier society demands environmental improvements.  
However - could this hold for the potentially most important Pollutant C02, the main driver of anthropogenic climate change.  Controversially the impacts of global CO2 pollution are not experienced locally, but are experienced as global effects. So it is not clear whether the Environmental Kuznets hypothesis will hold.  

Today, we'll look to build an C02 Kuznets curve for an *association* between the amount of CO2 emitted per capita (t/CO2) and the growing GDP per capita (USD). This dataset is collected from Our World in Data, a great source of all sorts of data types!

![kuznets.png](kuznets.png)

## Building our own Environmental Kuznets Curve

We start by importing data on GDP per capita and Per Capita CO2 emissions for every country in the world for as long as it has been recorded.

```python
co2_table = Table.read_table('co2-emissions-vs-gdp.csv').drop('145446-annotations','Total population (Gapminder, HYDE & UN)','Code')
co2_table = co2_table.relabeled('Entity', 'Country')
co2_table
```

### Low Income Countries  
Let's start by selecting a set of Low Income Countries to graph the movement of CO2 intensity

```python
#Low-Income Nations
LIH_array = make_array('Haiti', 'Afghanistan','Rwanda','Pakistan', 'Nicaragua')
LIH_table = co2_table.where('Country', are.contained_in(LIH_array))
LIH_table = LIH_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
plt.figure(figsize = (8,6), dpi=250)
LIH_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

Note that each dot represents a nation at a given level of emissions and GDP per capita

With these three countries we see a few different stories:
 - In Afghanistan, Haiti and Rwanda we see little income growth and little CO2 intensity growth with a slight upward trend
 - In Nicaragua we see some jumping around, in fact Nicaragua GDP per capita has gone up and down, as has CO2 per capita
 - In Pakistan, a larger and more populous country we see strong linear upward growth in both GDP per capita and CO2 per capita, with no signs of turning down

In these countries it is hard to tell the complete story without the exact time trend.

### BRICS  
Lets look at the BRICS countries, the rapidly growing upper middle income countries

```python
BRICS_array = make_array('Brazil','Russia','India','China','South Africa')
BRICS_table = co2_table.where('Country', are.contained_in(BRICS_array))
BRICS_table = BRICS_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
BRICS_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

The BRICS nations seem to have a variety of development pathways but all show the linear trend of increasing emissions as wealth grows.  
- Russia has an interesting dip while GDP per capita decrease and then increase again
- South Africa has a recent period where growth in both GDP per capita and CO2 per capita have stagnated
- China and India show linearly increasing trends, with China both wealthier and more CO2 intensive

## Individual country graphs
Lets look at some individual countries, starting with the US.   
We can plot both total and logged quantities

### USA

```python
US_table = co2_table.where('Country', 'United States').where('Year', are.between(1800,2018))
US_table = US_table.with_column('LogGDP', np.log(US_table.column('GDP per capita'))).with_column('LogCO2',np.log(US_table.column('Per capita CO2 emissions')))
US_table.scatter('GDP per capita', 'Per capita CO2 emissions')
US_table.scatter('LogGDP', 'LogCO2')
```

**Looks like we have a curve!**

In the case of the US it does indeed look like the C02 per capita does indeed start to deline after about $40,000
Somewhere around 2000-2004 the CO2 emissions leveled off and then began to decline

### China

In COP26, China has been a large part of the discussion (and emissions). Let's have a look at their curve!

```python
#China Example + LOG
NO_table = co2_table.where('Country', 'China').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
```

Indeed it appears that China has an inflection point and is starting to level off in the Carbon intensity per capita

### India

What about India?

```python
#India Example + LOG
NO_table = co2_table.where('Country', 'India').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
```

### Norway

As I'm Norwegian, I thought it might be cool to see how things are going back home as well:

```python
#Norway Example + LOG
NO_table = co2_table.where('Country', 'Norway').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
```

Turns out we're ahead of the US in CO2 emissions per capita, but there's still a long way to go until our development resembles a full Kuznets curve. However, it certainly looks like something! An almost vertical linear growth in terms of per capita CO2 emissions in the early economic stages stagnated into a period of fluctuations. As of now, it looks like it’s heading in a downward trend.

Let's look at a set of other High Income Nations:

```python
HIN_array = make_array('United States', 'Netherlands', 'United Kingdom','Germany','Canada')
HIN_table = co2_table.where('Country', are.contained_in(HIN_array))
HIN_table = HIN_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
HIN_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

As in the US and Norway, these nations have experienced a boom, stagnation, and now to some extent a downward trend. Let's finally plot all the previously observed nations together:

```python
ALL_array = np.append((np.append(LIH_array,BRICS_array)), HIN_array)
ALL_table = co2_table.where('Country', are.contained_in(ALL_array))
ALL_table = ALL_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
ALL_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
#What do we see? Can we spot the Environmental Kuznets Curve?
```

Here we see evidence for an Environmental Kuznets Curve.

It seems, at least to some extent, that as nations develop economically, the level of environmental degradation reaches a peak and then declines, mapping a downward-facing U-curve.

## Criticism of the Environmental Kuznets Curve Hypothesis

Some questions we ought to ask ourselves in the end are:
* Do all types of environmental degradation follow the curve? What if we plot Energy, Land, & Resource usage?
* What we plotted today shows the ratio between GDP and CO2 per capita, but what about the *absolute* numbers of emissions?
* What is the true long-term shape of the curve? Could it reshape itself to an "N" as an economy passes a certain threshold?
* What about its applicability on a global scale? Knowing that the HINs have a habit of exporting pollution to LINs, what will happen as LIN grow economically?

These are just some questions environmental economists have asked themselves throughout the years since the curve was hypothesized in 1955. Some, including Perman and Stern (2003) conclude that the level of environmental degradation has much more to do with a constant "battle" between scale and time than income alone. As nations scale up (BRICS, for instance) the growth results in higher emissions, while countries with lower growth (LIN & HIN) seem more influenced by the "time-effect", which results in lower emissions. Others, among Krueger & Grossman, argue that there is "no evidence that environmental quality deteriorates steadily with economic growth."

More on these theories can be found in the recommended readings below.

As data scientists motivated to help heal the plant with the tools of environmental economics, we can help to find these answers!

## What's next?

If you are interested in this area, there are even more fascinating applications of Data Science to environmental topics such as: finding the social cost of carbon, the valuation of our environment, and the economics of emissions trading. Besides purely economical modeling, the field of environmental data science is rapidly growing as we collect more and more data on our planet and its resources. Applying the power of Satellite Imagery, Machine Learning, and Geographic Information Systems (GIS), one can follow both technology and policy-based paths, both ensured to have a positive impact in shaping a data-driven, sustainable future.

## Further recommended readings

Levelized Cost of Carbon Abatement: An Improved Cost-Assessment Methodology for a Net-Zero Emissions World (also the main source of this Jupyter Notebook)

https://www.energypolicy.columbia.edu/sites/default/files/file-uploads/LCCA_CGEP-Report_101620.pdf

Dynamic vs. Static costs are described further in K.Gillingham & J.H Stock's The Cost of Reducing Greenhouse Gas Emissions (italic) from 2018. - A highly recommended reading out of scope for this class.

https://scholar.harvard.edu/files/stock/files/gillingham_stock_cost_080218_posted.pdf

Goldman Sachs Research: Carbonomics: The Future of Energy in the Age of Climate Change
  
https://www.goldmansachs.com/insights/pages/carbonomics.html

EPA article on the Economics of Climate Change:
https://www.epa.gov/environmental-economics/economics-climate-change

Draw your own curve program:
https://tamc.github.io/macc/

Abatement curve for crops:
https://github.com/aj-sykes92/ggmacc/blob/main/README_files/figure-gfm/full-macc-1.png


Aalborg University's software:
https://github.com/matpri/EPLANoptMAC


--- END 12-environmental/.ipynb_checkpoints/KuznetsHypothesis-Copy1-checkpoint.md ---



--- START 12-environmental/.ipynb_checkpoints/index-checkpoint.md ---

---
title: index-checkpoint
type: textbook
source_path: content/12-environmental/.ipynb_checkpoints/index-checkpoint.md
chapter: 12
---

# What is Environmental Economics?

![](windmills.png)

In a broad sense, the field of Environmental Economics aims to relate and apply economic concepts to the environment.
 
One tenet of Environmental Economics is that the enjoyment of "environmental amenities"   (or conversely that the usage or degradation  of those resources) has an intrinsic value to humans that goes unaccounted for in the purely market-based model. These unaccounted costs are considered *market failures* and carry *negative externalities*.
 
The greatest single example of a negative externality of global importance is the emission of greenhouse gases (carbon dioxide, methane, nitrous oxide) from the combustion of hydrocarbons (coal, gasoline, diesel, oil). The *true cost*  is not reflected in the lower price one pays at e.g the gas station. Consequently, the equilibrium quantity consumed is higher than the *socially optimal quantity*. Environmental economists seek to model the costs and benefits of a reduction.  How could we reduce the quantity to the social optimum and weigh in the *costs and benefits* of such a reduction?
 
As a result, a major proportion of research and work within the field is devoted to building tools to reveal, address, and evaluate economic policies aimed at *internalizing* these externalities. 

Very often, these policy tools as applied by a government constitute interfering with the market to a varying degree. We can model environmental economic policies into two subsets:
 
* **Command and Control**: When the government limits the amount of pollution to control a negative externality, e.g letting each emitter in the market emit a fixed amount of GHG gases.
 
* **Market-based**: Where the government sets an emission goal, then introduces incentives or subsidies to alter market behavior. It is left to each market actor to decide how much to emit. A carbon tax and a cap-and-trade (carbon quotas) are examples of marked-based interventions.
 
A useful environmental economic model is **The Marginal Abatement Cost Curve (MAC)** which aims to describe the *cost of abatement* (not emitting) greenhouse gases into the atmosphere using various strategies. A famous version of this was contained in a report  McKinsey 2009 "Pathways to a Low Carbon Economy".  This example we will discuss as an  example of the intersection between of Environmental Economics and Data Science. In this notebook, we'll walk through the concepts of it and build one of our own! 

Related to the discussion of global greenhouse gas emissions, we will also explore the **Environmental Kuznets Curve** through data, and compare the pathways of increasing emissions across countries and across time.  
 
These are only a few of the applications of environmental economics, there are many more fields to explore, and even an entire major at UC Berkeley to explore.  

**Student Learning Outcomes:**
* An introduction to applications  of Environmental Economics with illustrations from global CO2 emissions
* Motivation and understanding of the  Marginal Abatement Cost Curve for global Greenhouse Gas emissions, first by the (McKinsey 2009) curve for CO2, and then an application for Methane its data science application.
* A discussion of the Marginal Abatement Curve's limitations with the concept of Capital Intensity and static vs dynamic costs.
* An understanding of the Environmental Kuznets Curve Hypothesis and its data science applications


--- END 12-environmental/.ipynb_checkpoints/index-checkpoint.md ---



--- START 12-environmental/.ipynb_checkpoints/textbook1-checkpoint.md ---

---
title: textbook1-checkpoint
type: textbook
source_path: content/12-environmental/.ipynb_checkpoints/textbook1-checkpoint.ipynb
chapter: 12
---

```python
# HIDDEN
#Import packages
from datascience import *
import matplotlib.pyplot as plt
%matplotlib inline 

import numpy as np
import pandas as pd
from matplotlib import patches
```

# Marginal Abatement Cost Curves

## Marginal Cost

As of know, you should be familiar with the economic concept of **marginal cost**. The classical textbook example is a production factory, so let's call ours Peter's Pens Limited. Whenever Peter's Pens' management is deciding whether or not to increase production to maximize their profit, they observe the firm's marginal cost: the change in its **total cost** that occurs as 1 more pen is produced. In this case, we measure the marginal cost in $ / per extra pen produced. In environmental economics, we talk about the **marginal cost of emissions** and the **marginal cost of emission abatement**. We think of it as follows:

$$ \text{MC of Emissions (\$/ton)} = \frac{\text{$\Delta$ Cost of Emissions (\$)}}{\text{$\Delta$ Quantity of Emissions (tons)}}$$

And: 

$$ \text{MC of Emission Abatement (\$/ton)} = \frac{\text{$\Delta$ Cost of Emissions Abatement (\$)}}{\text{$\Delta$ Quantity of Emission Abated (tons)}}$$

## Government Intervention

Let's say that Peter's Pens Limited is operating in a market where the government aims to make firms **internalize** the negative externality created by $CO_2$ pollution from their production. From earlier, we know that the government may choose between **prescriptive** or **market-based** policies, but for this case we assume they chose the latter. In that case, the government introduces a *Pigouvian tax* on per ton of $CO_2$ emitted from all firms. As any Pigouvian tax, its rate is set to the *social marginal cost of the negative externality* it is set to internalize. This is the *social cost of carbon*, quantified by extensive research. In the big picture, it pushes the private marginal cost towards the higher social marginal cost of emissions. As with any price increase, we assume the ‘supplied’ quantity of emissions to be pushed leftwards (decreased). This results in a new equilibrium quantity of emissions at a higher price (= social marginal cost) and the quantity reached becomes the *optimal quantity* of emissions in society. Now, how does the management of Peter’s Pens Limited react to the new market conditions?

![](tax.png)

### The Firm’s Reaction
 
From a ‘bottom-up-perspective’, the management at Peter’s Pens Limited is faced with two choices in reaction to the new emissions tax: The Business-as-Usual (BAU) alternative or the Emissions Abatement alternative. The firm can either choose to continue emitting and pay the tax or invest in new, environmentally friendly technologies that abate their $CO_2$ emissions. A rational decision maker ought to invest in abatement technologies with a marginal cost of abatement up to the emissions tax put in place. Assuming they chose to pursue the latter, how would they prioritize investments within the firm? By observing the cost and abatement potential of each investment, the firm sorts its options from cheapest to most expensive and starts ‘picking’ the low-hanging fruits of $CO_2$ abatement. At any given carbon tax rate, they reduce their tax bill to $0 and observe their total abatement as the sum of all technologies invested in. The resulting plot: The Marginal Abatement Cost (MAC) Curve of Peter’s Pens Limited. Here's how a simple MAC for Peter's Pens might look:

![SampleMACC_Nov23.png](SampleMACC_Nov23.png)

In this example, the firm has a few activities with a low cost of abatement, and then a few with higher cost of abatement. It would make sense for the firm to start with the lowest cost of abatement first.

### Back to the Government 

Returning to the government policymaker’s perspective, we can motivate this approach to a market wide perspective. Just as Peter’s Pens has their firm-specific MAC, society as a whole has one. Through research on policy and technological interventions, an environmental data science team can quantify the *abatement potential* and the *abatement cost* of different technologies (e.g electric cars, wind & solar power generation), policies (e.g fuel standards), and land management projects (e.g  soil restoration). From here, public policy can start  ‘picking’ the low-hanging fruits of the lowest cost technology first. 

Earlier, we described the effect of introducing a tax on the emissions ‘market’ and observed how rational decision-makers in individual firms react by constructing their own MAC curves. Now, let’s say that the government has set an emission reduction target but has not decided on a specific tax rate yet. How could they guarantee that the target is met? By taking the cumulative sum of the abatement potential of each intervention they expect to take place at a given rate, they should keep doing so until their sum equals the emission reduction target. From there, they observe the marginal abatement cost of the last intervention they added to the sum. This could be a way to set the emissions tax rate. Here's how a simplified version could look:

![image.png](image.png)

### Conclusion
 
In conclusion, both an individual firm and society as a whole could be modeled to have MAC curves. By either choosing an emission reduction target or a emissions tax rate, one can arrive at a lowest cost level of abatement. Whenever an emission tax is introduced to the market, rational firms build their MAC curves and start investing in the cheapest relevant technologies up to the level of the given tax rate to avoid paying the tax. A rational policy maker can map out the different abatement opportunities in a society as a whole and prioritize the cheapest alternatives. Public policy could stop emissions abatement when the technology or policy we invest in has a higher marginal cost than the societal marginal cost of emissions. From there, any intervention reduces either individual (firm) or societal welfare.
 
Now, let’s delve deeper into the  MAC curve and build one of our own for Methane Abatement.

## The McKinsey Marginal Abatement Cost Curve (MAC)

As earlier described, the Marginal Abatement Cost Curve gives policy makers and firms an opportunity to differentiate the costs of the multiple approaches we have in reducing our carbon (CO2) emissions. It shows where society can get the best "bang for the buck" when the goal is to abate carbon emissions. The **Abatement Potential (GtCO2 per year)** follows the x-axis, and the ** Marginal Abatement Cost (€ per ton of CO2)** lies on the y-axis.
 
Each rectangle represents a specific technology or policy (e.g switch to LED lights or install Carbon Capture Systems (CCS) in older coal plants). The *wider* the rectangle, the *larger* the abatement potential, and the *taller* the rectangle, the *higher* abatement cost for that specific intervention.
 
Below you'll find a very influential MAC out there, the McKinsey MAC from 2009. It has the goal of showing the tradoffs in a single visualization of GHG emissions across a variety of sectors and across all countries. A policy maker, or innovator, or investor could decide to focus on areas where GHG emissions can be reduced at a low cost.

![Screen%20Shot%202021-05-15%20at%201.39.57%20PM.png](mac.png)

### Why are some costs negative?

Something that understandably strikes a lot of observers from the plot above is the concept of *negative costs*. Take LEDs (about - €100 per tCO2) for instance: The negative cost here entails that for every investment that reduces CO2 from traditional lightning by 1 ton CO2, the investor receives a €100 saving. In one sense these are all investments that should happen regardless of the GHG consequences they make economic sense. One way to motivate this is that the decision maker may not be able to see the long term benefits in a short-term decision making framework. People dont rush out to buy $5 LED light bulbs when they have working incandescent light bulbs.
 
The cause of negative costs are disputed, but a commonly used idea is that within our current market model, there exists *inefficiencies* such as lack of incentives or information that prohibit market participants from taking the full advantage of the returns offered from investing in technologies with negative costs.

## Constructing a MAC for Methane Gas Abatement

For an illustrative example we will construct a Marginal Abatement Curve for Methane emissions from the Oil and Gas Sector.  
We start by importing a dataset on methane abatement from the International Environmental Agency (IEA):

```python
abatement_table = Table.read_table("abatement_data.csv").where('Cost',are.between(-10.1,10)).where('Possible Savings', are.below(200)).drop('Emissions').relabel('Possible Savings', 'Abatement Potential').relabel('Cost','Abatement Cost')
abatement_table
```

```python
selection = 'Asia Pacific'
Group = abatement_table.where('Region', selection)
Group
```

```python
#The find_x_pos function used for plotting! (out of scope)

def find_x_pos(widths):
    cumulative_widths = [0]
    cumulative_widths.extend(np.cumsum(widths))
    half_widths = [i/2 for i in widths]
    x_pos = []
    for i in range(0, len(half_widths)):
        x_pos.append(half_widths[i] + cumulative_widths[i])
    return x_pos
```

```python
#Prepare the data for plotting
width_group = Group.column('Abatement Potential')
height_group = Group.column('Abatement Cost')
new_x_group = find_x_pos(width_group)
```

With the following function, we introduce an arbritary level of taxation to measure the total abatement outcome later.

```python
#The methane_tax function -- Let's introduce a tax!
def methane_tax(tax, table):
    if tax < min(table.column('Abatement Cost')):
        print("No Abatement")
    else:
        abatement = table.where('Abatement Cost', are.below_or_equal_to(tax))
        total_abatement = sum(abatement.column('Abatement Potential'))
        abatement_technologies = abatement.column('Abatement technology')
        print('Methane tax: ', tax)
        print('Total Abatement: ', total_abatement)
        print("")
        print("Abatement Technologies", abatement_technologies)
```

```python
#The group_plot function used for plotting (out of scope)
def group_plot(tax):
    print(f"Methane: ${tax}")
    methane_tax(tax, Group)
    plt.figure(figsize=(9,6))
    plt.bar(new_x_group, height_group,width=width_group,edgecolor = 'black')
    plt.title(selection)
    plt.xlabel('Abatement Potential')
    plt.ylabel('Abatement Cost')
    plt.axhline(y=tax, color='r',linewidth = 2)
    
group_plot(4)
```

```python
#Prepare data for plotting (second round)
width = abatement_table.column('Abatement Potential')
height = abatement_table.column('Abatement Cost')
new_x = find_x_pos(width)
```

```python
#Let's give each type of technology a different color!
abatement_colors_dict = {}
count = 0
colors = ['#EC5F67', '#F29056', '#F9C863', '#99C794', '#5FB3B3', '#6699CC', '#C594C5','#85E827','#F165FD','#1F9F7F','#945CF8','#ff3a1d','#2a8506']
for i in set(abatement_table['Abatement technology']):
    abatement_colors_dict[i] = colors[count]
    count += 1

colors_mapped = list(pd.Series(abatement_table['Abatement technology']).map(abatement_colors_dict))
abatement_table = abatement_table.with_column('Color', colors_mapped)
```

```python
#The Methane curve plot - function!
def mckinsey_curve(tax):
    print(f"Methane Tax: ${tax}")
    methane_tax(tax, abatement_table)
    plt.figure(figsize=(18,12))
    plt.bar(new_x, height, width=width, linewidth=0.1, color=abatement_table['Color'], edgecolor = "black")
    plt.title('The McKinsey Abatement Cost Curve (MAC)')
    plt.xlabel('Abatement Potential')
    plt.ylabel('Abatement Cost')
    plt.axhline(y=tax, color='r', linewidth = 2)

    plt.figure(figsize=(5,1))
    plt.bar(abatement_colors_dict.keys(), 1, color = abatement_colors_dict.values())
    plt.xticks(rotation=60)
    plt.title('Legend')
    
mckinsey_curve(3)
```

What a plot! From here, we can differentiate the multiple methane abatement technologies on a cost basis, finding the most efficient ways of reducing methane emissions from gas production. We also observe the result of introducing a tax: With a tax of \$3 per ton, we expect the total abatement to be almost 20.000 tons within our industry.

## The MAC curve's important limitations

Before moving on to the next topic of this chapter, we ought to consider the drawbacks of our newfound knowledge:

### Limitation 1: Introducing Capital Intensity

From the MAC curve for CO2 (see figure 1), we are interested in finding the **capital intensity of an intervention**. This is different from the **Marginal Abatement Cost**, as it does not take potential savings from e.g lower energy consumption in the future into its calculation. However, it's a great measurement for which technologies require the highest *upfront capital* investment to abate an amount of GHG.


The formula is as follows: 

$$CI = \frac{\text{Additional Investment}}{\text{Lifetime Emissions Savings}}$$

Where Additional Investment is the the additional upfront investment for new technology relative to the Business as Usual (BAU) alternative).

Had we re-arranged the MAC curve from figure 1 for capital intensity, it would have looked like this:

![Screen%20Shot%202021-08-11%20at%2012.41.27%20PM.png](ci_curve.png)

### Limitation 2: Lack of Dynamic Cost Assessment

The most crucial limitations of the MAC curve is its inability to consider **Dynamic Costs**. In 2009, when the MAC was first created, it only considered **Static Costs**. Let's define them within the context of solar energy prices (graph below):
 
**Static Costs**
* The fixed costs of a new intervention, unchanged over a lifetime of an investment. Think of it as the costs you observe from this current point of view in time and expect to pay years ahead. For example, the 1976 Solar Energy Price at +100 USD/Watt.
 
 

**Dynamic Costs**

* Cost considering potential cost-reduction from increased efficiency, learning-by-doing, and other positive spillovers. For example, the actual cost of Solar Energy in 2019 at below 1 USD/Watt.
 
 
 
As a result of this shortcoming, the MAC curve tends to *overestimate costs* and do not fully represent the required investments within the energy transition (Kesicki and Edwins, 2012; Vogt-Schilb et al., 2015). It is not hard to understand that people were skeptical of investing in solar energy when the static costs were that high! A current example of this is the Carbon Capture & Storage (CCS) intervention: With high static costs, it might look like an unfavourable investment. What would it look like if we took dynamic costs into our calculations?
 
**A final note on the MAC curve:** As future data scientists, we have a responsibility to improve the MAC curve, and use our skills in prediction and analysis to assess dynamic costs in GHG abatement with higher certainty. The original McKinsey MAC has rounded its 10th year anniversary. Perhaps it's time for you to build a new one?

![solar-pv-prices.png](pv_prices.png)

## What's next?

If you are interested in this area, there are even more fascinating applications of Data Science to environmental topics such as: finding the social cost of carbon, the valuation of our environment, and the economics of emissions trading. Besides purely economical modeling, the field of environmental data science is rapidly growing as we collect more and more data on our planet and its resources. Data Science can apply to these problems the power of Satellite Imagery, Machine Learning, and Geographic Information Systems (GIS).  These tools can help to build a positive impact in shaping a data-informed, sustainable future.

## Further recommended readings

Levelized Cost of Carbon Abatement: An Improved Cost-Assessment Methodology for a Net-Zero Emissions World (also the main source of this Jupyter Notebook)

https://www.energypolicy.columbia.edu/sites/default/files/file-uploads/LCCA_CGEP-Report_101620.pdf

Dynamic vs. Static costs are described further in in K.Gillingham & J.H Stock's The Cost of Reducing Greenhouse Gas Emissions (italic) from 2018. - A highly recommended reading out of scope for this class.

https://scholar.harvard.edu/files/stock/files/gillingham_stock_cost_080218_posted.pdf

Goldman Sachs Research: Carbonomics: The Future of Energy in the Age of Climate Change
  
https://www.goldmansachs.com/insights/pages/carbonomics.html

EPA article on the Economics of Climate Change:
https://www.epa.gov/environmental-economics/economics-climate-change

Draw your own curve program:
https://tamc.github.io/macc/

Abatement curve for crops:
https://github.com/aj-sykes92/ggmacc/blob/main/README_files/figure-gfm/full-macc-1.png


Aalborg University's software:
https://github.com/matpri/EPLANoptMAC


--- END 12-environmental/.ipynb_checkpoints/textbook1-checkpoint.md ---



--- START 12-environmental/KuznetsHypothesis-Copy1.md ---

---
title: KuznetsHypothesis-Copy1
type: textbook
source_path: content/12-environmental/KuznetsHypothesis-Copy1.ipynb
chapter: 12
---

```python
# Importing packages

from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import patches
%matplotlib inline
```

# Environmental Kuznets Curve Hypothesis

The Environmental Kuznets curve hypothesis that the economic development of a nation is associated with a downward-facing U-shape.  The Y-axis is in terms of the level of environmental degradation (e.g pollution, water quality, deforestation.  The X-axis would be the GDP/capita.  The idea is that the environmental degradation worsens, until a certain level of income, and after which it gets better. In the US this could be seen in terms of air or water quality, where the skies or rivers were very polluted in the 1960s, until the Clean Air Act and Clean Water Act were passed and Air Quality and Water Quality improved.  Another motivation for the downward slope would be the idea that at some point a wealthier society demands environmental improvements.  
However - could this hold for the potentially most important Pollutant C02, the main driver of anthropogenic climate change.  Controversially the impacts of global CO2 pollution are not experienced locally, but are experienced as global effects. So it is not clear whether the Environmental Kuznets hypothesis will hold.  

Today, we'll look to build an C02 Kuznets Curve (as below) for an *association* between the amount of CO2 emitted per capita (t/CO2) and the growing GDP per capita (USD). This dataset is collected from Our World in Data, a great source of all sorts of data types!

![kuznets.png](kuznets.png)

## Building our own Environmental Kuznets Curve

We start by importing data on GDP per capita and Per Capita CO2 emissions for every country in the world for as long as it has been recorded.

```python
co2_table = Table.read_table('co2-emissions-vs-gdp.csv').drop('145446-annotations','Total population (Gapminder, HYDE & UN)','Code')
co2_table = co2_table.relabeled('Entity', 'Country')
co2_table
```

### Low Income Countries  
Let's start by selecting a set of Low Income Countries to graph a scatter plot on the movement of CO2 intensity per capita based on GDP per capita.

```python
#Low-Income Nations
LIH_array = make_array('Haiti', 'Afghanistan','Rwanda','Pakistan', 'Nicaragua')
LIH_table = co2_table.where('Country', are.contained_in(LIH_array))
LIH_table = LIH_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
plt.figure(figsize = (18,12), dpi=250)
LIH_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

Note that each dot represents a nation at a given level of emissions and GDP per capita

With these three countries we see a few different stories:
 - In Afghanistan, Haiti and Rwanda we see little income growth and little CO2 intensity growth with a slight upward trend
 - In Nicaragua we see some jumping around, in fact Nicaragua GDP per capita has gone up and down, as has CO2 per capita
 - In Pakistan, a larger and more populous country we see strong linear upward growth in both GDP per capita and CO2 per capita, with no signs of turning down

In these countries it is hard to tell the complete story without the exact time trend.

### BRICS  
Let's plot a similar scatter plot to examine some of the BRICS countries (rapidly growing upper middle income countries).

```python
BRICS_array = make_array('Brazil','Russia','India','China','South Africa')
BRICS_table = co2_table.where('Country', are.contained_in(BRICS_array))
BRICS_table = BRICS_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
plt.figure(figsize = (18,12), dpi=250)
BRICS_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

The BRICS nations seem to have a variety of development pathways but all show the linear trend of increasing emissions as wealth grows.  
- Russia has an interesting dip while GDP per capita decrease and then increase again
- South Africa has a recent period where growth in both GDP per capita and CO2 per capita have stagnated
- China and India show linearly increasing trends, with China both wealthier and more CO2 intensive

## Individual country graphs
Let's look at some individual countries, starting with the United States.   
We can plot both total and logged quantities on a scatter plot:

### United States

```python
#US Example + LOG
US_table = co2_table.where('Country', 'United States').where('Year', are.between(1800,2018))
US_table = US_table.with_column('LogGDP', np.log(US_table.column('GDP per capita'))).with_column('LogCO2',np.log(US_table.column('Per capita CO2 emissions')))
US_table.scatter('GDP per capita', 'Per capita CO2 emissions')
US_table.scatter('LogGDP', 'LogCO2')
plt.figure(figsize = (18,12), dpi=250)
```

**Looks like we have a curve!**

In the case of the US it does indeed look like the C02 per capita does indeed start to deline after about $40,000
Somewhere around 2000-2004 the CO2 emissions leveled off and then began to decline.

### China

In COP26, China has been a large part of the discussion (and emissions). Let's have a look at their scatter plot!

```python
#China Example + LOG
NO_table = co2_table.where('Country', 'China').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
plt.figure(figsize = (18,12), dpi=250)
```

Indeed it appears that China has an inflection point and is starting to level off in the Carbon intensity per capita

### India

What about India's scatter plot?

```python
#India Example + LOG
NO_table = co2_table.where('Country', 'India').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
plt.figure(figsize = (18,12), dpi=250)
```

Let's look at a set of other High Income Nations:

```python
HIN_array = make_array('United States', 'Netherlands', 'United Kingdom','Germany','Canada')
HIN_table = co2_table.where('Country', are.contained_in(HIN_array))
HIN_table = HIN_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
HIN_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
plt.figure(figsize = (18,12), dpi=250)
```

As in the US and Norway, these nations have experienced a boom, stagnation, and now to some extent a downward trend. Let's finally plot all the previously observed nations together in one singular scatter plot:

```python
ALL_array = np.append((np.append(LIH_array,BRICS_array)), HIN_array)
ALL_table = co2_table.where('Country', are.contained_in(ALL_array))
ALL_table = ALL_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
ALL_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
plt.figure(figsize = (18,12), dpi=250)
```

Here we see evidence for an Environmental Kuznets Curve.

It seems, at least to some extent, that as nations develop economically, the level of environmental degradation reaches a peak and then declines, mapping a downward-facing U-curve.

## Criticism of the Environmental Kuznets Curve Hypothesis

Some questions we ought to ask ourselves in the end are:
* Do all types of environmental degradation follow the curve? What if we plot Energy, Land, & Resource usage?
* What we plotted today shows the ratio between GDP and CO2 per capita, but what about the *absolute* numbers of emissions?
* What is the true long-term shape of the curve? Could it reshape itself to an "N" as an economy passes a certain threshold?
* What about its applicability on a global scale? Knowing that the HINs have a habit of exporting pollution to LINs, what will happen as LIN grow economically?

These are just some questions environmental economists have asked themselves throughout the years since the curve was hypothesized in 1955. Some, including Perman and Stern (2003) conclude that the level of environmental degradation has much more to do with a constant "battle" between scale and time than income alone. As nations scale up (BRICS, for instance) the growth results in higher emissions, while countries with lower growth (LIN & HIN) seem more influenced by the "time-effect", which results in lower emissions. Others, among Krueger & Grossman, argue that there is "no evidence that environmental quality deteriorates steadily with economic growth."

More on these theories can be found in the recommended readings below.

As data scientists motivated to help heal the plant with the tools of environmental economics, we can help to find these answers!

## What's next?

If you are interested in this area, there are even more fascinating applications of Data Science to environmental topics such as: finding the social cost of carbon, the valuation of our environment, and the economics of emissions trading. Besides purely economical modeling, the field of environmental data science is rapidly growing as we collect more and more data on our planet and its resources. Applying the power of Satellite Imagery, Machine Learning, and Geographic Information Systems (GIS), one can follow both technology and policy-based paths, both ensured to have a positive impact in shaping a data-driven, sustainable future.

## Further recommended readings

Levelized Cost of Carbon Abatement: An Improved Cost-Assessment Methodology for a Net-Zero Emissions World (also the main source of this Jupyter Notebook)

https://www.energypolicy.columbia.edu/publications/levelized-cost-carbon-abatement-improved-cost-assessment-methodology-net-zero-emissions-world/

Dynamic vs. Static costs are described further in K.Gillingham & J.H Stock's The Cost of Reducing Greenhouse Gas Emissions (italic) from 2018. - A highly recommended reading out of scope for this class.

https://scholar.harvard.edu/files/stock/files/gillingham_stock_cost_080218_posted.pdf


EPA article on the Economics of Climate Change:
https://www.epa.gov/environmental-economics/economics-climate-change

Draw your own curve program:
https://tamc.github.io/macc/

Abatement curve for crops:
https://github.com/aj-sykes92/ggmacc/blob/main/README_files/figure-gfm/full-macc-1.png


Aalborg University's software:
https://github.com/matpri/EPLANoptMAC


--- END 12-environmental/KuznetsHypothesis-Copy1.md ---



--- START 12-environmental/KuznetsHypothesis.md ---

---
title: KuznetsHypothesis
type: textbook
source_path: content/12-environmental/KuznetsHypothesis.ipynb
chapter: 12
---

```python
# HIDDEN
#Importing packages
from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import patches
%matplotlib inline
```

# Environmental Kuznets Curve Hypothesis

The Environmental Kuznets curve hypothesis that the economic development of a nation is associated with a downward-facing U-shape.  The Y-axis is in terms of the level of environmental degradation (e.g pollution, water quality, deforestation.  The X-axis would be the GDP/capita.  The idea is that the environmental degradation worsens, until a certain level of income, and after which it gets better. In the US this could be seen in terms of air or water quality, where the skies or rivers were very polluted in the 1960s, until the Clean Air Act and Clean Water Act were passed and Air Quality and Water Quality improved.  Another motivation for the downward slope would be the idea that at some point a wealthier society demands environmental improvements.  
However - could this hold for the potentially most important Pollutant C02, the main driver of anthropogenic climate change.  Controversially the impacts of global CO2 pollution are not experienced locally, but are experienced as global effects. So it is not clear whether the Environmental Kuznets hypothesis will hold.  

Today, we'll look to build an C02 Kuznets Curve (as below) for an *association* between the amount of CO2 emitted per capita (t/CO2) and the growing GDP per capita (USD). This dataset is collected from Our World in Data, a great source of all sorts of data types!

![kuznets.png](kuznets.png)

## Building our own Environmental Kuznets Curve

We start by importing data on GDP per capita and Per Capita CO2 emissions for every country in the world for as long as it has been recorded.

```python
co2_table = Table.read_table('co2-emissions-vs-gdp.csv').drop('145446-annotations','Total population (Gapminder, HYDE & UN)','Code')
co2_table = co2_table.relabeled('Entity', 'Country')
co2_table
```

### Low Income Countries  
Let's start by selecting a set of Low Income Countries to graph a scatter plot on the movement of CO2 intensity per capita based on GDP per capita.

```python
#Low-Income Nations
LIH_array = make_array('Haiti', 'Afghanistan','Rwanda','Pakistan', 'Nicaragua')
LIH_table = co2_table.where('Country', are.contained_in(LIH_array))
LIH_table = LIH_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
plt.figure(figsize = (18,12), dpi=250)
LIH_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

Note that each dot represents a nation at a given level of emissions and GDP per capita

With these three countries we see a few different stories:
 - In Afghanistan, Haiti and Rwanda we see little income growth and little CO2 intensity growth with a slight upward trend
 - In Nicaragua we see some jumping around, in fact Nicaragua GDP per capita has gone up and down, as has CO2 per capita
 - In Pakistan, a larger and more populous country we see strong linear upward growth in both GDP per capita and CO2 per capita, with no signs of turning down

In these countries it is hard to tell the complete story without the exact time trend.

### BRICS  
Let's plot a similar scatter plot to examine some of the BRICS countries (rapidly growing upper middle income countries).

```python
BRICS_array = make_array('Brazil','Russia','India','China','South Africa')
BRICS_table = co2_table.where('Country', are.contained_in(BRICS_array))
BRICS_table = BRICS_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
plt.figure(figsize = (18,12), dpi=250)
BRICS_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
```

The BRICS nations seem to have a variety of development pathways but all show the linear trend of increasing emissions as wealth grows.  
- Russia has an interesting dip while GDP per capita decrease and then increase again
- South Africa has a recent period where growth in both GDP per capita and CO2 per capita have stagnated
- China and India show linearly increasing trends, with China both wealthier and more CO2 intensive

## Individual country graphs
Let's look at some individual countries, starting with the United States.   
We can plot both total and logged quantities on a scatter plot:

### United States

```python
#US Example + LOG
US_table = co2_table.where('Country', 'United States').where('Year', are.between(1800,2018))
US_table = US_table.with_column('LogGDP', np.log(US_table.column('GDP per capita'))).with_column('LogCO2',np.log(US_table.column('Per capita CO2 emissions')))
US_table.scatter('GDP per capita', 'Per capita CO2 emissions')
US_table.scatter('LogGDP', 'LogCO2')
plt.figure(figsize = (18,12), dpi=250)
```

**Looks like we have a curve!**

In the case of the US it does indeed look like the C02 per capita does indeed start to deline after about $40,000
Somewhere around 2000-2004 the CO2 emissions leveled off and then began to decline.

### China

In COP26, China has been a large part of the discussion (and emissions). Let's have a look at their scatter plot!

```python
#China Example + LOG
NO_table = co2_table.where('Country', 'China').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
plt.figure(figsize = (18,12), dpi=250)
```

Indeed it appears that China has an inflection point and is starting to level off in the Carbon intensity per capita

### India

What about India's scatter plot?

```python
#India Example + LOG
NO_table = co2_table.where('Country', 'India').where('Year', are.between(1800,2018))
NO_table = NO_table.with_column('LogGDP', np.log(NO_table.column('GDP per capita'))).with_column('LogCO2',np.log(NO_table.column('Per capita CO2 emissions')))
NO_table.scatter('GDP per capita', 'Per capita CO2 emissions')
NO_table.scatter('LogGDP', 'LogCO2')
plt.figure(figsize = (18,12), dpi=250)
```

Let's look at a set of other High Income Nations:

```python
HIN_array = make_array('United States', 'Netherlands', 'United Kingdom','Germany','Canada')
HIN_table = co2_table.where('Country', are.contained_in(HIN_array))
HIN_table = HIN_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
HIN_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
plt.figure(figsize = (18,12), dpi=250)
```

As in the US and Norway, these nations have experienced a boom, stagnation, and now to some extent a downward trend. Let's finally plot all the previously observed nations together in one singular scatter plot:

```python
ALL_array = np.append((np.append(LIH_array,BRICS_array)), HIN_array)
ALL_table = co2_table.where('Country', are.contained_in(ALL_array))
ALL_table = ALL_table.where('GDP per capita', are.above_or_equal_to(0)).where('Per capita CO2 emissions', are.above_or_equal_to(0))
ALL_table.scatter('GDP per capita', 'Per capita CO2 emissions',group='Country')
plt.figure(figsize = (18,12), dpi=250)
```

Here we see evidence for an Environmental Kuznets Curve.

It seems, at least to some extent, that as nations develop economically, the level of environmental degradation reaches a peak and then declines, mapping a downward-facing U-curve.

## Criticism of the Environmental Kuznets Curve Hypothesis

Some questions we ought to ask ourselves in the end are:
* Do all types of environmental degradation follow the curve? What if we plot Energy, Land, & Resource usage?
* What we plotted today shows the ratio between GDP and CO2 per capita, but what about the *absolute* numbers of emissions?
* What is the true long-term shape of the curve? Could it reshape itself to an "N" as an economy passes a certain threshold?
* What about its applicability on a global scale? Knowing that the HINs have a habit of exporting pollution to LINs, what will happen as LIN grow economically?

These are just some questions environmental economists have asked themselves throughout the years since the curve was hypothesized in 1955. Some, including Perman and Stern (2003) conclude that the level of environmental degradation has much more to do with a constant "battle" between scale and time than income alone. As nations scale up (BRICS, for instance) the growth results in higher emissions, while countries with lower growth (LIN & HIN) seem more influenced by the "time-effect", which results in lower emissions. Others, among Krueger & Grossman, argue that there is "no evidence that environmental quality deteriorates steadily with economic growth."

More on these theories can be found in the recommended readings below.

As data scientists motivated to help heal the plant with the tools of environmental economics, we can help to find these answers!

## What's next?

If you are interested in this area, there are even more fascinating applications of Data Science to environmental topics such as: finding the social cost of carbon, the valuation of our environment, and the economics of emissions trading. Besides purely economical modeling, the field of environmental data science is rapidly growing as we collect more and more data on our planet and its resources. Applying the power of Satellite Imagery, Machine Learning, and Geographic Information Systems (GIS), one can follow both technology and policy-based paths, both ensured to have a positive impact in shaping a data-driven, sustainable future.

## Further recommended readings

Levelized Cost of Carbon Abatement: An Improved Cost-Assessment Methodology for a Net-Zero Emissions World (also the main source of this Jupyter Notebook)

https://www.energypolicy.columbia.edu/publications/levelized-cost-carbon-abatement-improved-cost-assessment-methodology-net-zero-emissions-world/

Dynamic vs. Static costs are described further in K.Gillingham & J.H Stock's The Cost of Reducing Greenhouse Gas Emissions (italic) from 2018. - A highly recommended reading out of scope for this class.

https://scholar.harvard.edu/files/stock/files/gillingham_stock_cost_080218_posted.pdf


EPA article on the Economics of Climate Change:
https://www.epa.gov/environmental-economics/economics-climate-change

Draw your own curve program:
https://tamc.github.io/macc/

Abatement curve for crops:
https://github.com/aj-sykes92/ggmacc/blob/main/README_files/figure-gfm/full-macc-1.png


Aalborg University's software:
https://github.com/matpri/EPLANoptMAC


--- END 12-environmental/KuznetsHypothesis.md ---



--- START 12-environmental/MAC.md ---

---
title: MAC
type: textbook
source_path: content/12-environmental/MAC.ipynb
chapter: 12
---

```python
# Importing packages

from datascience import *
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import patches
%matplotlib inline
```

# Marginal Abatement Cost Curves

## Marginal Cost

As of know, you should be familiar with the economic concept of **marginal cost**. The classical textbook example is a production factory, so let's call ours Peter's Pens Limited. Whenever Peter's Pens' management is deciding whether or not to increase production to maximize their profit, they observe the firm's marginal cost: the change in its **total cost** that occurs as 1 more pen is produced. In this case, we measure the marginal cost in $ / per extra pen produced. In environmental economics, we talk about the **marginal cost of emissions** and the **marginal cost of emission abatement**. We think of it as follows:

$$ \text{MC of Emissions (\$/ton)} = \frac{\text{$\Delta$ Cost of Emissions (\$)}}{\text{$\Delta$ Quantity of Emissions (tons)}}$$

And: 

$$ \text{MC of Emission Abatement (\$/ton)} = \frac{\text{$\Delta$ Cost of Emissions Abatement (\$)}}{\text{$\Delta$ Quantity of Emission Abated (tons)}}$$

## Government Intervention

Let's say that Peter's Pens Limited is operating in a market where the government aims to make firms **internalize** the negative externality created by $CO_2$ pollution from their production. From earlier, we know that the government may choose between **prescriptive** or **market-based** policies, but for this case we assume they chose the latter. In that case, the government introduces a *Pigouvian tax* on per ton of $CO_2$ emitted from all firms. As any Pigouvian tax, its rate is set to the *social marginal cost of the negative externality* it is set to internalize. This is the *social cost of carbon*, quantified by extensive research. In the big picture, it pushes the private marginal cost towards the higher social marginal cost of emissions. As with any price increase, we assume the ‘supplied’ quantity of emissions to be pushed leftwards (decreased). This results in a new equilibrium quantity of emissions at a higher price (= social marginal cost) and the quantity reached becomes the *optimal quantity* of emissions in society. This can be seen in the diagram below. Now, how does the management of Peter’s Pens Limited react to the new market conditions?

![](tax.png)

### The Firm’s Reaction
 
From a ‘bottom-up-perspective’, the management at Peter’s Pens Limited is faced with two choices in reaction to the new emissions tax: The Business-as-Usual (BAU) alternative or the Emissions Abatement alternative. The firm can either choose to continue emitting and pay the tax or invest in new, environmentally friendly technologies that abate their $CO_2$ emissions. A rational decision maker ought to invest in abatement technologies with a marginal cost of abatement up to the emissions tax put in place. Assuming they chose to pursue the latter, how would they prioritize investments within the firm? By observing the cost and abatement potential of each investment, the firm sorts its options from cheapest to most expensive and starts ‘picking’ the low-hanging fruits of $CO_2$ abatement. At any given carbon tax rate, they reduce their tax bill to $0 and observe their total abatement as the sum of all technologies invested in. The resulting plot: The Marginal Abatement Cost (MAC) Curve of Peter’s Pens Limited. Here's how a simple MAC for Peter's Pens might look:

![SampleMACC_Nov23.png](SampleMACC_Nov23.png)

In this example, the firm has a few activities with a low cost of abatement, and then a few with higher cost of abatement. It would make sense for the firm to start with the lowest cost of abatement first.

### Back to the Government 

Returning to the government policymaker’s perspective, we can motivate this approach to a market wide perspective. Just as Peter’s Pens has their firm-specific MAC, society as a whole has one. Through research on policy and technological interventions, an environmental data science team can quantify the *abatement potential* and the *abatement cost* of different technologies (e.g electric cars, wind & solar power generation), policies (e.g fuel standards), and land management projects (e.g  soil restoration). From here, public policy can start  ‘picking’ the low-hanging fruits of the lowest cost technology first. 

Earlier, we described the effect of introducing a tax on the emissions ‘market’ and observed how rational decision-makers in individual firms react by constructing their own MAC curves. Now, let’s say that the government has set an emission reduction target but has not decided on a specific tax rate yet. How could they guarantee that the target is met? By taking the cumulative sum of the abatement potential of each intervention they expect to take place at a given rate, they should keep doing so until their sum equals the emission reduction target. From there, they observe the marginal abatement cost of the last intervention they added to the sum. This could be a way to set the emissions tax rate. Here's how a simplified version of the MAC could look:

![image.png](image.png)

### Conclusion
 
In conclusion, both an individual firm and society as a whole could be modeled to have MAC curves. By either choosing an emission reduction target or a emissions tax rate, one can arrive at a lowest cost level of abatement. Whenever an emission tax is introduced to the market, rational firms build their MAC curves and start investing in the cheapest relevant technologies up to the level of the given tax rate to avoid paying the tax. A rational policy maker can map out the different abatement opportunities in a society as a whole and prioritize the cheapest alternatives. Public policy could stop emissions abatement when the technology or policy we invest in has a higher marginal cost than the societal marginal cost of emissions. From there, any intervention reduces either individual (firm) or societal welfare.
 
Now, let’s delve deeper into the  MAC curve and build one of our own for Methane Abatement.

## The McKinsey Marginal Abatement Cost Curve (MAC)

As earlier described, the Marginal Abatement Cost Curve gives policy makers and firms an opportunity to differentiate the costs of the multiple approaches we have in reducing our carbon (CO2) emissions. It shows where society can get the best "bang for the buck" when the goal is to abate carbon emissions. The **Abatement Potential (GtCO2 per year)** follows the x-axis, and the ** Marginal Abatement Cost (€ per ton of CO2)** lies on the y-axis.
 
Each rectangle represents a specific technology or policy (e.g switch to LED lights or install Carbon Capture Systems (CCS) in older coal plants). The *wider* the rectangle, the *larger* the abatement potential, and the *taller* the rectangle, the *higher* abatement cost for that specific intervention.
 
Below you'll find a very influential MAC out there, the McKinsey MAC from 2009. It has the goal of showing the tradoffs in a single visualization of GHG emissions across a variety of sectors and across all countries. A policy maker, or innovator, or investor could decide to focus on areas where GHG emissions can be reduced at a low cost.

![Screen%20Shot%202021-05-15%20at%201.39.57%20PM.png](mac.png)

### Why are some costs negative?

Something that understandably strikes a lot of observers from the plot above is the concept of *negative costs*. Take LEDs (about - €100 per tCO2) for instance: The negative cost here entails that for every investment that reduces CO2 from traditional lightning by 1 ton CO2, the investor receives a €100 saving. In one sense these are all investments that should happen regardless of the GHG consequences they make economic sense. One way to motivate this is that the decision maker may not be able to see the long term benefits in a short-term decision making framework. People dont rush out to buy $5 LED light bulbs when they have working incandescent light bulbs.
 
The cause of negative costs are disputed, but a commonly used idea is that within our current market model, there exists *inefficiencies* such as lack of incentives or information that prohibit market participants from taking the full advantage of the returns offered from investing in technologies with negative costs.

## Constructing a MAC for Methane Gas Abatement

For an illustrative example we will construct a Marginal Abatement Curve for Methane emissions from the Oil and Gas Sector.  
We start by importing a dataset on methane abatement from the International Environmental Agency (IEA):

```python
abatement_table = Table.read_table("abatement_data.csv").where('Cost',are.between(-10.1,10)).where('Possible Savings', are.below(200)).drop('Emissions').relabel('Possible Savings', 'Abatement Potential').relabel('Cost','Abatement Cost')
abatement_table
```

```python
selection = 'Asia Pacific'
Group = abatement_table.where('Region', selection)
Group
```

```python
#The find_x_pos function used for plotting! (out of scope)

def find_x_pos(widths):
    cumulative_widths = [0]
    cumulative_widths.extend(np.cumsum(widths))
    half_widths = [i/2 for i in widths]
    x_pos = []
    for i in range(0, len(half_widths)):
        x_pos.append(half_widths[i] + cumulative_widths[i])
    return x_pos
```

```python
#Prepare the data for plotting
width_group = Group.column('Abatement Potential')
height_group = Group.column('Abatement Cost')
new_x_group = find_x_pos(width_group)
```

With the following function, we introduce an arbritary level of taxation to measure the total abatement outcome later.

```python
#The methane_tax function -- Let's introduce a tax!
def methane_tax(tax, table):
    if tax < min(table.column('Abatement Cost')):
        print("No Abatement")
    else:
        abatement = table.where('Abatement Cost', are.below_or_equal_to(tax))
        total_abatement = sum(abatement.column('Abatement Potential'))
        abatement_technologies = abatement.column('Abatement technology')
        print('Methane tax: ', tax)
        print('Total Abatement: ', total_abatement)
        print("")
        print("Abatement Technologies", abatement_technologies)
```

```python
#The group_plot function used for plotting (out of scope)
def group_plot(tax):
    print(f"Methane: ${tax}")
    methane_tax(tax, Group)
    plt.figure(figsize=(18,12))
    plt.bar(new_x_group, height_group,width=width_group,edgecolor = 'black')
    plt.title("Marginal Abatement Cost Curve (MAC) for the Asia Pacific Region", size = 20)
    plt.xlabel('Abatement Potential', fontsize = 14)
    plt.ylabel('Abatement Cost', fontsize = 14)
    plt.axhline(y=tax, color='r',linewidth = 2)
    
group_plot(4)
```

```python
#Prepare data for plotting (second round)
width = abatement_table.column('Abatement Potential')
height = abatement_table.column('Abatement Cost')
new_x = find_x_pos(width)
```

```python
#Let's give each type of technology a different color!
abatement_colors_dict = {}
count = 0
colors = ['#EC5F67', '#F29056', '#F9C863', '#99C794', '#5FB3B3', '#6699CC', '#C594C5','#85E827','#F165FD','#1F9F7F','#945CF8','#ff3a1d','#2a8506']
for i in set(abatement_table['Abatement technology']):
    abatement_colors_dict[i] = colors[count]
    count += 1

colors_mapped = list(pd.Series(abatement_table['Abatement technology']).map(abatement_colors_dict))
abatement_table = abatement_table.with_column('Color', colors_mapped)
```

```python
#The Methane curve plot - function!
def mckinsey_curve(tax):
    print(f"Methane Tax: ${tax}")
    methane_tax(tax, abatement_table)
    plt.figure(figsize=(18,12))
    plt.bar(new_x, height, width=width, linewidth=0.1, color=abatement_table['Color'], edgecolor = "black")
    plt.title('The McKinsey Abatement Cost Curve (MAC)', size = 20)
    plt.xlabel('Abatement Potential', fontsize = 14)
    plt.ylabel('Abatement Cost', fontsize = 14)
    plt.axhline(y=tax, color='r', linewidth = 2)

    plt.figure(figsize=(18,12))
    plt.bar(abatement_colors_dict.keys(), 1, color = abatement_colors_dict.values())
    plt.xticks(rotation=45)
    plt.title('Legend', size = 20) 
    
mckinsey_curve(3)
```

What a plot! From here, we can differentiate the multiple methane abatement technologies on a cost basis, finding the most efficient ways of reducing methane emissions from gas production. We also observe the result of introducing a tax: With a tax of \$3 per ton, we expect the total abatement to be almost 20.000 tons within our industry.

## The MAC curve's important limitations

Before moving on to the next topic of this chapter, we ought to consider the drawbacks of our newfound knowledge:

### Limitation 1: Introducing Capital Intensity

From the MAC curve for CO2 (see figure 1), we are interested in finding the **capital intensity of an intervention**. This is different from the **Marginal Abatement Cost**, as it does not take potential savings from e.g lower energy consumption in the future into its calculation. However, it's a great measurement for which technologies require the highest *upfront capital* investment to abate an amount of GHG.


The formula is as follows: 

$$CI = \frac{\text{Additional Investment}}{\text{Lifetime Emissions Savings}}$$

Where Additional Investment is the the additional upfront investment for new technology relative to the Business as Usual (BAU) alternative).

Had we re-arranged the MAC curve from figure 1 for capital intensity, it would have looked like this:

![Screen%20Shot%202021-08-11%20at%2012.41.27%20PM.png](ci_curve.png)

### Limitation 2: Lack of Dynamic Cost Assessment

The most crucial limitations of the MAC curve is its inability to consider **Dynamic Costs**. In 2009, when the MAC was first created, it only considered **Static Costs**. Let's define them within the context of solar energy prices (graph below):
 
**Static Costs**
* The fixed costs of a new intervention, unchanged over a lifetime of an investment. Think of it as the costs you observe from this current point of view in time and expect to pay years ahead. For example, the 1976 Solar Energy Price at +100 USD/Watt.
 
 

**Dynamic Costs**

* Cost considering potential cost-reduction from increased efficiency, learning-by-doing, and other positive spillovers. For example, the actual cost of Solar Energy in 2019 at below 1 USD/Watt.
 
 
 
As a result of this shortcoming, the MAC curve tends to *overestimate costs* and do not fully represent the required investments within the energy transition (Kesicki and Edwins, 2012; Vogt-Schilb et al., 2015). It is not hard to understand that people were skeptical of investing in solar energy when the static costs were that high! A current example of this is the Carbon Capture & Storage (CCS) intervention: With high static costs, it might look like an unfavourable investment. What would it look like if we took dynamic costs into our calculations?
 
**A final note on the MAC curve:** As future data scientists, we have a responsibility to improve the MAC curve, and use our skills in prediction and analysis to assess dynamic costs in GHG abatement with higher certainty. The original McKinsey MAC has rounded its 10th year anniversary. Perhaps it's time for you to build a new one?

![solar-pv-prices.png](pv_prices.png)

## What's next?

If you are interested in this area, there are even more fascinating applications of Data Science to environmental topics such as: finding the social cost of carbon, the valuation of our environment, and the economics of emissions trading. Besides purely economical modeling, the field of environmental data science is rapidly growing as we collect more and more data on our planet and its resources. Data Science can apply to these problems the power of Satellite Imagery, Machine Learning, and Geographic Information Systems (GIS).  These tools can help to build a positive impact in shaping a data-informed, sustainable future.

## Further recommended readings

Levelized Cost of Carbon Abatement: An Improved Cost-Assessment Methodology for a Net-Zero Emissions World (also the main source of this Jupyter Notebook)

https://www.energypolicy.columbia.edu/sites/default/files/file-uploads/LCCA_CGEP-Report_101620.pdf

Dynamic vs. Static costs are described further in in K.Gillingham & J.H Stock's The Cost of Reducing Greenhouse Gas Emissions (italic) from 2018. - A highly recommended reading out of scope for this class.

https://scholar.harvard.edu/files/stock/files/gillingham_stock_cost_080218_posted.pdf

Goldman Sachs Research: Carbonomics: The Future of Energy in the Age of Climate Change
  
https://www.goldmansachs.com/insights/pages/carbonomics.html

EPA article on the Economics of Climate Change:
https://www.epa.gov/environmental-economics/economics-climate-change

Draw your own curve program:
https://tamc.github.io/macc/

Abatement curve for crops:
https://github.com/aj-sykes92/ggmacc/blob/main/README_files/figure-gfm/full-macc-1.png


Aalborg University's software:
https://github.com/matpri/EPLANoptMAC


--- END 12-environmental/MAC.md ---



--- START 12-environmental/index.md ---

---
title: index
type: textbook
source_path: content/12-environmental/index.md
chapter: 12
---

# Environmental Economics

![](windmills.png)

In a broad sense, the field of Environmental Economics aims to relate and apply economic concepts to the environment.
 
One tenet of Environmental Economics is that the enjoyment of "environmental amenities"   (or conversely that the usage or degradation  of those resources) has an intrinsic value to humans that goes unaccounted for in the purely market-based model. These unaccounted costs are considered *market failures* and carry *negative externalities*.
 
The greatest single example of a negative externality of global importance is the emission of greenhouse gases (carbon dioxide, methane, nitrous oxide) from the combustion of hydrocarbons (coal, gasoline, diesel, oil). The *true cost*  is not reflected in the lower price one pays at e.g the gas station. Consequently, the equilibrium quantity consumed is higher than the *socially optimal quantity*. Environmental economists seek to model the costs and benefits of a reduction.  How could we reduce the quantity to the social optimum and weigh in the *costs and benefits* of such a reduction?
 
As a result, a major proportion of research and work within the field is devoted to building tools to reveal, address, and evaluate economic policies aimed at *internalizing* these externalities. 

Very often, these policy tools as applied by a government constitute interfering with the market to a varying degree. We can model environmental economic policies into two subsets:
 
* **Command and Control**: When the government limits the amount of pollution to control a negative externality, e.g letting each emitter in the market emit a fixed amount of GHG gases.
 
* **Market-based**: Where the government sets an emission goal, then introduces incentives or subsidies to alter market behavior. It is left to each market actor to decide how much to emit. A carbon tax and a cap-and-trade (carbon quotas) are examples of marked-based interventions.
 
A useful environmental economic model is **The Marginal Abatement Cost Curve (MAC)** which aims to describe the *cost of abatement* (not emitting) greenhouse gases into the atmosphere using various strategies. A famous version of this was contained in a report  McKinsey 2009 "Pathways to a Low Carbon Economy".  This example we will discuss as an  example of the intersection between of Environmental Economics and Data Science. In this notebook, we'll walk through the concepts of it and build one of our own! 

Related to the discussion of global greenhouse gas emissions, we will also explore the **Environmental Kuznets Curve** through data, and compare the pathways of increasing emissions across countries and across time.  
 
These are only a few of the applications of environmental economics, there are many more fields to explore, and even an entire major at UC Berkeley to explore.  

**Student Learning Outcomes:**
* An introduction to applications  of Environmental Economics with illustrations from global CO2 emissions
* Motivation and understanding of the  Marginal Abatement Cost Curve for global Greenhouse Gas emissions, first by the (McKinsey 2009) curve for CO2, and then an application for Methane its data science application.
* A discussion of the Marginal Abatement Curve's limitations with the concept of Capital Intensity and static vs dynamic costs.
* An understanding of the Environmental Kuznets Curve Hypothesis and its data science applications


--- END 12-environmental/index.md ---



--- START LICENSE.md ---

---
title: LICENSE
type: textbook
source_path: content/LICENSE.md
---

# License for this book

All content in this book (ie, any files and content in the `content/` folder)
is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)
(CC BY-SA 4.0) license.


--- END LICENSE.md ---



--- START intro.md ---

---
title: intro
type: textbook
source_path: content/intro.md
---

# <a href="https://data-88e.github.io"> Data 88E: Economic Models </a>

Economics is in the world around us, and so is Data Science! It's in our every day lives. As we connect Data Science with Economics, we will be exploring real life datasets to illustrate how Economics concepts are shaped and how decisions lead to real-life impacts. This is a textbook developed for UC Berkeley's course Data 88E: Economic Models. Data 88E is a generic course listing for Data 8 connector courses.


## Course Description

The idea for the class is to take students through a series of exercises to motivate and illustrate key concepts in Economics with examples in Python Jupyter notebooks. The class will cover concepts from Introductory Economics, MIcroeconomic Theory, Econometrics, Development Economics, Environmental Economics and Public Economics. The course will give data science students a pathway to apply python programming and data science concepts within the discipline of economics. The course will also give economics students a pathway to apply programming to reinforce fundamental concepts and to advance the level of study in upper division coursework and possible thesis work.

## Acknowledgements

**Course Instructor:** Eric Van Dusen

**Textbook Developers:** [Christopher Pyles](https://chrispyles.io), Rohan Jha

**Content Developers:** [Akhil Venkatesh](https://akhilv.webflow.io), [Alan Liang](http://alanliang.me/), Amal Bhatnagar, Andrei Caprau, [Christopher Pyles](https://chrispyles.io), Cole Ginter, Eric Van Dusen, Peter F. Grinde-Hollevik, Matthew Yep, Rohan Jha, Sreeja Apparaju, Shashank Dalmia, Sushil Vishwanathan, Umar Maniku

## License

This textbook is licensed under a [BSD 3-Clause License](https://github.com/ds-connectors/econ-models-textbook/blob/master/LICENSE).


--- END intro.md ---



--- START references.md ---

---
title: references
type: textbook
source_path: content/references.md
---

# Bibliography

\[BdP\]
Marijn Bolhuis and Alexandra de Pleijt. [Inequality fuels population growth: new cross-national evidence](https://www.ehs.org.uk/press/inequality-fuels-population-growth-new-cross-national-evidence-1870-2000) 1870-2000.

\[Car99\]
David Card. [The causal effect of education on earnings.](https://davidcard.berkeley.edu/papers/causal_educ_earnings.pdf) Handbook of Labor Economics, 1999.

\[Hoo41\]
S Hoos. An investigation on complementarity relations between fresh fruits. Journal of Farm Economics, 23(2):421–433, 1941.

\[KS\]
Homi Kharas and Brina Seidel. [What's happening to the world income distribution? the elephant chart revisited.](https://www.brookings.edu/research/whats-happening-to-the-world-income-distribution-the-elephant-chart-revisited/)

\[MIL16\]
BRANKO MILANOVIC. [Global Inequality: A New Approach for the Age of Globalization.](http://www.jstor.org/stable/j.ctvjghwk4) Harvard University Press, 2016. ISBN 9780674737136.

\[Mor11\]
Hanan Morsy. [Unemployed in europe.](https://www.researchgate.net/publication/341109902_Unemployed_in_Europe) Finance & development, 09 2011. 


--- END references.md ---

